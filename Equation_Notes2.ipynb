{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Equation.Notes2",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "NSkHPd7oBubz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# MODULE 2"
      ]
    },
    {
      "metadata": {
        "id": "W2WH1P-RBzCm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Introduction to Pytorch\n",
        "\n",
        "Pytorch is a library that looks a lot like numpy. It deals with tensors and manipulation of tensors."
      ]
    },
    {
      "metadata": {
        "id": "4y0M8IeHB4N0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## PyTorch Setup"
      ]
    },
    {
      "metadata": {
        "id": "t2pAJ2ArBmbt",
        "colab_type": "code",
        "outputId": "ccb60e4d-067e-4c2f-8136-8663ceff9df1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "!python -V"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python 3.6.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6BaJtBsTCFoq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!pip3 install torch torchvision\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JglEB5xUCFjb",
        "colab_type": "code",
        "outputId": "7617f9f4-149d-40f2-94fa-4e6994b4cfa4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"PyTorch version: \")\n",
        "torch.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PyTorch version: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.0.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "35i5hEGUCFgE",
        "colab_type": "code",
        "outputId": "446db43e-36df-4681-8969-ceac9cef96bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"Device Name: \")\n",
        "torch.cuda.get_device_name(0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device Name: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tesla K80'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "AM9s6ad1CLZn",
        "colab_type": "code",
        "outputId": "bf4cd522-0648-46c7-8bcb-0fa7eea3ebaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"CUDA Version: \")\n",
        "print(torch.version.cuda)\n",
        "print(\"cuDNN version is: \")\n",
        "print(torch.backends.cudnn.version())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA Version: \n",
            "9.0.176\n",
            "cuDNN version is: \n",
            "7401\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SwsmPGshCLWx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# NVIDIA profiling tool for the available GPU\n",
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "h3TXuLKmCLTZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cat /proc/cpuinfo"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gOEg2B67CPW5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cat /proc/meminfo"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D5aZGAwEjPrs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## What is a tensor?"
      ]
    },
    {
      "metadata": {
        "id": "UsEXgaaUjMiQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The inputs, outputs, and transformations within neural networks are all represented using tensors, and as a result, neural network programming utilizes tensors heavily. \n",
        "\n",
        " The concept of a tensor is a mathematical generalization of other more specific concepts. Let’s look at some specific instances of tensors.\n",
        " \n",
        "**Specific instances of tensors**\n",
        "\n",
        "Each of these examples are specific instances of the more general concept of a tensor:\n",
        "\n",
        "* number\n",
        "* scalar\n",
        "* array\n",
        "* vector\n",
        "* 2d-array\n",
        "* matrix\n",
        "\n",
        "Let’s organize the above list of example tensors into two groups:\n",
        "\n",
        "* number, array, 2d-array\n",
        "* scalar, vector, matrix\n",
        "\n",
        "The first group of three terms (number, array, 2d-array) are terms that are typically used in computer science, while the second group (scalar, vector, matrix) are terms that are typically used in mathematics. \n",
        "\n",
        "Tensors can be created using the following **syntax**."
      ]
    },
    {
      "metadata": {
        "id": "ecX6ojK3DP7N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "c = torch.tensor([[1,2],[1,2]]) \n",
        "d = torch.tensor([[[1,2],[3,4]],[[5,6],[7,8]]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PM4zSHBdCcMB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Tensor Types \n",
        "Tensors are **homogeneous** as all the items within it are of the **same data type**. PyTorch tensors are instances of the torch.Tensor Python class. We can create a torch.Tensor object using the class constructor as below. They can also be created from torch.tensor. "
      ]
    },
    {
      "metadata": {
        "id": "lmwBiwQvCroa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "a = torch.tensor([0,1,2,3,4])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "k77w0EeiC5WT",
        "colab_type": "code",
        "outputId": "0c6e7cc6-4c31-46a5-93d2-fab48a4f0654",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"Type of entire array:\")\n",
        "type(a)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Type of entire array:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Tensor"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "cmXDjXtJC-Cp",
        "colab_type": "code",
        "outputId": "e1fde346-9158-4375-e7ff-dcdd1cbdc844",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"Datatype of tensor:\")\n",
        "a.type()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Datatype of tensor:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'torch.LongTensor'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "Mv4vAQ6SpeYE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Every torch.Tensor has these attributes:\n",
        "\n",
        "* torch.dtype\n",
        "* torch.device\n",
        "* torch.layout\n",
        "\n",
        "Looking at our Tensor a, we can see the following default attribute values: "
      ]
    },
    {
      "metadata": {
        "id": "juDMalVrpdYA",
        "colab_type": "code",
        "outputId": "f0631348-e0a5-4367-d71a-e94d6f6699d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "print(a.dtype)\n",
        "print(a.device)\n",
        "print(a.layout)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.int64\n",
            "cpu\n",
            "torch.strided\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OhEQSoFXDVoG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Tensor datatypes are given in the table below. Note that torch.tensor is an alias for the default tensor type (torch.FloatTensor).\n",
        "\n",
        "| Data type | dtype   | CPU tensor | GPU Tensor   |\n",
        "|-------  |------|------   |------|\n",
        "|  32-bit floating point    | torch.float32 or torch.float     |   torch.FloatTensor      | torch.cuda.FloatTensor |\n",
        "|  64-bit floating point    | torch.float64 or torch.double |   torch.DoubleTensor   | torch.cuda.DoubleTensor |\n",
        "|  16-bit floating point    | torch.float16 or torch.half      |   torch.HalfTensor        | torch.cuda.HalfTensor |\n",
        "|  8-bit integer (unsigned)    | torch.uint8                          |   torch.ByteTensor       | torch.cuda.ByteTensor |\n",
        "|  8-bit integer (signed)    | torch.int8                                 |   torch.CharTensor       | torch.cuda.CharTensor |\n",
        "|  16-bit integer (signed)    | torch.int16 or torch.short   |   torch.ShortTensor     | torch.cuda.ShortTensor |\n",
        "|  32-bit integer (signed)    | torch.int32 or torch.int        |   torch.IntTensor          | torch.cuda.IntTensor |\n",
        "|  64-bit integer (signed)    | torch.int64 or torch.long     |   torch.LongTensor     | torch.cuda.LongTensor |"
      ]
    },
    {
      "metadata": {
        "id": "Am95d70Xqn3N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Type casting** is also supported. In the example below, the method FloatTensors casts integers into float. If we have a device like above, we can create a tensor on the device by passing the device to the tensor’s constructor. "
      ]
    },
    {
      "metadata": {
        "id": "fX4nM2OFDHlm",
        "colab_type": "code",
        "outputId": "1b2a353b-a79b-4c52-9bb6-cadca7e974dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "b = torch.FloatTensor([0,1,2,3,4])\n",
        "print(\"Type of b: \")\n",
        "b.type()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Type of b: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'torch.FloatTensor'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "ho0Q59DFDJql",
        "colab_type": "code",
        "outputId": "8f71dd1b-f2c4-4144-d5d3-8a9d8f3987c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "a = a.type(torch.FloatTensor)\n",
        "print(\"Type of a: \")\n",
        "a.type()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Type of a: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'torch.FloatTensor'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "LWf2w6qqpzJI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Notice how each type in the table has a CPU and GPU version. One thing to keep in mind about tensor data types is that tensor operations between tensors must happen between tensors with the same type of data. \n",
        "\n",
        "The device, **cpu** in our case, specifies the device (CPU or GPU) where the tensor's **data** is **allocated**. This determines where **tensor computations** for the given tensor will be performed.\n",
        "\n",
        "PyTorch supports the use of multiple devices, and they are specified using an index like so: "
      ]
    },
    {
      "metadata": {
        "id": "xYGX-PVQpyxT",
        "colab_type": "code",
        "outputId": "a6d50550-da0f-4fac-9ab3-8b84e29e8eb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0')\n",
        "device"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "metadata": {
        "id": "4Ksv3OMUDCuE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "One thing to keep in mind about using **multiple devices** is that tensor operations between **tensors** must happen between tensors that **exists** on the **same device**. "
      ]
    },
    {
      "metadata": {
        "id": "_7DYF4YEqt4m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Tensors have a **torch.layout**\n",
        "\n",
        "The **layout, strided** in our case, specifies how the tensor is **stored** **in memory**.\n",
        "\n",
        "As neural network programmers, we need to be aware of the following:\n",
        "\n",
        "* Tensors contain data of a uniform type (dtype).\n",
        "* Tensor computations between tensors depend on the dtype and the device.\n"
      ]
    },
    {
      "metadata": {
        "id": "PiiaNZO0DMmt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Creating tensors using data**"
      ]
    },
    {
      "metadata": {
        "id": "dmmCZcdsrc19",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "These are the primary ways of creating tensor objects (instances of the torch.Tensor class), with data (array-like) in PyTorch:\n",
        "\n",
        "1. torch.Tensor(data)\n",
        "2. torch.tensor(data)\n",
        "3. torch.as_tensor(data)\n",
        "4. torch.from_numpy(data)"
      ]
    },
    {
      "metadata": {
        "id": "GPit6rsDsgT2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can use a Python list, or sequence, but numpy.ndarrays are going to be the more common option, so we’ll go with a numpy.ndarray like so: "
      ]
    },
    {
      "metadata": {
        "id": "42jNRZOErWCb",
        "colab_type": "code",
        "outputId": "5cdd833f-470b-44a8-8855-bbd4af3298a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "datanp = np.array([1,2,3])\n",
        "type(datanp)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "KTKLCjHcswGC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Note PyTorch tensors can be created with the torch.Tensor constructor, which takes the tensor’s dimensions as input and returns a tensor occupying an uninitialized region of memory\n",
        "\n",
        "Now, let’s create our tensors with each of these options 1-4, and have a look at what we get: "
      ]
    },
    {
      "metadata": {
        "id": "zwhntgiZs0FU",
        "colab_type": "code",
        "outputId": "5dc2142d-1e39-41b2-840f-7dc154052df6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "cell_type": "code",
      "source": [
        "o1 = torch.Tensor(datanp)\n",
        "o2 = torch.tensor(datanp)\n",
        "o3 = torch.as_tensor(datanp)\n",
        "o4 = torch.from_numpy(datanp)\n",
        "\n",
        "print(o1)\n",
        "print(o2)\n",
        "print(o3)\n",
        "print(o4)\n",
        "\n",
        "print(o1.dtype)\n",
        "print(o2.dtype)\n",
        "print(o3.dtype)\n",
        "print(o4.dtype)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1., 2., 3.])\n",
            "tensor([1, 2, 3])\n",
            "tensor([1, 2, 3])\n",
            "tensor([1, 2, 3])\n",
            "torch.float32\n",
            "torch.int64\n",
            "torch.int64\n",
            "torch.int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AdI6CvUMtBh2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "All of the options (o1, o2, o3, o4) appear to have produced the same tensors except for the first one. \n",
        "\n",
        "The first option (o1) has dots after the number indicating that the numbers are floats, while the next three options have a type of int64. \n",
        "\n",
        "The difference here arises in the fact that the **torch.Tensor() constructor uses the default dtype** when building the tensor.\n",
        "\n",
        "The other calls **choose a dtype based on the incoming data**. This is called **type inference**. The dtype is inferred based on the incoming data. Note that the **dtype** can also be **explicitly set** for these calls by specifying the dtype as an argument: "
      ]
    },
    {
      "metadata": {
        "id": "UhtfE7B7tFKg",
        "colab_type": "code",
        "outputId": "ba7672c4-fcf6-4a22-8a32-be1f070c0c11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "print(torch.tensor(datanp, dtype=torch.float32))\n",
        "print(torch.as_tensor(datanp, dtype=torch.float32))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1., 2., 3.])\n",
            "tensor([1., 2., 3.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0SRc7fkuukvu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "With torch.Tensor(), we are unable to pass a dtype to the constructor. This is an example of the torch.Tensor() constructor lacking in configuration options. This is one of the reasons to go with the torch.tensor() factory function for creating our tensors. "
      ]
    },
    {
      "metadata": {
        "id": "WifVAxcTu3-e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Memory sharing in tensors**\n",
        "\n",
        "Note that originally, we had datanp[0]=1, and also note that we only changed the data in the original numpy.ndarray. Notice we didn't explicity make any changes to our tensors (o1, o2, o3, o4).\n",
        "\n",
        "However, after setting datanp[0]=0, we can see some of our tensors have changes."
      ]
    },
    {
      "metadata": {
        "id": "o7RkQdDrxOzu",
        "colab_type": "code",
        "outputId": "1909973c-b818-4e12-8e6a-1fbe1fd8079f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "cell_type": "code",
      "source": [
        "print('old:', datanp)\n",
        "datanp[0] = 0\n",
        "print('new:', datanp)\n",
        "\n",
        "print(o1)\n",
        "print(o2)\n",
        "print(o3)\n",
        "print(o4)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "old: [1 2 3]\n",
            "new: [0 2 3]\n",
            "tensor([1., 2., 3.])\n",
            "tensor([1, 2, 3])\n",
            "tensor([0, 2, 3])\n",
            "tensor([0, 2, 3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZpEC6qaMxkeB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The first two o1 and o2 still have the original value of 1 for index 0, while the second two o3 and o4 have the new value of 0 for index 0.\n",
        "\n",
        "This happens because **torch.Tensor() and torch.tensor() copy** their input data while **torch.as_tensor() and torch.from_numpy() share** their input data in memory with the original input object. \n",
        "\n",
        "This sharing just means that the actual data in memory exists in a single place. As a result, any changes that occur in the underlying data will be reflected in both objects.\n",
        "\n",
        "Sharing data is more efficient and uses less memory than copying data because the data is not written to two locations in memory. \n",
        "\n",
        "This establishes that torch.as_tensor() and torch.from_numpy() both share memory with their input data. \n",
        "\n",
        "The torch.from_numpy() function only accepts numpy.ndarrays, while the torch.as_tensor() function accepts a wide variety of Python array-like objects including other PyTorch tensors. For this reason, torch.as_tensor() is the winning choice in the memory sharing game.\n",
        "\n",
        " Given all of these details, these two are the best options:\n",
        "\n",
        "* torch.tensor()\n",
        "* torch.as_tensor()\n",
        "\n",
        "The torch.tensor() call is the sort of go-to call, while torch.as_tensor() should be employed when tuning our code for performance. "
      ]
    },
    {
      "metadata": {
        "id": "NTDvpDn3Ddqj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Rank, Axes and Shape of Tensors"
      ]
    },
    {
      "metadata": {
        "id": "MaIwVTSYkWrh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " The concepts of rank, axes, and shape are the tensor attributes that will concern us most in deep learning.\n",
        "\n",
        "* Rank\n",
        "* Axes\n",
        "* Shape\n",
        "\n",
        "These concepts build on one another starting with rank, then axes, and building up to shape, so keep any eye out for this relationship between these three. "
      ]
    },
    {
      "metadata": {
        "id": "qhUJbp7elEx9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Rank of a tensor**\n",
        "\n",
        "The rank of a tensor refers to the number of dimensions present within the tensor. Suppose we are told that we have a rank-2 tensor. This means all of the following:\n",
        "\n",
        "* We have a matrix\n",
        "* We have a 2d-array\n",
        "* We have a 2d-tensor\n",
        "\n",
        "We are introducing the word **rank** here because it is commonly used in deep learning when referring to the number of **dimensions** present within a given tensor. \n",
        "\n",
        "The rank of a tensor tells us **how many indexes are required to access (refer to) a specific data element** contained within the tensor data structure.\n",
        "\n",
        "A tensor's rank tells us how many indexes are needed to refer to a specific element within the tensor."
      ]
    },
    {
      "metadata": {
        "id": "bHt12NttDhRS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The dimension and size of any tensor can be found using the methods shown below."
      ]
    },
    {
      "metadata": {
        "id": "cyvLSVsUDjur",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "s1d = torch.tensor([1,9,1,1])\n",
        "\n",
        "s2d = torch.tensor([[1,9,1,1], [4,5,6,7]])\n",
        "\n",
        "#s3d = torch.tensor([ [[1,9,1,1],[4,5,6,7]] , [[2,10,2,2],[8,9,10,11]] ])\n",
        "\n",
        "s3d = torch.tensor([\n",
        "                     [\n",
        "                      [1,9,1,1],\n",
        "                      [4,5,6,7]\n",
        "                     ],\n",
        "                     [\n",
        "                      [2,10,2,2],\n",
        "                      [8,9,10,11]\n",
        "                     ]\n",
        "                   ])\n",
        "s4d = torch.tensor([[[[4,1]]]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EbYXznwmDmqR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*The dimension corresponds to the number of nested list sets*"
      ]
    },
    {
      "metadata": {
        "id": "M5LjXeXBDyUb",
        "colab_type": "code",
        "outputId": "5f76016d-77d9-4f09-d1e3-7c0896c40a5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"The dimension of tensors s1d, s2d, s3d,s4d are:\")\n",
        "print(s1d.ndimension(),s2d.ndimension(),s3d.ndimension(),s4d.ndimension())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The dimension of tensors s1d, s2d, s3d,s4d are:\n",
            "1 2 3 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YrfMIlmBnfX7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Axes of a tensor**\n",
        "\n",
        "If we have a tensor, and we want to refer to a specific dimension, we use the word axis in deep learning. \n",
        "\n",
        "An **axis** of a tensor is a **specific dimension of a tensor**.\n",
        "\n",
        "If we say that a tensor is a **rank 2 tensor**, we mean that the tensor has 2 dimensions, or **equivalently**, the tensor has **two axes**.\n",
        "\n",
        "Elements are said to exist or run along an axis. This running is constrained by the length of each axis. \n",
        "\n",
        "The **length of each axis** tells us how many **indexes are available along each axis**."
      ]
    },
    {
      "metadata": {
        "id": "LOwY-fpGnpcM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dd = torch.tensor([\n",
        "[1,2,3],\n",
        "[4,5,6],\n",
        "[7,8,9]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HicRqIZln8rG",
        "colab_type": "code",
        "outputId": "fb2ad5bf-8f3a-473e-e9c1-269979fb68f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "#Each element along the first axis, is an array:\n",
        "print(dd[0])\n",
        "print(dd[1])\n",
        "print(dd[2])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1, 2, 3])\n",
            "tensor([4, 5, 6])\n",
            "tensor([7, 8, 9])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zSalLJieoKKv",
        "colab_type": "code",
        "outputId": "2d1fbbe2-18d2-4f8e-a603-ccdec2986ff2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        }
      },
      "cell_type": "code",
      "source": [
        "#Each element along the second axis, is a number: \n",
        "print(dd[0][0])\n",
        "print(dd[1][0])\n",
        "print(dd[2][0])\n",
        "print(dd[0][1])\n",
        "print(dd[1][1])\n",
        "print(dd[2][1])\n",
        "print(dd[0][2])\n",
        "print(dd[1][2])\n",
        "print(dd[2][2])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(1)\n",
            "tensor(4)\n",
            "tensor(7)\n",
            "tensor(2)\n",
            "tensor(5)\n",
            "tensor(8)\n",
            "tensor(3)\n",
            "tensor(6)\n",
            "tensor(9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ejj9UqBsoTA1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Note that, with tensors, the elements of the last axis are always numbers (scalar). Every other axis will contain n-dimensional arrays. \n",
        "\n",
        "The rank of a tensor tells us how many axes a tensor has, and the length of these axes leads us to the very important concept known as the shape of a tensor. "
      ]
    },
    {
      "metadata": {
        "id": "6G6jVygFmbpm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Shape of a tensor**\n",
        "\n",
        "The **shape** of a tensor is determined by the **length of each axis**, so if we know the shape of a given tensor, then we know the length of each axis, and this tells us how many indexes are available along each axis.\n",
        "\n",
        "The shape of a tensor gives us the length of each axis of the tensor. Note that, in PyTorch, size and shape of a tensor are the same thing. "
      ]
    },
    {
      "metadata": {
        "id": "VQ_bzFsEoq6J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "s1d = torch.tensor([1,9,1,1])\n",
        "\n",
        "s2d = torch.tensor([[1,9,1,1], [4,5,6,7]])\n",
        "\n",
        "#s3d = torch.tensor([ [[1,9,1,1],[4,5,6,7]] , [[2,10,2,2],[8,9,10,11]] ])\n",
        "\n",
        "s3d = torch.tensor([\n",
        "                     [\n",
        "                      [1,9,1,1],\n",
        "                      [4,5,6,7]\n",
        "                     ],\n",
        "                     [\n",
        "                      [2,10,2,2],\n",
        "                      [8,9,10,11]\n",
        "                     ]\n",
        "                   ])\n",
        "s4d = torch.tensor([[[[4,1]]]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n9JbUaaODzCa",
        "colab_type": "code",
        "outputId": "1d0d774c-f0e1-4976-f230-2683e59f1060",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"The size tensor s3d i.e, interpreted as two sets of [2,4] tensors:\")\n",
        "s3d.size()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The size tensor s3d i.e, interpreted as two sets of [2,4] tensors:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 2, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "metadata": {
        "id": "xBb68q-aD1F5",
        "colab_type": "code",
        "outputId": "9045a68a-a43e-4fe5-f511-2674cfaed7bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"The shape of tensor s3d is:\")\n",
        "s3d.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The shape of tensor s3d is:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 2, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "metadata": {
        "id": "Hq67lvKoD5mX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**.shape is an alias for .size()**"
      ]
    },
    {
      "metadata": {
        "id": "UxfutCz6EUVs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Converting Tensors of Pandas, Numpy"
      ]
    },
    {
      "metadata": {
        "id": "cStRnTQL45TJ",
        "colab_type": "code",
        "outputId": "f10fa913-0eb0-44fa-9268-b3c864ac8df9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "nparray = np.array([0.0,1.0,2.0,3.0,4.0])\n",
        "print(\"\\n Original Array in numpy: \",nparray)\n",
        "\n",
        "torcharray = torch.from_numpy(nparray)\n",
        "print(\"\\n Torch Array from numpy: \",torcharray)\n",
        "\n",
        "back_to_nparray = torcharray.numpy()\n",
        "print(\"\\n Numpy array converted back from torch: \",back_to_nparray)\n",
        "\n",
        "pdarray = pd.Series([19.0,0.1,5.8,2.3])\n",
        "print(\"\\n Original Array in Pandas: \\n\",pdarray)\n",
        "\n",
        "ptorcharray = torch.from_numpy(pdarray.values) #pdarray.values returns a numpy array\n",
        "print(\"\\n Torch Array from numpy: \",ptorcharray)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Original Array in numpy:  [0. 1. 2. 3. 4.]\n",
            "\n",
            " Torch Array from numpy:  tensor([0., 1., 2., 3., 4.], dtype=torch.float64)\n",
            "\n",
            " Numpy array converted back from torch:  [0. 1. 2. 3. 4.]\n",
            "\n",
            " Original Array in Pandas: \n",
            " 0    19.0\n",
            "1     0.1\n",
            "2     5.8\n",
            "3     2.3\n",
            "dtype: float64\n",
            "\n",
            " Torch Array from numpy:  tensor([19.0000,  0.1000,  5.8000,  2.3000], dtype=torch.float64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "avHJ7IaWEX_7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Tensor Indexing and Slicing\n",
        " \n",
        "Pytorch supports indexing and slicing of arrays just like sequences in python. Slices reference a subset of the array without copying the underlying data"
      ]
    },
    {
      "metadata": {
        "id": "nfMgA9HPEbKJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We use rectangular brackets to access different elements of a tensor. \n",
        "\n",
        "Depending on the number of dimensions of the tensor, we use the following notation for indexing\n",
        "\n",
        "$$ tensor[dim_0, dim_1, dim_2,\\cdots,dim_n]$$"
      ]
    },
    {
      "metadata": {
        "id": "fGXSena3DSlR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Similary, slices are taken depending on the rank of the tensor.\n",
        "\n",
        "All index values of a slice are optional, and the defaults values are the start of the sequence, the last item in the sequence, and a default step size of one where it cannot be zero. A negative step size can reverse the order of the output result.\n",
        "\n",
        "$$tensor[slice_0,slice_1,slice_2,\\cdots,slice_n]$$\n",
        "\n",
        "where\n",
        "\n",
        "* [start: end: step] - Slice from [start] to [end-1] but step ahead based on step values\n",
        "\n",
        "* [ : : ] – All elements of the entire tensor\n",
        "* n representing the number of dimensions of original tensor\n"
      ]
    },
    {
      "metadata": {
        "id": "CkIKSF44V2n9",
        "colab_type": "code",
        "outputId": "d1c2df61-55b4-45c7-eec3-b2fbcdb4395e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        }
      },
      "cell_type": "code",
      "source": [
        "#!pip3 install torch torchvision\n",
        "import torch\n",
        "a = torch.tensor([[9,8,7],[6,5,4],[3,2,1]])\n",
        "#[ [a[0][0],a[0][1],a[0][2]], [a[1][0],a[1][1],a[1][2]],[a[2][0],a[2][1],a[2][2]] ]\n",
        "print(\"\\nGiven array: \\n\",a)\n",
        "\n",
        "print(\"\\nIndexing: \")\n",
        "print(a[0,0])  # Single bracket notation for accessing individual elements\n",
        "print(a[0][0]) # Double bracket notation for accessing individual elements\n",
        "print(a[0])    \n",
        "print(a[1])\n",
        "\n",
        "print(\"\\nSlicing: \")\n",
        "slice1 = a[0:3,0:2]\n",
        "print(slice1)\n",
        "\n",
        "print(\"\\n\")\n",
        "slice1[0][0]=0\n",
        "print(slice1)\n",
        "\n",
        "print(\"\\nMixing Indexing and Slicing:\")\n",
        "print(a[0,0:2])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Given array: \n",
            " tensor([[9, 8, 7],\n",
            "        [6, 5, 4],\n",
            "        [3, 2, 1]])\n",
            "\n",
            "Indexing: \n",
            "tensor(9)\n",
            "tensor(9)\n",
            "tensor([9, 8, 7])\n",
            "tensor([6, 5, 4])\n",
            "\n",
            "Slicing: \n",
            "tensor([[9, 8],\n",
            "        [6, 5],\n",
            "        [3, 2]])\n",
            "\n",
            "\n",
            "tensor([[0, 8],\n",
            "        [6, 5],\n",
            "        [3, 2]])\n",
            "\n",
            "Mixing Indexing and Slicing:\n",
            "tensor([0, 8])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "00kS6lwqEf7F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Indexing** gets an item but destroys the data structure as returned item is not the original tensor. Simply put, it **does not preserve** the dimensionality of the original tensor.\n",
        "\n",
        "**Slicing preserves** the data structure and returned item has the same of kind structure that the original contained.\n",
        "\n",
        "**Slices** reference a subset of the array **without copying** the underlying data.\n",
        "\n",
        "Since slices are references to memory in original array,  changing values in a slice also changes the original array.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "iQHDB-EWPRaR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Fancy indexing used when ordinary slicing and indexing operations cannot retrieve a value. Creates new copies.\n",
        "\n",
        "* By specifying non-sequential integers locations of data \n",
        "* By applying masks\n",
        "\n",
        "Fancy indexing is like the simple indexing, but we pass arrays of indices in place of single scalars.\n"
      ]
    },
    {
      "metadata": {
        "id": "FZguh90p9xgK",
        "colab_type": "code",
        "outputId": "35fd9550-ba76-422c-826e-bf3e73b1854e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "#!pip3 install torch torchvision\n",
        "import torch\n",
        "fancy = torch.tensor([78, 55, 82, 93])\n",
        "print(fancy[0],fancy[1],fancy[2])   # Suppose we want to access three different elements. We could do it like this.\n",
        "\n",
        "print([fancy[0],fancy[1],fancy[2]]) # Alternatively, we can pass a single list or array of indices to obtain the same\n",
        "\n",
        "fancy_index = [0,1,2]\n",
        "print(fancy[fancy_index])           # When using fancy indexing, the shape of the result reflects the shape of the index arrays rather than the shape of the array being indexed"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(78) tensor(55) tensor(82)\n",
            "[tensor(78), tensor(55), tensor(82)]\n",
            "tensor([78, 55, 82])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gnluo00R5dAi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Mathematical operations on tensors can be performed element wise, and flat arrays can be returned which can used as masks.\n"
      ]
    },
    {
      "metadata": {
        "id": "77L87X2m5tIc",
        "colab_type": "code",
        "outputId": "0e65a24a-5494-4c1b-d87d-49af82012d90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "print(fancy>78)\n",
        "print(fancy[fancy>78])\n",
        "\n",
        "print(fancy.gt(78))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0, 0, 1, 1], dtype=torch.uint8)\n",
            "tensor([82, 93])\n",
            "tensor([0, 0, 1, 1], dtype=torch.uint8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xHvAtKDq7XKj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can \n",
        "\n",
        "* Combine fancy indexing with simple indices\n",
        "\n",
        "* Combine fancy indexing with slicing\n",
        "\n",
        "* Combine fancy indexing with masking"
      ]
    },
    {
      "metadata": {
        "id": "nCFg8xFA7JB0",
        "colab_type": "code",
        "outputId": "73648d9d-b630-4989-b78b-1e49ba873669",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "cell_type": "code",
      "source": [
        "r = torch.tensor([[0.0,1.0,2.0,3.0],\n",
        "       [4.0,5.0,6.0,7.0],\n",
        "       [8.0,9.0,10.0,11.0]])\n",
        "\n",
        "print(r)\n",
        "print(\"\\nCombine fancy indexing and simple indices :\", r[2, [2, 0, 1]])\n",
        "print(\"\\nCombine fancy indexing with slicing: \", r[1:, [2, 0, 1]])\n",
        "#print(\"\\nCombine fancy indexing with masking: \",)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.,  1.,  2.,  3.],\n",
            "        [ 4.,  5.,  6.,  7.],\n",
            "        [ 8.,  9., 10., 11.]])\n",
            "\n",
            "Combine fancy indexing and simple indices : tensor([10.,  8.,  9.])\n",
            "\n",
            "Combine fancy indexing with slicing:  tensor([[ 6.,  4.,  5.],\n",
            "        [10.,  8.,  9.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "807gxRxQEkG0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Tensor Functions"
      ]
    },
    {
      "metadata": {
        "id": "65srFiaUyowj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In practice, one will most often want to use one of PyTorch’s functions that return tensors initialized in a certain manner, such as:\n",
        "\n",
        "1. torch.rand: values initialized from a random uniform distribution,\n",
        "2. torch.randn: values initialized from a random normal distribution,\n",
        "3. torch.eye(n): an $n×n$ identity matrix,\n",
        "4. torch.from_numpy(ndarray): a PyTorch tensor from a NumPy ndarray,\n",
        "5. torch.linspace(start, end, steps): a 1-D tensor with steps values spaced linearly between start and end,\n",
        "6. torch.ones : a tensor with ones everywhere,\n",
        "7. torch.zeros_like(other) : a tensor with the same shape as other and zeros everywhere,\n",
        "8. torch.arange(start, end, step): a 1-D tensor with values filled from a range."
      ]
    },
    {
      "metadata": {
        "id": "lXV-3r5lElnJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Tensor Operations"
      ]
    },
    {
      "metadata": {
        "id": "HND0WEC_y-Ux",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "PyTorch tensors provide a very rich API for combination with other tensors as well as in-place mutation. \n",
        "\n",
        "Also like NumPy, unary and binary operations can usually be performed via functions in the torch module, like torch.add(x, y), or directly via methods on the tensor objects, like x.add(y). \n",
        "\n",
        "For the usual suspects, operator overloads like x + y exist. Furthermore, many functions have in-place alternatives that will mutate the receiver instance rather than creating a new tensor. These functions have the same name as the out-of-place variants, but are suffixed with an underscore, e.g. x.add_(y).\n",
        "\n",
        "A selection of operations includes:\n",
        "\n",
        "1. torch.add(x, y): elementwise addition,\n",
        "2. torch.mm(x, y): matrix multiplication (not matmul or dot),\n",
        "3. torch.mul(x, y): elementwise multiplication,\n",
        "4. torch.exp(x): elementwise exponential,\n",
        "5. torch.pow(x, power): elementwise exponentiation,\n",
        "6. torch.sqrt(x): elementwise squaring,\n",
        "7. torch.sqrt_(x): in-place elementwise squaring,\n",
        "8. torch.sigmoid(x): elementwise sigmoid.\n",
        "9. torch.cumprod(x): product of all values,\n",
        "10. torch.sum(x): sum of all values,\n",
        "11. torch.std(x): standard deviation of all values,\n",
        "12. torch.mean(x): mean of all values.\n",
        "\n",
        "Tensors support many of the familiar semantics of NumPy ndarray’s, such as broadcasting, advanced (fancy) indexing (x[x > 5]) and elementwise relational operators (x > y). \n",
        "\n",
        "PyTorch tensors can also be converted to NumPy ndarray’s directly via the torch.Tensor.numpy() function. \n",
        "\n",
        "Finally, since the primary improvement of PyTorch tensors over NumPy ndarrays is supposed to be GPU acceleration, there is also a torch.Tensor.cuda() function, which will copy the tensor memory onto a CUDA-capable GPU device, if one is available."
      ]
    },
    {
      "metadata": {
        "id": "MzDxcPWO04bF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Tensor operation types\n",
        "\n",
        "Before we dive in with specific tensor operations, let’s get a quick overview of the landscape by looking at the main operation categories that encompass the operations we’ll cover. We have the following high-level categories of operations:\n",
        "\n",
        "1. Reshaping operations\n",
        "2. Element-wise operations\n",
        "3. Reduction operations\n",
        "4. Access operations\n",
        "\n",
        "There are a lot of individual operations out there, so much so that it can sometimes be intimidating when you're just beginning, but grouping similar operations into categories based on their likeness can help make learning about tensor operations more manageable. "
      ]
    },
    {
      "metadata": {
        "id": "FvVLVd3yVp8F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Reshaping operations\n",
        "\n",
        "The shape also encodes all of the relevant information about axes, rank, and therefore indexes. Additionally, one of the types of operations we must perform frequently when we are programming our neural networks is called reshaping.\n",
        "\n",
        "Reshaping changes the grouping of the terms but does not change the underlying terms themselves.\n",
        "\n",
        "For reshaping the product of the component values in the shape must equal the total number of elements in the tensor so that there are enough positions inside the tensor data structure to contain all of the original data elements after the reshaping.\n",
        "\n",
        "Reshaping changes the shape but not the underlying data elements.\n"
      ]
    },
    {
      "metadata": {
        "id": "z_rXSDiyMhVJ",
        "colab_type": "code",
        "outputId": "94888f3f-d576-49b7-ef57-6d910ea28604",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "t = torch.tensor([\n",
        "    [1,1,1,1],\n",
        "    [2,2,2,2],\n",
        "    [3,3,3,3]\n",
        "], dtype=torch.float32)\n",
        "\n",
        "print(t.size()) \n",
        "print(t.shape)  #shape is an alias for size()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 4])\n",
            "torch.Size([3, 4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9FeW8TueM_WW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Typically, after we know a tensor’s shape, we can deduce a couple of things. First, we can deduce the tensor's rank. The rank of a tensor is equal to the length of the tensor's shape.\n",
        "\n",
        "We can also deduce the number of elements contained within the tensor. The number of elements inside a tensor (12 in our case) is equal to the product of the shape's component values."
      ]
    },
    {
      "metadata": {
        "id": "Dkyxo9O7NF7d",
        "colab_type": "code",
        "outputId": "4205dd36-62e2-4e1f-b4f0-e601f3e550ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "print(len(t.shape))\n",
        "print(torch.tensor(t.shape).prod())\n",
        "print(t.numel()) #In PyTorch, there is a dedicated function for determining the number of elements inside a tensor"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "tensor(12)\n",
            "12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XGmgOLt8NuNg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Since the above tensor has 12 elements, so any reshaping must account for exactly 12 elements. "
      ]
    },
    {
      "metadata": {
        "id": "5mEdVarfNxo9",
        "colab_type": "code",
        "outputId": "2848f0b8-9b9d-4b3a-8189-daa4cd05e8ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 531
        }
      },
      "cell_type": "code",
      "source": [
        "print(t.reshape([1,12]))\n",
        "print(t.reshape([2,6]))\n",
        "print(t.reshape([3,4]))\n",
        "print(t.reshape([4,3]))\n",
        "print(t.reshape(6,2))\n",
        "print(t.reshape(12,1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.]])\n",
            "tensor([[1., 1., 1., 1., 2., 2.],\n",
            "        [2., 2., 3., 3., 3., 3.]])\n",
            "tensor([[1., 1., 1., 1.],\n",
            "        [2., 2., 2., 2.],\n",
            "        [3., 3., 3., 3.]])\n",
            "tensor([[1., 1., 1.],\n",
            "        [1., 2., 2.],\n",
            "        [2., 2., 3.],\n",
            "        [3., 3., 3.]])\n",
            "tensor([[1., 1.],\n",
            "        [1., 1.],\n",
            "        [2., 2.],\n",
            "        [2., 2.],\n",
            "        [3., 3.],\n",
            "        [3., 3.]])\n",
            "tensor([[1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [1.],\n",
            "        [2.],\n",
            "        [2.],\n",
            "        [2.],\n",
            "        [2.],\n",
            "        [3.],\n",
            "        [3.],\n",
            "        [3.],\n",
            "        [3.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gXb0IpJNAuJ-",
        "colab_type": "code",
        "outputId": "b6c5199b-e794-4c53-df1d-5f5634faf2d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        }
      },
      "cell_type": "code",
      "source": [
        "reshapea = t.reshape([1,12])\n",
        "print(reshapea)\n",
        "print(\"\\n\")\n",
        "reshapea[0][0]=0\n",
        "print(reshapea)\n",
        "print(t)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.]])\n",
            "\n",
            "\n",
            "tensor([[0., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.]])\n",
            "tensor([[0., 1., 1., 1.],\n",
            "        [2., 2., 2., 2.],\n",
            "        [3., 3., 3., 3.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-WJABYPUOLyQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " PyTorch has another function called view() that does the same thing as the reshape() function. We can use -1 in the argments if we dont know the actual size of the tensor."
      ]
    },
    {
      "metadata": {
        "id": "9IrXz9jJ3Ity",
        "colab_type": "code",
        "outputId": "05c21f8d-1be6-4cae-8a9b-ec0d9eab215c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        }
      },
      "cell_type": "code",
      "source": [
        "#!pip3 install torch torchvision\n",
        "import torch\n",
        "v = torch.Tensor([9,8,7,6,5,4])\n",
        "v_col= v.view(6,1)\n",
        "print(\"\\nView 6x1: \\n\",v_col)\n",
        "v_col1= v.view(3,-1)\n",
        "print(\"\\nView 3x2: \\n\",v_col1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "View 6x1: \n",
            " tensor([[9.],\n",
            "        [8.],\n",
            "        [7.],\n",
            "        [6.],\n",
            "        [5.],\n",
            "        [4.]])\n",
            "\n",
            "View 3x2: \n",
            " tensor([[9., 8.],\n",
            "        [7., 6.],\n",
            "        [5., 4.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DEEfFtyhBDm6",
        "colab_type": "code",
        "outputId": "b53f4901-5170-4a68-c2e0-cec37141ddfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "cell_type": "code",
      "source": [
        "print(v)\n",
        "v_col1[0][0]=0\n",
        "print(v_col1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([9., 8., 7., 6., 5., 4.])\n",
            "tensor([[0., 8.],\n",
            "        [7., 6.],\n",
            "        [5., 4.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xj1modrqOUlF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can also change the shape of our tensors is by squeezing and unsqueezing them.\n",
        "\n",
        "* Squeezing a tensor **removes** the **dimensions** or axes that have a **length of one**.\n",
        "* Unsqueezing a tensor adds a dimension with a length of one.\n",
        "\n",
        "These functions allow us to expand or shrink the rank (number of dimensions) of our tensor."
      ]
    },
    {
      "metadata": {
        "id": "nm-xH2s6OuXs",
        "colab_type": "code",
        "outputId": "046bf093-1ba5-4345-a7f5-e5ff3f404592",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "cell_type": "code",
      "source": [
        "print(t.reshape([1,12]))\n",
        "print(t.reshape([1,12]).shape)\n",
        "print(t.reshape([1,12]).squeeze())\n",
        "print(t.reshape([1,12]).squeeze().shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.]])\n",
            "torch.Size([1, 12])\n",
            "tensor([0., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.])\n",
            "torch.Size([12])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "D9YxdiRYOzaC",
        "colab_type": "code",
        "outputId": "65c3da7a-633f-495b-a71b-dcb940fae5d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "print(t.reshape([1,12]).squeeze().unsqueeze(dim=0))\n",
        "print(t.reshape([1,12]).squeeze().unsqueeze(dim=0).shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.]])\n",
            "torch.Size([1, 12])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yIdwbHiDCBeY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Flatten a tensor**\n",
        "\n",
        "A flatten operation on a tensor reshapes the tensor to have a shape that is equal to the number of elements contained in the tensor. This is the same thing as a 1d-array of elements.\n",
        "\n",
        "Flattening a tensor means to remove all of the dimensions except for one. "
      ]
    },
    {
      "metadata": {
        "id": "khNxsJsnCNRi",
        "colab_type": "code",
        "outputId": "67ec64a3-eb30-4d57-d279-1230bf50eef3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        }
      },
      "cell_type": "code",
      "source": [
        "f = torch.ones(4, 3)\n",
        "print(f)\n",
        "print(torch.flatten(f)) #https://pytorch.org/docs/master/torch.html\n",
        "\n",
        "print(\"\\n\")\n",
        "f1 = torch.tensor([[[1, 2],\n",
        "                   [3, 4]],\n",
        "                   [[5, 6],\n",
        "                   [7, 8]]])\n",
        "print(\"Original Array: \\n\",f1)\n",
        "\n",
        "print(\"\\n\")\n",
        "print(torch.flatten(f1))\n",
        "\n",
        "print(torch.flatten(f1, start_dim=1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.]])\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
            "\n",
            "\n",
            "Original Array: \n",
            " tensor([[[1, 2],\n",
            "         [3, 4]],\n",
            "\n",
            "        [[5, 6],\n",
            "         [7, 8]]])\n",
            "\n",
            "\n",
            "tensor([1, 2, 3, 4, 5, 6, 7, 8])\n",
            "tensor([[1, 2, 3, 4],\n",
            "        [5, 6, 7, 8]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PzLE4gkETNjU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Most times we deal with batches of color images and we don’t want to flatten the whole tensor. We only want to flatten the image tensors within the batch tensor leaving the batch dimension intact\n",
        "\n",
        "$$(Batch Size,\\; Channels,\\; Height,\\; Width) $$\n",
        "\n",
        "We could thus do $flatten(start\\_dim=1)$ which tells the flatten() method which axis it should start the flatten operation. \n",
        "\n",
        "The one here is an index, so it’s the second axis which is the color channel axis. We **skip over the batch axis **so to speak, leaving it intact. "
      ]
    },
    {
      "metadata": {
        "id": "8pJRlIS4G7Dc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Concatenating tensors **"
      ]
    },
    {
      "metadata": {
        "id": "1oLrWYJSG_5S",
        "colab_type": "code",
        "outputId": "4a08d5f6-1047-4df2-dc5f-fe4ccd948e87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "cell_type": "code",
      "source": [
        "t1 = torch.tensor([\n",
        "    [1,2],\n",
        "    [3,4]\n",
        "])\n",
        "t2 = torch.tensor([\n",
        "    [5,6],\n",
        "    [7,8]\n",
        "])\n",
        "\n",
        "\n",
        "# We can combine t1 and t2 row-wise (axis-0) in the following way: \n",
        "print(torch.cat((t1, t2), dim=0))\n",
        "\n",
        "# We can combine them column-wise (axis-1) like this: \n",
        "print(torch.cat((t1, t2), dim=1))\n",
        "\n",
        "print(torch.cat((t1, t2), dim=0).shape)\n",
        "\n",
        "print(torch.cat((t1, t2), dim=1).shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1, 2],\n",
            "        [3, 4],\n",
            "        [5, 6],\n",
            "        [7, 8]])\n",
            "tensor([[1, 2, 5, 6],\n",
            "        [3, 4, 7, 8]])\n",
            "torch.Size([4, 2])\n",
            "torch.Size([2, 4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IFuHVWulcZyv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "torch.cat() concatenate tensors along an existing dimension and torch.stack() stack tensors along a new dimension.\n",
        "\n",
        "torch.cat and torch.stack requires the arguments to have the same shape (except along the concatenating dimension). They don't implement broadcasting. Pytorch don't enforce this sufficiently, which leads to the incorrect values."
      ]
    },
    {
      "metadata": {
        "id": "MWMC8IZzcg74",
        "colab_type": "code",
        "outputId": "93cf6af3-1fca-4949-9a4b-1b11e0715414",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "a = torch.randn(1, 4)\n",
        "b = torch.randn(1, 4)\n",
        "print(a)\n",
        "print(b)\n",
        "\n",
        "print(\"\\n\")\n",
        "abcat = torch.cat((a, b), 0)\n",
        "print(abcat)\n",
        "print(abcat.shape)\n",
        "\n",
        "print(\"\\n\")\n",
        "abstack = torch.stack((a, b),0) # The last element is the dimension to insert. Has to be between 0 and the number of dimensions of concatenated tensors (inclusive)\n",
        "print(abstack)\n",
        "print(abstack.shape) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.4219, -1.0212,  0.0908, -0.0733]])\n",
            "tensor([[ 0.8556,  1.5097, -1.9209, -1.5583]])\n",
            "\n",
            "\n",
            "tensor([[-0.4219, -1.0212,  0.0908, -0.0733],\n",
            "        [ 0.8556,  1.5097, -1.9209, -1.5583]])\n",
            "torch.Size([2, 4])\n",
            "\n",
            "\n",
            "tensor([[[-0.4219, -1.0212,  0.0908, -0.0733]],\n",
            "\n",
            "        [[ 0.8556,  1.5097, -1.9209, -1.5583]]])\n",
            "torch.Size([2, 1, 4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "MU_03lLVgcBk",
        "colab_type": "code",
        "outputId": "87669d53-b6f7-488f-a74a-a7d8b2b6f32c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "cell_type": "code",
      "source": [
        "tensor_one = torch.tensor([[1,2,3],[4,5,6]])\n",
        "print(tensor_one.shape)\n",
        "\n",
        "tensor_two = torch.tensor([[7,8,9],[10,11,12]])\n",
        "print(tensor_two.shape)\n",
        "\n",
        "tensor_tre = torch.tensor([[13,14,15],[16,17,18]])\n",
        "print(tensor_tre.shape)\n",
        "\n",
        "tensor_list = [tensor_one, tensor_two, tensor_tre]\n",
        "\n",
        "stacked_tensor = torch.stack(tensor_list)\n",
        "print(stacked_tensor.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 3])\n",
            "torch.Size([2, 3])\n",
            "torch.Size([2, 3])\n",
            "torch.Size([3, 2, 3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ulBj4IIohGp0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Our initial three tensors were all of shape 2x3.  So the default of torch.stack is that it’s going to insert a new dimension in front of the 2x3, so we’re going to end up with a 3x2x3 tensor.\n",
        "\n",
        "The reason it’s 3 is because we have three tensors in this list we are converting to one tensor. "
      ]
    },
    {
      "metadata": {
        "id": "ptR9ObejVtoK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Element-wise operations\n",
        "\n",
        "An element-wise operation is an operation between two tensors that operates on corresponding elements within the respective tensors. \n",
        "\n",
        "Two elements are said to be corresponding if the two elements occupy the same position within the tensor. The position is determined by the indexes used to locate each element.\n"
      ]
    },
    {
      "metadata": {
        "id": "fwD_ZC_uXDZO",
        "colab_type": "code",
        "outputId": "fd075c43-c526-4947-b6a5-df89b6ef3785",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        }
      },
      "cell_type": "code",
      "source": [
        "ele = torch.tensor([\n",
        "    [1,2],\n",
        "    [3,4]],dtype=torch.float32)\n",
        "\n",
        "# (1) Using these symbolic operations:\n",
        "print(ele + 2)\n",
        "print(ele - 2)\n",
        "print(ele * 2)\n",
        "print(ele / 2)\n",
        "print(\"\\n\")\n",
        "\n",
        "# (2) these built-in tensor object methods:\n",
        "print(ele.add(2))\n",
        "print(ele.sub(2))\n",
        "print(ele.mul(2))\n",
        "print(ele.div(2))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[3., 4.],\n",
            "        [5., 6.]])\n",
            "tensor([[-1.,  0.],\n",
            "        [ 1.,  2.]])\n",
            "tensor([[2., 4.],\n",
            "        [6., 8.]])\n",
            "tensor([[0.5000, 1.0000],\n",
            "        [1.5000, 2.0000]])\n",
            "\n",
            "\n",
            "tensor([[3., 4.],\n",
            "        [5., 6.]])\n",
            "tensor([[-1.,  0.],\n",
            "        [ 1.,  2.]])\n",
            "tensor([[2., 4.],\n",
            "        [6., 8.]])\n",
            "tensor([[0.5000, 1.0000],\n",
            "        [1.5000, 2.0000]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lYTWsVaIX7lC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Comparison operations** are also element-wise. For a given comparison operations between tensors, a new tensor of the same shape is returned with each element containing either a 0 or a 1.\n",
        "\n",
        "* 0 if the comparison between corresponding elements is False.\n",
        "* 1 if the comparison between corresponding elements is True.\n"
      ]
    },
    {
      "metadata": {
        "id": "A_vGGKixYF1L",
        "colab_type": "code",
        "outputId": "3836d3c1-aa57-47bd-d04f-7ee3e2ae3209",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        }
      },
      "cell_type": "code",
      "source": [
        "t = torch.tensor([\n",
        "    [0,5,0],\n",
        "    [6,0,7],\n",
        "    [0,8,0]], dtype=torch.float32)\n",
        "\n",
        "print(t.eq(0))\n",
        "print(t.ge(0))\n",
        "print(t.gt(0))\n",
        "print(t.lt(0))\n",
        "print(t.le(7))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1, 0, 1],\n",
            "        [0, 1, 0],\n",
            "        [1, 0, 1]], dtype=torch.uint8)\n",
            "tensor([[1, 1, 1],\n",
            "        [1, 1, 1],\n",
            "        [1, 1, 1]], dtype=torch.uint8)\n",
            "tensor([[0, 1, 0],\n",
            "        [1, 0, 1],\n",
            "        [0, 1, 0]], dtype=torch.uint8)\n",
            "tensor([[0, 0, 0],\n",
            "        [0, 0, 0],\n",
            "        [0, 0, 0]], dtype=torch.uint8)\n",
            "tensor([[1, 1, 1],\n",
            "        [1, 1, 1],\n",
            "        [1, 0, 1]], dtype=torch.uint8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "x165Ad8gaPwd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " There are some other ways to refer to element-wise operations, and all of these mean the same thing:\n",
        "\n",
        " * Element-wise\n",
        " * Component-wise\n",
        " * Point-wise\n"
      ]
    },
    {
      "metadata": {
        "id": "WgIZktn6ajTY",
        "colab_type": "code",
        "outputId": "b635d269-6c8a-43ee-f005-71357ed70f29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        }
      },
      "cell_type": "code",
      "source": [
        "print(t.abs())\n",
        "print(t.sqrt())\n",
        "print(t.neg())\n",
        "print(t.neg().abs())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0., 5., 0.],\n",
            "        [6., 0., 7.],\n",
            "        [0., 8., 0.]])\n",
            "tensor([[0.0000, 2.2361, 0.0000],\n",
            "        [2.4495, 0.0000, 2.6458],\n",
            "        [0.0000, 2.8284, 0.0000]])\n",
            "tensor([[-0., -5., -0.],\n",
            "        [-6., -0., -7.],\n",
            "        [-0., -8., -0.]])\n",
            "tensor([[0., 5., 0.],\n",
            "        [6., 0., 7.],\n",
            "        [0., 8., 0.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aDZbLJmhawzY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Broadcasting of Tensors**\n",
        "\n",
        "Tensors of different dimensionality can be combined in the same expression. Tensors with smaller dimension are broadcasted to match the larger tensors, without copying data. Broadcasting has two rules.\n",
        "\n",
        "* Rule 1:  For broadcasting to be successful, the dimensions of the tensors need to be compatible. Two dimensions are **compatible** when they are **equal**\n",
        "\n",
        "* Rule 2: Two dimensions of different tensors are also compatible when one of the dimensions is 1. The resulting tensor size is actually the maximum size along each dimension of the input tensors. In other words, given x(3,4) and y (4,) the maximum size along each dimension of x and y is taken to make up the shape of the new, resulting tensor is of shape (3,4)"
      ]
    },
    {
      "metadata": {
        "id": "XnJW8CDAb_nm",
        "colab_type": "code",
        "outputId": "d2308c6b-ea5e-4ff3-c98c-0ea0f8b02c0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        }
      },
      "cell_type": "code",
      "source": [
        "x = torch.ones((3,4))\n",
        "y = torch.arange(4,dtype=torch.float32)\n",
        "print(x)\n",
        "print(y)\n",
        "print(\"\\n\")\n",
        "print(x-y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.]])\n",
            "tensor([0., 1., 2., 3.])\n",
            "\n",
            "\n",
            "tensor([[ 1.,  0., -1., -2.],\n",
            "        [ 1.,  0., -1., -2.],\n",
            "        [ 1.,  0., -1., -2.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dLLukBHehp3L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Reduction operations\n",
        "\n",
        "A reduction operation on a tensor is an operation that reduces the number of elements contained within the tensor. "
      ]
    },
    {
      "metadata": {
        "id": "MsOWuJzwhy3f",
        "colab_type": "code",
        "outputId": "b1222d89-619b-4427-c78e-93ad0f0e9976",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        }
      },
      "cell_type": "code",
      "source": [
        "red = torch.tensor([\n",
        "    [0,1,0],\n",
        "    [2,0,2],\n",
        "    [0,3,0]], dtype=torch.float32)\n",
        "\n",
        "print(red.sum())\n",
        "print(\"\\nThe number of elements in original tensor\",red.numel())\n",
        "\n",
        "print(\"\\nConcating Operations: \")\n",
        "print(red.sum().numel())\n",
        "print(red.sum().numel() < red.numel())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(8.)\n",
            "\n",
            "The number of elements in original tensor 9\n",
            "\n",
            "Concating Operations: \n",
            "1\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "q3H2gzKYjuqc",
        "colab_type": "code",
        "outputId": "fc28ce40-fd3a-40a9-c645-78b9a37ef3a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "cell_type": "code",
      "source": [
        "print(red.sum())\n",
        "print(red.prod())\n",
        "print(red.mean())\n",
        "print(red.std())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(8.)\n",
            "tensor(0.)\n",
            "tensor(0.8889)\n",
            "tensor(1.1667)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "B90Q5CJzltVn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Reduction by Axis**"
      ]
    },
    {
      "metadata": {
        "id": "2LZygwWfmABS",
        "colab_type": "code",
        "outputId": "d5987129-2ef8-430b-8a26-03b58a169e7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "ra = torch.tensor([\n",
        "    [1,1,1,1],\n",
        "    [2,2,2,2],\n",
        "    [3,3,3,3]], dtype=torch.float32)\n",
        "\n",
        "print(ra.size())     # 3 x 4 rank-2 tensor\n",
        "print(ra.sum(dim=0)) # Sum along the axis containing 3 elements i.e, column wise\n",
        "print(ra.sum(dim=1)) # Sum along the axis containing 4 elements i.e, row wise"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 4])\n",
            "tensor([6., 6., 6., 6.])\n",
            "tensor([ 4.,  8., 12.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "o2rBmLTqoR0U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Argmax tensor reduction operation**\n",
        "\n",
        "Argmax returns the index location of the maximum value inside a tensor. \n",
        "\n",
        "When we call the argmax() method on a tensor, the tensor is reduced to a new tensor that contains an index value indicating where the max value is inside the tensor\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "rLgpmJgVocNK",
        "colab_type": "code",
        "outputId": "129a658f-d881-4b9e-c3a1-a3cda251822e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "xa = torch.tensor([\n",
        "    [1,0,0,2],\n",
        "    [0,3,3,0],\n",
        "    [4,0,0,5]], dtype=torch.float32)\n",
        "\n",
        "print(xa.max())\n",
        "print(xa.argmax())\n",
        "print(xa.flatten())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(5.)\n",
            "tensor(11)\n",
            "tensor([1., 0., 0., 2., 0., 3., 3., 0., 4., 0., 0., 5.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hKw1P9N7oo6k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If we don’t specify an axis to the argmax() method, it returns the index location of the max value from the flattened tensor, which in this case is indeed 11. "
      ]
    },
    {
      "metadata": {
        "id": "IlUr2uO_q2ZF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Argmax along Specific axes**\n",
        "\n",
        "Notice how the call to the max() method returns two tensors. The first tensor contains the max values and the second tensor contains the index locations for the max values along dim0. \n",
        "\n",
        "The max values are determined by taking the element-wise maximum across each array running across the first axis."
      ]
    },
    {
      "metadata": {
        "id": "qvo_a6MkrBXp",
        "colab_type": "code",
        "outputId": "f4a93442-d517-4c95-8e59-49531ac64840",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "cell_type": "code",
      "source": [
        "print(xa.max(dim=0))\n",
        "print(xa.argmax(dim=0))\n",
        "print(xa.max(dim=1))\n",
        "print(xa.argmax(dim=1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(tensor([4., 3., 3., 5.]), tensor([2, 1, 1, 2]))\n",
            "tensor([2, 1, 1, 2])\n",
            "(tensor([2., 3., 5.]), tensor([3, 2, 3]))\n",
            "tensor([3, 2, 3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "I0-evvtmsBfC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Accessing elements inside tensors\n",
        "\n",
        "Operations can return scalar valued tensors. If we want to actually get the value as a number, we use the item() tensor method. This works for scalar valued tensors. \n",
        "\n",
        "When multiple values are returned, and we can access the numeric values by transforming the output tensor into a Python list or a NumPy array."
      ]
    },
    {
      "metadata": {
        "id": "_KTws0CXshse",
        "colab_type": "code",
        "outputId": "0c6d220d-89a4-40c2-b127-a9f6f654c2c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "cell_type": "code",
      "source": [
        "ta = torch.tensor([\n",
        "    [1,2,3],\n",
        "    [4,5,6],\n",
        "    [7,8,9]], dtype=torch.float32)\n",
        "print(ta.mean())\n",
        "print(ta.mean().item())\n",
        "\n",
        "print(ta.mean(dim=0).tolist())\n",
        "print(ta.mean(dim=0).numpy())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(5.)\n",
            "5.0\n",
            "[4.0, 5.0, 6.0]\n",
            "[4. 5. 6.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LB4Q1mUvtTMB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Additional operators"
      ]
    },
    {
      "metadata": {
        "id": "BFRg8M3ttaRW",
        "colab_type": "code",
        "outputId": "acc0dc38-b034-4be3-fc8c-088fab79b561",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "a = torch.randn(4)\n",
        "print(a)\n",
        "torch.clamp(a, min=0, max=1) # If less than zero make it zero, if greater than make it 1 and leave rest as is"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 0.3394, -1.3161, -1.3622,  0.0774])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.3394, 0.0000, 0.0000, 0.0774])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "metadata": {
        "id": "Az-SeZZcuJim",
        "colab_type": "code",
        "outputId": "5ee0304a-df26-4560-a291-9714135cff64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "a = torch.tensor([[2,4,3],[5,6,1]])\n",
        "b = torch.tensor([[10,11,10],[10,2,8]])\n",
        "torch.mul(a, b) # Each element of the tensor input is multiplied by each element of the Tensor other, must be broadcastable"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[20, 44, 30],\n",
              "        [50, 12,  8]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "metadata": {
        "id": "ky7zAYY10RKN",
        "colab_type": "code",
        "outputId": "3a18e865-7117-41b8-b80b-11b6d5be4d73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "torch.dot(torch.tensor([2, 3]), torch.tensor([2, 1])) # This function does not broadcast."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(7)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "metadata": {
        "id": "cz0znRNt0lYa",
        "colab_type": "code",
        "outputId": "9de77df6-6b2e-4591-fce8-353298bf4b56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "mat1 = torch.randn(2, 3)\n",
        "mat2 = torch.randn(3, 3)\n",
        "print(torch.mm(mat1, mat2)) # matrix multiplication This function does not broadcast. For broadcasting matrix products, use torch.matmul()."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-3.6598,  2.9365, -1.5345],\n",
            "        [-4.1402,  1.7081, -3.1929]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "doepEugNsKQU",
        "colab_type": "code",
        "outputId": "4e00d300-263d-46e2-b326-4101d3fe0c48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "print(mat1 @ mat2) # A @ B is the matrix product"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-3.6598,  2.9365, -1.5345],\n",
            "        [-4.1402,  1.7081, -3.1929]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ssQW7mCQsqNY",
        "colab_type": "code",
        "outputId": "6dc0bf55-6502-47ec-9e30-2b372d3c8a8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "mat1 = torch.randn(3, 3)\n",
        "mat2 = torch.randn(3, 3)\n",
        "print(mat1 * mat2) # A * B the element-wise product"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.6314,  0.2136, -0.4333],\n",
            "        [ 0.0046, -0.1410,  1.1838],\n",
            "        [ 0.9022, -0.0231,  0.0100]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sCq_mCxesHW9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Homework \n",
        "\n",
        "Visit https://pytorch.org/docs/master/torch.html and prepare a list of all available functions and operators in pytorch."
      ]
    },
    {
      "metadata": {
        "id": "rOC_4gfhHaJS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Key insight from calculus\n",
        "\n",
        "\n",
        "A key insight from calculus is that the gradient indicates the rate of change of the loss, or the slope of the loss function w.r.t. the weights and biases.\n",
        "\n",
        "    If a gradient element is postive,\n",
        "        increasing the element's value slightly will increase the loss.\n",
        "        decreasing the element's value slightly will decrease the loss.\n",
        "    \n",
        "    If a gradient element is negative,\n",
        "        increasing the element's value slightly will decrease the loss.\n",
        "        decreasing the element's value slightly will increase the loss.\n",
        "\n",
        "The increase or decrease is proportional to the value of the gradient."
      ]
    },
    {
      "metadata": {
        "id": "CrMxhm9HYRnT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Tensor Derivatives\n",
        "\n",
        "To optimize neural networks, we need to **calculate derivatives**, and to do this computationally, deep learning frameworks use what are called **computational graphs**.\n",
        "\n",
        "Computational graphs are used to graph the function operations that occur on tensors inside neural networks.\n",
        "\n",
        "These graphs are then used to compute the derivatives needed to optimize the neural network. PyTorch uses a computational graph that is called a **dynamic computational graph**. This means that the graph is **generated** **on the fly** as the operations are created.\n",
        "\n",
        "This is in contrast to static graphs that are fully determined before the actual operations occur."
      ]
    },
    {
      "metadata": {
        "id": "purYeVLGqd6U",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "| Package | Description  |\n",
        "|------           |------                |\n",
        "|   torch        | The top-level PyTorch package and tensor library. |\n",
        "|   torch.nn   | A subpackage that contains modules and extensible classes for building neural networks. |\n",
        "|   torch.autograd     | A subpackage that supports all the differentiable Tensor operations in PyTorch. |\n",
        "|   torch.nn.functional     | \tA functional interface that contains typical operations used for building neural networks like loss functions, activation functions, and convolution operations. |\n",
        "|   torch.optim         | \tA subpackage that contains standard optimization operations like SGD and Adam. |\n",
        "|   torch.utils        | \tA subpackage that contains utility classes like data sets and data loaders that make data preprocessing easier. |\n",
        "|   torchvision        | \t \tA package that provides access to popular datasets, model architectures, and image transformations for computer vision.  |\n",
        "\n",
        "\n",
        " \n",
        " \t\n",
        " \t\n",
        " \t\n",
        " \n",
        " \n",
        " \t\n"
      ]
    },
    {
      "metadata": {
        "id": "x4_AVfZdVMT2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In the computational graph, \n",
        "* a **node is an array** and \n",
        "* an **edge is an operation** on the array. \n",
        "\n",
        "To make a computational graph, we make a node by wrapping an array inside the torch.tensor() function. All operations that we do on this node from then on will be defined as edges in the computational graph. The edges of the graph also result in new nodes in the computational graph. \n",
        "\n",
        "Each node in the graph has \n",
        "* a ** .data property** which is a multi-dimensional array and \n",
        "* a **.grad property** which is it’s gradient with respect to some scalar value (.grad is also a tensor itself). \n",
        "\n",
        "After defining the computational graph, we can calculate gradients of the output with respect to all nodes in the graph with a single command i.e. tensor.backward().\n",
        "\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1zDYbkX0qF4-9xzzNtPQ3sUpTJbQh6pya)"
      ]
    },
    {
      "metadata": {
        "id": "ZWRtJpq0dAP8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Thus **torch.autograd** is the library that supports **automatic differentiation** (auto-compute gradients) in PyTorch. The central class of this package is torch.Tensor. \n",
        "\n",
        "    Automatic differentiation (autodiff) refers to a general way of taking a program which computes a value, and automatically constructing a procedure for computing derivatives of that value. We focus on reverse mode autodiff.  There is also a forward mode, which is for computing directional derivatives.\n",
        "  \n",
        "    Backpropagation is the special case of autodiff applied to neural nets But in machine learning, we often use backprop synonymously with autodiff.\n",
        "\n",
        "    Autograd is the name of a particular autodiff package. But lots of people, including the PyTorch developers, got confused and started using \"autograd\" to mean \"autodiff\".\n",
        "    \n",
        "    The goal of autodiff is not a formula, but a procedure for computing derivatives. An autodiff system will convert the program into a sequence of primitive operations which have specified routines for computing derivatives.\n",
        "\n",
        "    In this representation, backprop can be done in a completely mechanical way. Most autodiff systems, including Autograd, explicitly construct the computation graph.\n",
        "    \n",
        "\n",
        "* A Tensor has a Boolean field **requires_grad**, set to **False by default**, which states if PyTorch should build the graph of operations so that gradients with respect to it can be computed. \n",
        "\n",
        "* Thus to track all operations on tensors, we set **.requires_grad** as **True**. \n",
        "\n",
        "* Only **floating point** type tensors can have their **gradient computed**.\n",
        "\n",
        "* We define a function which constructs a **dynamic graph**.\n",
        "\n",
        "* To compute the derivative, we call **.backward()**. \n",
        "\n",
        "* The function **Tensor.backward() accumulates** the **gradients** in the different Tensors, so one may have to set them to zero before calling it.\n",
        "\n",
        "* The the derivative for the tensor will be accumulated in the **.grad** attribute\n",
        "\n",
        "* This **accumulating behavior** is **desirable** in particular to compute the gradient of a loss summed over several “mini-batches,” or the gradient of a sum of losses.\n",
        "\n",
        "* **torch.autograd.grad(outputs, inputs)** computes and returns the gradient of outputs with respect to inputs. The function Tensor.backward()  function is an **alternative** to torch.autograd.grad(...) and standard for training models.\n",
        "\n",
        "* Although  they  are  related, the  **autograd  graph  is  not  the  network’s structure**, but the graph of operations to compute the gradient.  It can be data-dependent and miss or replicate sub-parts of the network.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "MO8cuH5hViFL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "* Once a Tensor is converted to a node in the computational graph using torch.tensor()\n",
        " * access its value using **x.data**\n",
        " * access its gradient using **x.grad**\n",
        " * access its gradient value using **x.grad.data**\n",
        "* We can do operations on the tensor to make edges of the graph.\n",
        "* Each tensor has a **.grad_fn** attribute that references a **\"Function\"** that has created the tensor (except for tensors created by the user – their grad_fn is None).\n",
        "* The inputs  and the parameters in each layer are **leaf variables**, the **outputs** (usually it is called the loss and we minimize it to update the parameters of the network) of neural networks are the **root variables** in the graph.\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1Nd1BbSvHrSnIVtATTXP2C-sq2By_YJ61)"
      ]
    },
    {
      "metadata": {
        "id": "Nda9_W0ms4t_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bc6838e9-6fd5-4ed6-8e9a-348b050f9df6"
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "t = torch.tensor([1., 2., 4.]).requires_grad_()\n",
        "u = torch.tensor([10., 20.]).requires_grad_()\n",
        "a = t.pow(2).sum() + u.log().sum()\n",
        "torch.autograd.grad(a, (t, u))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([2., 4., 8.]), tensor([0.1000, 0.0500]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "ly3BW7vZFZWo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Example 1:** $$y(x) = x^2$$ \n",
        "\n",
        "The derivative of the above function is given as $$ \\frac {dy}{dx} = 2x$$\n",
        "\n",
        "Then the derivative of above function at $x=2$ is $$ \\frac {d\\; y(2)}{dx} = 2 x = 2 (2) = 4$$\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "avXzQS_IGptO",
        "colab_type": "code",
        "outputId": "c4f8e68a-c493-46fa-a7bb-353f366a9491",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "#!pip3 install torch torchvision\n",
        "import torch\n",
        "\n",
        "x = torch.tensor(2.0,requires_grad=True)  # We create a Torch tensor with a value of 2 and we set the parameter requires_grad equals\n",
        "                                          # The requires_grad=True stores all the operations associated with the variables\n",
        "\n",
        "y = x**2                                  # We have function y in terms of x i.e, y(x)=x^2 and this function call constructs a graph\n",
        "y.backward()                              # The backward method calculates the derivative of y w.r.t x when x=2 using chain rule\n",
        "x.grad                                    # The grad method shows the value of the derivative of y w.r.t x when x=2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4.)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "metadata": {
        "id": "GFXvJHmHIknh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Note that**, to get access to the value derivative of Y with respect to X we use **X.grad** not Y.grad"
      ]
    },
    {
      "metadata": {
        "id": "QwBttu0IvwHi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The **requires_grad** allows calculation of gradients w.r.t. the tensor allowing gradients accumulation"
      ]
    },
    {
      "metadata": {
        "id": "_U2y8A3MLCWl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Example 2:**  $$z(x) = x^2 + 2x + 1$$\n",
        "\n",
        "The derivative of the above function is given as $$ \\frac{d\\;z(x)}{dx} = 2x + 2$$\n",
        "\n",
        "Then the derivative of above function at $x=2$ is $$ \\frac {d\\; z(2)}{dx} = 2 x + 2 = 2 (2) + 2= 6$$"
      ]
    },
    {
      "metadata": {
        "id": "0Y2hg3uDKb6Q",
        "colab_type": "code",
        "outputId": "99c1dc22-2e72-48da-cd15-fd039570662b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "x = torch.tensor(2.0, requires_grad=True) # Assign x=2.0\n",
        "z = x**2 + 2*x+1                          # Construct the graph\n",
        "z.backward()                              # Use chain rule to calculate the gradient\n",
        "print(x.grad)                             # Print the gradient\n",
        "print(x)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(6.)\n",
            "tensor(2., requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tPyIc9OtTDME",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Gradients are **accumulated** with every cycle which allow us to get the correct gradient for all the computations with a given variable. \n",
        "\n",
        "Thus we may need to manually reset the values to 0 so that the gradients computed previously do not interfere with the ones we are currently computing."
      ]
    },
    {
      "metadata": {
        "id": "jGSyTavwX_0E",
        "colab_type": "code",
        "outputId": "243c72b2-e7b6-4a0a-e7e3-2f43abfd27d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "#x = torch.tensor(2.0, requires_grad=True)\n",
        "z = x**2 + 2*x+1                          # Reconstruct the graph\n",
        "z.backward()                              # Use chain rule to calculate the gradient\n",
        "print(x.grad)\n",
        "print(x)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(12.)\n",
            "tensor(2., requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ATu6bblTW513",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "After the backward call is called, the derivative is automatically calculated. \n",
        "\n",
        "Because the automatic derivation is done twice, and old gradients are accumulated, so the first gradient 6 and the second gradient 6 are added to get 12.\n",
        "\n",
        "In the example below we do **x.grad.zero_()** before the graph is reconstructed."
      ]
    },
    {
      "metadata": {
        "id": "oHBbPN6JezVr",
        "colab_type": "code",
        "outputId": "21f9f7de-f918-4ee7-a095-ffe9f3e3a9b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "#x = torch.tensor(2.0, requires_grad=True)\n",
        "x.grad.zero_()                            # Zero the gradients before reconstructing the graph\n",
        "z = x**2 + 2*x+1                          # Reconstruct the graph\n",
        "z.backward()                              # Use chain rule to calculate the gradient\n",
        "print(x.grad)\n",
        "print(x)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(6.)\n",
            "tensor(2., requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3CJ9_M59h9Ya",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Since the backward() function accumulates gradients, and you don’t want to mix up gradients between minibatches, you have to zero them out at the start of a new minibatch. This is exactly like how a general (additive) accumulator variable is initialized to 0 in code.\n",
        "\n",
        "By the way, **the best practice is to use the zero_grad() function on the optimizer.**"
      ]
    },
    {
      "metadata": {
        "id": "QtHrritxdwCG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Example 3:**\n",
        "\n",
        "The autograd graph is encoded through the \n",
        "* fields **grad_fn** of Tensor s, and \n",
        "* the fields **next_functions** of Function s."
      ]
    },
    {
      "metadata": {
        "id": "YrAE42Dmr52-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "6af6947d-1c77-407d-c201-0dba49b0cb91"
      },
      "cell_type": "code",
      "source": [
        "#!pip3 install torch torchvision\n",
        "import torch\n",
        "\n",
        "x = torch.tensor([ 1.0, -2.0, 3.0, -4.0 ]).requires_grad_()\n",
        "a = x.abs()\n",
        "s = a.sum()\n",
        "print(s)\n",
        "print(s.grad_fn.next_functions)\n",
        "print(s.grad_fn.next_functions[0][0].next_functions)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(10., grad_fn=<SumBackward0>)\n",
            "((<AbsBackward object at 0x7fd939cc2550>, 0),)\n",
            "((<AccumulateGrad object at 0x7fd97bc8fd30>, 0),)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FCnhpsypFX3h",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://drive.google.com/uc?id=1Jz_3xnDPr-3Q8vIut3_ZaEl2Ju6-LZDM)"
      ]
    },
    {
      "metadata": {
        "id": "MrWz0NuUd0nn",
        "colab_type": "code",
        "outputId": "776f511c-bda6-4aa5-c087-e4f204031281",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "cell_type": "code",
      "source": [
        "#!pip3 install torch torchvision\n",
        "import torch\n",
        "\n",
        "# Requires_grad=True turns on differential mode\n",
        "a=torch.tensor(([1.0]),requires_grad=True)\n",
        "b=torch.tensor(([2.0]),requires_grad=True)\n",
        "c=torch.tensor(([3.0]),requires_grad=True)\n",
        "\n",
        "d=a+b\n",
        "e=d+c\n",
        "e.backward()\n",
        "print(a.grad,b.grad,c.grad)\n",
        "\n",
        "print(d.grad) # Intermediate gradient value is not saved, and is empty\n",
        "\n",
        "print(a.grad_fn) # The first node's .grad_fn is empty\n",
        "print(e.grad_fn)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1.]) tensor([1.]) tensor([1.])\n",
            "None\n",
            "None\n",
            "<AddBackward0 object at 0x7f993dbb8208>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TA0z0fj6jCbx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Multi-Dimensional Tensor Inputs"
      ]
    },
    {
      "metadata": {
        "id": "ZcJxKyo4jsSd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If we want to compute the derivatives, we call .backward() on a tensor. \n",
        "\n",
        "If tensor is a scalar (i.e. it holds a one element data), we don’t need to specify any arguments to backward(), however if it has more elements, you need to specify a **grad_output** **argument** that is a tensor of **matching** **shape**.\n",
        "\n",
        "I.e, if the Tensor contains one element, you don’t have to specify any parameters for the backward() function. **If the Tensor contains more than one element, specify a gradient that’s a tensor of matching shape*.*\n",
        "\n",
        "Basically to start the chain rule we need a gradient at the output, to get it going. In the event the output is a scalar loss function (which it usually is - normally we are beginning the backward pass at the loss variable ), its an implied value of 1.0\n",
        "\n",
        "The function y.backward() is equivalent to doing ** y.backward(torch.Tensor([1.0]))**"
      ]
    },
    {
      "metadata": {
        "id": "7TQf9BOMDVvF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Example3: **\n",
        "\n",
        "$$ f(x,y) = z =x^2 \\cdot y$$\n",
        "\n",
        "computing partial derivatives looks something like this:\n",
        "\n",
        "$$\\frac {\\partial f}{\\partial x} = \\frac {\\partial}{\\partial x}x^2 y = 2 x y$$\n",
        "$$\\frac {\\partial f}{\\partial y} = \\frac {\\partial}{\\partial y}x^2 y = x^2\\cdot 1$$"
      ]
    },
    {
      "metadata": {
        "id": "0idRQxbM9O6x",
        "colab_type": "code",
        "outputId": "c43fb9d6-d71c-4b22-c811-a90b19f2972a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "#!pip3 install torch torchvision\n",
        "import torch\n",
        "x = torch.tensor([1.0, 2.0, 3.0, 4.0], requires_grad=True)\n",
        "y = torch.tensor([5.0, 6.0, 7.0, 8.0], requires_grad=True)\n",
        "z = (x**2) * y\n",
        "print(z)\n",
        "\n",
        "z.backward(torch.FloatTensor([1, 0, 0, 0])) # do backward for first element of z\n",
        "#z.backward()\n",
        "print(\"The derivative of z_1 w.r.t to x: \",x.grad.data)\n",
        "print(\"The derivative of z_1 w.r.t to y: \",y.grad.data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([  5.,  24.,  63., 128.], grad_fn=<MulBackward0>)\n",
            "The derivative of z_1 w.r.t to x:  tensor([10.,  0.,  0.,  0.])\n",
            "The derivative of z_1 w.r.t to y:  tensor([1., 0., 0., 0.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uuAahj-FXgZI",
        "colab_type": "code",
        "outputId": "a2c29b0d-3995-4780-ac5c-f63cd3cf702b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "x = torch.tensor([1.0, 2.0, 3.0, 4.0], requires_grad=True)\n",
        "y = torch.tensor([5.0, 6.0, 7.0, 8.0], requires_grad=True)\n",
        "z = (x**2) * y\n",
        "z.backward(torch.FloatTensor([0, 1, 0, 0])) # do backward for second element of z\n",
        "print(\"The derivative of z_2 w.r.t to x:, \",x.grad.data)\n",
        "print(\"The derivative of z_2 w.r.t to y: \",y.grad.data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The derivative of z_2 w.r.t to x:,  tensor([ 0., 24.,  0.,  0.])\n",
            "The derivative of z_2 w.r.t to y:  tensor([0., 4., 0., 0.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LaYXD0sLXibH",
        "colab_type": "code",
        "outputId": "6608946e-f0f8-4c3a-fb85-e49f1399d596",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "# do backward for all elements of z, equal to the collection of partial derivatives z_1, z_2, z_3 and z_4\n",
        "x = torch.tensor([1.0, 2.0, 3.0, 4.0], requires_grad=True)\n",
        "y = torch.tensor([5.0, 6.0, 7.0, 8.0], requires_grad=True)\n",
        "z = (x**2) * y\n",
        "z.backward(torch.FloatTensor([1, 1, 1, 1]))\n",
        "print(x.grad.data)\n",
        "print(y.grad.data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([10., 24., 42., 64.])\n",
            "tensor([ 1.,  4.,  9., 16.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lt86fPUJYJBL",
        "colab_type": "code",
        "outputId": "c1ee9a23-9e54-45e0-90b0-5bc74d3c074d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "x = torch.tensor([1.0, 2.0, 3.0, 4.0], requires_grad=True)\n",
        "y = torch.tensor([5.0, 6.0, 7.0, 8.0], requires_grad=True)\n",
        "z = (x**2) * y\n",
        "z.backward(gradient=torch.ones(z.size()))\n",
        "print(x.grad.data)\n",
        "print(y.grad.data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([10., 24., 42., 64.])\n",
            "tensor([ 1.,  4.,  9., 16.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bg83JG2laK0-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The above code can also be written as:"
      ]
    },
    {
      "metadata": {
        "id": "nUUZikk7Z__o",
        "colab_type": "code",
        "outputId": "94a0e0d1-7e83-469c-94e3-4b8189fa8d39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "x = torch.tensor([1.0, 2.0, 3.0, 4.0], requires_grad=True)\n",
        "y = torch.tensor([5.0, 6.0, 7.0, 8.0], requires_grad=True)\n",
        "z = (x**2) * y\n",
        "print(z)\n",
        "print(z.shape)\n",
        "\n",
        "z.backward(torch.FloatTensor([1, 0, 0, 0])) # do backward for first element of z\n",
        "print(\"The derivative of z_1 w.r.t to x: \",x.grad.data)\n",
        "print(\"The derivative of z_1 w.r.t to y: \",y.grad.data)\n",
        "\n",
        "x.grad.zero_() # Zero the gradients of x\n",
        "y.grad.zero_() # Zero the gradients of y\n",
        "z = (x**2) * y # Reconstruct the graph\n",
        "z.backward(torch.FloatTensor([1, 1, 1, 1])) # do backward for second element of z\n",
        "print(\"The derivative of z_2 w.r.t to x:, \",x.grad.data)\n",
        "print(\"The derivative of z_2 w.r.t to y: \",y.grad.data)\n",
        "\n",
        "x.grad.zero_()\n",
        "y.grad.zero_()\n",
        "z = (x**2) * y\n",
        "z.backward(torch.FloatTensor([1, 1, 1, 1]))\n",
        "print(x.grad.data)\n",
        "print(y.grad.data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([  5.,  24.,  63., 128.], grad_fn=<MulBackward0>)\n",
            "torch.Size([4])\n",
            "The derivative of z_1 w.r.t to x:  tensor([10.,  0.,  0.,  0.])\n",
            "The derivative of z_1 w.r.t to y:  tensor([1., 0., 0., 0.])\n",
            "The derivative of z_2 w.r.t to x:,  tensor([10., 24., 42., 64.])\n",
            "The derivative of z_2 w.r.t to y:  tensor([ 1.,  4.,  9., 16.])\n",
            "tensor([10., 24., 42., 64.])\n",
            "tensor([ 1.,  4.,  9., 16.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-6HVkZx4iyIO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For varying dimensional matrices, we can clearly see the gradients of z are computed w.r.t to each dimension of x, because the operations are all element-wise."
      ]
    },
    {
      "metadata": {
        "id": "Ulo_sLnZixwT",
        "colab_type": "code",
        "outputId": "36e623b3-c023-4a98-95de-9cbc68a5d19b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        }
      },
      "cell_type": "code",
      "source": [
        "x = torch.randn(2, 2, requires_grad=True) #x is a leaf created by user, thus grad_fn is none\n",
        "# Each tensor has a .grad_fn attribute that references a \"Function\" that has created the tensor (except for tensors created by the user – their grad_fn is None).\n",
        "\n",
        "print('x', x)\n",
        "\n",
        "y = 2 * x  # define an operation on x and construct the graph\n",
        "z = y ** 3 # define one more operation to check the chain rule and continue constructing the graph\n",
        "\n",
        "print('z shape:', z.size())\n",
        "z.backward(torch.FloatTensor([[1, 1], [1, 1]]))\n",
        "print('x gradient for its all elements:\\n', x.grad)\n",
        "\n",
        "x.grad.zero_()\n",
        "x.grad.data.zero_() #the gradient for x will be accumulated, it needs to be cleared.\n",
        "y = 2 * x   \n",
        "z = y ** 3  \n",
        "z.backward(torch.FloatTensor([[0, 1], [0, 1]]))\n",
        "print('x gradient for the second column:\\n', x.grad)\n",
        "\n",
        "x.grad.zero_()\n",
        "x.grad.data.zero_()\n",
        "y = 2 * x   \n",
        "z = y ** 3\n",
        "z.backward(torch.FloatTensor([[1, 1], [0, 0]]))\n",
        "print('x gradient for the first row:\\n', x.grad)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x tensor([[-2.3170, -0.4011],\n",
            "        [ 1.3753, -0.7153]], requires_grad=True)\n",
            "z shape: torch.Size([2, 2])\n",
            "x gradient for its all elements:\n",
            " tensor([[128.8384,   3.8609],\n",
            "        [ 45.3933,  12.2790]])\n",
            "x gradient for the second column:\n",
            " tensor([[ 0.0000,  3.8609],\n",
            "        [ 0.0000, 12.2790]])\n",
            "x gradient for the first row:\n",
            " tensor([[128.8384,   3.8609],\n",
            "        [  0.0000,   0.0000]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8UgbJHXJjnzQ",
        "colab_type": "code",
        "outputId": "c3ebfb7d-8c0a-457a-c222-2d7cab0c462b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        }
      },
      "cell_type": "code",
      "source": [
        "x = torch.randn(2, 2, requires_grad=True) #x is a leaf created by user, thus grad_fn is none\n",
        "print('x', x)\n",
        "\n",
        "y = 2 * x #define an operation on x\n",
        "print('y', y)\n",
        "z = y ** 3 #define one more operation to check the chain rule\n",
        "\n",
        "out = z.mean()\n",
        "print('out', out)\n",
        "out.backward()\n",
        "print('x gradient:\\n', x.grad)\n",
        "\n",
        "x.grad.data.zero_()\n",
        "y = 2 * x \n",
        "z = y ** 3\n",
        "out = z.mean()\n",
        "#out.backward(torch.FloatTensor([[1, 1], [1, 1]]))\n",
        "out.backward(torch.ones(z.size())) #Note the use of size() function of the output vector\n",
        "print('x gradient second time', x.grad)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x tensor([[ 1.2084,  1.0644],\n",
            "        [ 0.0523, -0.8410]], requires_grad=True)\n",
            "y tensor([[ 2.4168,  2.1287],\n",
            "        [ 0.1046, -1.6819]], grad_fn=<MulBackward0>)\n",
            "out tensor(4.7513, grad_fn=<MeanBackward1>)\n",
            "x gradient:\n",
            " tensor([[8.7612, 6.7971],\n",
            "        [0.0164, 4.2434]])\n",
            "x gradient second time tensor([[35.0449, 27.1885],\n",
            "        [ 0.0656, 16.9736]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aoJhaoCdYYHY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Tensor Gradients"
      ]
    },
    {
      "metadata": {
        "id": "6UFbWXfkYbXk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Consider the function of two variables $u$ and $v$ \n",
        "\n",
        "$$ f(u,v) = uv + u^2$$\n",
        "\n",
        "Thus the partial derivatives w.r.t the two variables $u$ and $v$ are given as:\n",
        "\n",
        "$$\\frac {\\partial f(u,v)}{\\partial u} = v+2u$$\n",
        "\n",
        "$$\\frac {\\partial f(u,v)}{\\partial v} = u$$\n",
        "\n",
        "Now at u=1 and v=2 the function: $$f(u=1,v=2) = uv + u^2 = 1(2)+ 1^2 = 3$$\n",
        "\n",
        "The partial derivatives at u=1 and at v=2 are:\n",
        "$$\\frac {\\partial f(1,2)}{\\partial u} = v+2u = 2 + 2 (1) = 4$$\n",
        "\n",
        "$$\\frac {\\partial f(1,2)}{\\partial v} = u = 1$$"
      ]
    },
    {
      "metadata": {
        "id": "7MV7TW0NZKOm",
        "colab_type": "code",
        "outputId": "c72febb0-4218-44c0-9c25-073d4b83e1a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "#Define two tensors u and v\n",
        "u=torch.tensor(1.0,requires_grad=True)\n",
        "v=torch.tensor(2.0,requires_grad=True)\n",
        "f=u*v+u**2\n",
        "f.backward()\n",
        "print(u.grad)\n",
        "print(v.grad)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(4.)\n",
            "tensor(1.)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "534Iov1boKoB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We need to calculate the derivative of y equals x squared. We generate values of x from -10 to 10. Note that we have to use the detach .option before we can cast it as a numpy array required for matplotlib function. The **.detach()** function will prevent future computations on the tensor from being tracked. Another way to prevent history tracking is by wrapping your code with **torch.no_grad()**.\n",
        "\n",
        "The **detach() method creates a tensor which shares the data, but does not require gradient computation, and is not connected to the current graph**. This method should be used when the gradient should not be propagated beyond a variable, or to update leaf tensors."
      ]
    },
    {
      "metadata": {
        "id": "mY4FiDfth0Nq",
        "colab_type": "code",
        "outputId": "d57076b7-a8b4-4248-a7e1-1f864657a1d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        }
      },
      "cell_type": "code",
      "source": [
        "x = torch.linspace(-10,10,10,requires_grad=True)\n",
        "print(x)\n",
        "Y=x**2\n",
        "print(Y)\n",
        "y=x**2\n",
        "print(y)\n",
        "y.backward(torch.Tensor([1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]))\n",
        "print(x.grad)\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(x.detach().numpy(),Y.detach().numpy(),label='function')\n",
        "plt.plot(x.detach().numpy(),x.grad.detach().numpy(),label='derivative')\n",
        "plt.legend()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([-10.0000,  -7.7778,  -5.5556,  -3.3333,  -1.1111,   1.1111,   3.3333,\n",
            "          5.5556,   7.7778,  10.0000], requires_grad=True)\n",
            "tensor([100.0000,  60.4938,  30.8642,  11.1111,   1.2346,   1.2346,  11.1111,\n",
            "         30.8642,  60.4938, 100.0000], grad_fn=<PowBackward0>)\n",
            "tensor([100.0000,  60.4938,  30.8642,  11.1111,   1.2346,   1.2346,  11.1111,\n",
            "         30.8642,  60.4938, 100.0000], grad_fn=<PowBackward0>)\n",
            "tensor([-20.0000, -15.5556, -11.1111,  -6.6667,  -2.2222,   2.2222,   6.6667,\n",
            "         11.1111,  15.5556,  20.0000])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7ff2aeff6128>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeQAAAFKCAYAAADMuCxnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n3wAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7ff2c003ef60>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "8svY7zbtenLb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Homework: A little hack**. Explain the function below as to why we are taking **sum** compared to the above function.\n",
        "\n",
        "Hint: Even if we have, y = x^2 ; loss = y.sum(); when we do loss.backward(), we still can get the gradient of loss w.r.t. y as sum willl be the last op of the last layer, and the graph will calculate backwards from there. "
      ]
    },
    {
      "metadata": {
        "id": "6WNBbbTbesdR",
        "colab_type": "code",
        "outputId": "d220b572-d745-4f63-f1c3-6b53fbba525f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 494
        }
      },
      "cell_type": "code",
      "source": [
        "x = torch.linspace(-10,10,10,requires_grad=True)\n",
        "print(x)\n",
        "Y=x**2\n",
        "print(Y)\n",
        "y=torch.sum(x**2)\n",
        "print(y)\n",
        "y.backward()\n",
        "print(x.grad)\n",
        "import matplotlib.pyplot as plt\n",
        "plt.plot(x.detach().numpy(),Y.detach().numpy(),label='function')\n",
        "plt.plot(x.detach().numpy(),x.grad.detach().numpy(),label='derivative')\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([-10.0000,  -7.7778,  -5.5556,  -3.3333,  -1.1111,   1.1111,   3.3333,\n",
            "          5.5556,   7.7778,  10.0000], requires_grad=True)\n",
            "tensor([100.0000,  60.4938,  30.8642,  11.1111,   1.2346,   1.2346,  11.1111,\n",
            "         30.8642,  60.4938, 100.0000], grad_fn=<PowBackward0>)\n",
            "tensor(407.4074, grad_fn=<SumBackward0>)\n",
            "tensor([-20.0000, -15.5556, -11.1111,  -6.6667,  -2.2222,   2.2222,   6.6667,\n",
            "         11.1111,  15.5556,  20.0000])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7ff2ac7224e0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeQAAAFKCAYAAADMuCxnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n3wAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7ff2c003e518>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "cgWeOXVhe9CI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Relu Function**"
      ]
    },
    {
      "metadata": {
        "id": "I6iuyQomrda9",
        "colab_type": "code",
        "outputId": "b708198e-fc72-495b-a253-b861cec2aeaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        }
      },
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "x=torch.linspace(-3,3,100,requires_grad=True)\n",
        "Y=F.relu(x)\n",
        "y=torch.sum(F.relu(x))\n",
        "y.backward()\n",
        "plt.plot(x.detach().numpy(),Y.detach().numpy(),label='function')\n",
        "plt.plot(x.detach().numpy(),x.grad.detach().numpy(),label='derivative')\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7ff2ac6996a0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeEAAAFKCAYAAAAqkecjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/w+gN4Ww6RFlLwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7ff2c08a3ac8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "O1EmVmYTwIEZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Retain graph\n"
      ]
    },
    {
      "metadata": {
        "id": "yMnXI57RwRj8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In PyTorch, the computation **graph** is **created** at the **start** of **each iteration** in an **epoch** and then subsequently **freed** to save memory at the **end** of that **iteration**.\n",
        "\n",
        "In each iteration, we execute the forward pass, compute the derivatives of output w.r.t to the parameters of the network, and update the parameters to fit the given examples. After doing the backward pass, the graph will be freed to save memory. In the **next iteration**, a **fresh** new **graph** is **created** and ready for back-propagation\n",
        "\n",
        "Because the computation graph will be freed by default after the first backward pass, you will encounter errors if you are trying to do backward on the same graph the second time. \n",
        "\n",
        "We can specify **retain_graph=True** when calling backward the first time to make sure the graph is retained and the buffers are not freed till we finish the backward propogation through the graph a second time.\n",
        "\n",
        "During the optimization step, we combine the chain rule and the graph to compute the derivative (partial) of the output w.r.t the learnable variable in the graph and update these variables to make the output close to what we want."
      ]
    },
    {
      "metadata": {
        "id": "EeJ1h-g7tAm1",
        "colab_type": "code",
        "outputId": "548219e6-8078-49b5-ad63-e6bb4e7bcf32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "#!pip3 install torch torchvision\n",
        "import torch\n",
        "f = torch.tensor([2.0,3.0],requires_grad=True)\n",
        "print(\"Original Tensor: \",f)\n",
        "g = f[0] * f[1]\n",
        "g.backward(retain_graph=True)\n",
        "#g.backward()\n",
        "print(\"1st Backward Pass: \",f.grad)\n",
        "g.backward()\n",
        "print(\"2nd Backward Pass: \",f.grad)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original Tensor:  tensor([2., 3.], requires_grad=True)\n",
            "1st Backward Pass:  tensor([3., 2.])\n",
            "2nd Backward Pass:  tensor([6., 4.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8DSg1aFB2Hcg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![alt text](https://drive.google.com/uc?id=1ZCqitzhUSotar1iBaepVrWZAdSpcl1wV)\n",
        "\n",
        "Suppose that we have a computation graph shown above. The variable d and e is the output, and a is the input. The underlining computation is:"
      ]
    },
    {
      "metadata": {
        "id": "cO0DbbePzJzg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "a = torch.randn((1,4),requires_grad=True)\n",
        "b = a**2\n",
        "c = b*2\n",
        "d = c.mean()\n",
        "e = c.sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5TGfSlRD2XbD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "When we do d.backward(), that is fine. After this computation, the part of graph that calculate d will be freed by default to save memory. So if we do e.backward(), the error message will pop up. In order to do e.backward(), we have to set the parameter retain_graph to True in d.backward(), i.e.,"
      ]
    },
    {
      "metadata": {
        "id": "B576eRWo2cut",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#d.backward(retain_graph=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6UjTWZhm2gdy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As long as you use retain_graph=True in your backward method, you can do backward any time you want."
      ]
    },
    {
      "metadata": {
        "id": "BhxDzae52hRB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "d.backward(retain_graph=True) # fine....graph is retained so no need to reconstruct again\n",
        "e.backward(retain_graph=True) # fine....graph is retained so no need to reconstruct again\n",
        "d.backward() # also fine\n",
        "#e.backward() # error will occur!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R6xFtJn_ZLyU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Here the call is made again in the last sentence, the error will be reported. Because the pytorch defaults to an automatic derivation, the calculation graph is discarded. The requests need to be set manually."
      ]
    },
    {
      "metadata": {
        "id": "WW4rgLljZ15Z",
        "colab_type": "code",
        "outputId": "0e3782de-7704-4cfe-ebc1-4d435c5354d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "x = torch.tensor([3.0],requires_grad=True) \n",
        "y = x * 2 + x ** 2 + 3 \n",
        "\n",
        "y.backward(retain_graph= True ) \n",
        "print(x.grad) # Tensor containing: 8, [torch.FloatTensor of size 1] \n",
        "\n",
        "y.backward(retain_graph= True ) \n",
        "print(x.grad) # Output16, because the automatic derivation is done twice, so the first gradient 8 and the second gradient 8 are added to get 16 \n",
        "\n",
        "y.backward()  # Do another automatic derivation, this time does not retain the calculation graph \n",
        "print(x.grad) # Outputs 24 \n",
        "\n",
        "#y.backward()  # will do an error, the calculation graph has been discarded "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([8.])\n",
            "tensor([16.])\n",
            "tensor([24.])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XPja3gjq4z_Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The addition operation don’t need buffers \n",
        "\n",
        "If $f(x) = x + w$ then the gradient of $f$ with respect to $w$ is 1. In this case the gradient doesn’t depend on the inputs.\n",
        "\n",
        "If $f(x) = x * w$ then the gradient of $f$ with respect to $w$ is $x$. In this case, we need to save the input value."
      ]
    },
    {
      "metadata": {
        "id": "fFCIHEAS5OIR",
        "colab_type": "code",
        "outputId": "4d13dc0d-4235-4415-abfc-dacb0a75c82d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "cell_type": "code",
      "source": [
        "f = torch.tensor([2.0,3.0], requires_grad=True)\n",
        "g = f[0] + f[1]\n",
        "g.backward()\n",
        "print(f.grad)\n",
        "g.backward()\n",
        "print(f.grad)\n",
        "print(f.grad.data[0])\n",
        "print(f.is_leaf)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1., 1.])\n",
            "tensor([2., 2.])\n",
            "tensor(2.)\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tfQN8MIq2sx6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Usually after a backpropagation you process the next batch so you don’t need the gradients of the previous batch anymore.\n",
        "\n",
        "**retain_variables** argument has been deprecated in favor of **retain_graph**\n",
        "\n",
        "A **real use case** that you want to backward through the graph for more than once is multi-task learning where you have multiple losses at different layers. Suppose that you have 2 losses: loss1 and loss2 and they reside in different layers. In order to back-prop the gradient of loss1 and loss2 w.r.t to the learnable weight of your network independently. You have to use retain_graph=True in backward() method in the first back-propagated loss."
      ]
    },
    {
      "metadata": {
        "id": "xyU3OGxk3b6B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#loss1.backward(retain_graph=True) # suppose you first back-propagate loss1, then loss2 (you can also do it in reverse order) \n",
        "#loss2.backward() # now the graph is freed, and next process of batch gradient descent is ready\n",
        "#optimizer.step() # update the network parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3LOzesnviKNz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Wrap-up\n",
        "\n",
        "1.   The backward() function made differentiation very simple\n",
        "2.   For non-scalar tensor, we need to specify **grad_tensors**\n",
        "3.   If you need to backward() twice on a graph or subgraph, you will need to set retain_graph to be true\n",
        "4.   Note that grad will accumulate from excuting the graph multiple times\n",
        "\n",
        "There are several attributes related to gradients that every tensor has:\n",
        "\n",
        "**grad**: A property which holds a tensor of the same shape containing computed gradients.\n",
        "\n",
        "**is_leaf**: True, if this tensor was constructed by the user and False, if the object is a result of function transformation.\n",
        "\n",
        "**requires_grad**: True if this tensor requires gradients to be calculated. This property is inherited from leaf tensors, which get this value from the tensor construction step (zeros() or torch.tensor() and so on). By default, the constructor has requires_grad=False, so if you want gradients to be calculated for your tensor, then you need to explicitly say so."
      ]
    },
    {
      "metadata": {
        "id": "Oz5voXR_1ycM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Homework\n",
        "\n",
        "Check if operations and functions in pytorch support broadcasting and which of them share memory and dont share memory (create new copies)."
      ]
    },
    {
      "metadata": {
        "id": "LRSu5x8fwR3v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Pytorch Example\n",
        "\n",
        "The network will have a single hidden layer, i.e, a two-layer network and will be trained with gradient descent to fit random data by minimizing the Euclidean distance between the network output and the true output. This is adapter from module1 but implemented using pytorch without helper functions."
      ]
    },
    {
      "metadata": {
        "id": "L7ZgxgdP85j-",
        "colab_type": "code",
        "outputId": "05ac4155-3c6e-4ea7-cc69-5f18e9a0017c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "cell_type": "code",
      "source": [
        "# Original Problem\n",
        "# -*- coding: utf-8 -*-\n",
        "import numpy as np\n",
        "\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10\n",
        "\n",
        "# Create random input and output data\n",
        "x = np.random.randn(N, D_in)\n",
        "y = np.random.randn(N, D_out)\n",
        "\n",
        "# Randomly initialize weights\n",
        "w1 = np.random.randn(D_in, H)\n",
        "w2 = np.random.randn(H, D_out)\n",
        "\n",
        "learning_rate = 1e-6\n",
        "for t in range(5):\n",
        "    # Forward pass: compute predicted y\n",
        "    h = x.dot(w1)\n",
        "    h_relu = np.maximum(h, 0)\n",
        "    y_pred = h_relu.dot(w2)\n",
        "    \n",
        "    # Zero the grads is not required as we are not using torch backward() function which accumulates buffers\n",
        "    \n",
        "    # Compute and print loss\n",
        "    loss = np.square(y_pred - y).sum()\n",
        "    print(t, loss)\n",
        "    \n",
        "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
        "    grad_y_pred = 2.0 * (y_pred - y)\n",
        "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
        "    \n",
        "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
        "    grad_h = grad_h_relu.copy()\n",
        "    grad_h[h < 0] = 0\n",
        "    grad_w1 = x.T.dot(grad_h)\n",
        "   \n",
        "    \n",
        "    # Update weights\n",
        "    w1 -= learning_rate * grad_w1\n",
        "    w2 -= learning_rate * grad_w2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 33873279.06498797\n",
            "1 30661600.0314354\n",
            "2 31819874.282981798\n",
            "3 31212882.333333947\n",
            "4 25930731.15114764\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "P-8QDfinJtdp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Original Problem using pytorch"
      ]
    },
    {
      "metadata": {
        "id": "ovtyKgeyraGu",
        "colab_type": "code",
        "outputId": "147980a9-046c-44c8-a0db-329cbc5354ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# device = torch.device(\"cpu\") # Uncomment this to run on CPU\n",
        "dtype = torch.float\n",
        "device = torch.device(\"cuda:0\") \n",
        "\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10\n",
        "\n",
        "# Create random input and output data\n",
        "x = torch.randn(N, D_in, device=device, dtype=dtype)  #64x1000\n",
        "y = torch.randn(N, D_out, device=device, dtype=dtype) #64x10\n",
        "\n",
        "# Randomly initialize weights\n",
        "w1 = torch.randn(D_in, H, device=device, dtype=dtype)  #1000 x 100\n",
        "w2 = torch.randn(H, D_out, device=device, dtype=dtype) #100 x 10\n",
        "\n",
        "\n",
        "learning_rate = 1e-6\n",
        "for t in range(5):\n",
        "    # Forward pass: compute predicted y\n",
        "    h = x.mm(w1)\n",
        "    h_relu = h.clamp(min=0)\n",
        "    y_pred = h_relu.mm(w2)\n",
        "\n",
        "    # Compute and print loss\n",
        "    loss = (y_pred - y).pow(2).sum().item()\n",
        "    print(t, loss)\n",
        "\n",
        "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
        "    grad_y_pred = 2.0 * (y_pred - y)\n",
        "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
        "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
        "    grad_h = grad_h_relu.clone()\n",
        "    grad_h[h < 0] = 0\n",
        "    grad_w1 = x.t().mm(grad_h)\n",
        "\n",
        "    # Update weights using gradient descent\n",
        "    w1 -= learning_rate * grad_w1\n",
        "    w2 -= learning_rate * grad_w2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 28795944.0\n",
            "1 23632760.0\n",
            "2 22605602.0\n",
            "3 22144822.0\n",
            "4 20285756.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LYOYvD0nIrit",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Using pytorch built in functions."
      ]
    },
    {
      "metadata": {
        "id": "r2nMuahv-2BL",
        "colab_type": "code",
        "outputId": "863fa84e-3e2d-4de0-dc66-fd8930aea8a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10\n",
        "\n",
        "x = torch.randn(N, D_in,requires_grad=False)\n",
        "y = torch.randn(N, D_out,requires_grad=False)\n",
        "\n",
        "w1 = torch.randn(D_in, H,requires_grad=True)\n",
        "w2 = torch.randn(H, D_out,requires_grad=True)\n",
        "\n",
        "learning_rate = 1e-6\n",
        "\n",
        "for t in range(5):\n",
        "    # Do forward\n",
        "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
        "    \n",
        "    # Compute Loss\n",
        "    loss = (y_pred - y).pow(2).sum()\n",
        "    print(t, loss.detach().item())  \n",
        "    \n",
        "    # Backprop to compute gradients (partial derivatives) of w1 and w2 with respect to loss\n",
        "    loss.backward()\n",
        "    \n",
        "    # Update weights using gradient descent\n",
        "    w1.data -= learning_rate * w1.grad.data\n",
        "    w2.data -= learning_rate * w2.grad.data\n",
        "\n",
        "    # Zero them for next iteration, as we have already update the weights with gradient data\n",
        "    w1.grad.data.zero_()\n",
        "    w2.grad.data.zero_()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 27877584.0\n",
            "1 21995582.0\n",
            "2 19702060.0\n",
            "3 17997614.0\n",
            "4 15818901.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-m5aNVp5EkEk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We can also use **loss.data** (**loss.data.item()**) instead of **loss.detach().item()** however it detaches the tensor from the computation graph and might lead to wrong results.\n",
        "\n",
        "While using loss.data is unrelated to the computation graph, **loss.detach** will have its in-place changes reported by autograd if loss is needed in backward and will raise an error if necessary.\n",
        "\n",
        "**Both share the underlying data of the tensor and have requires_grad=False**. \n",
        "\n",
        "Thus tensor.data gives a tensor that shares the storage with tensor, but doesn't track history hence no_grad() function is not required.\n",
        "\n",
        "Please note In autograd, if any input Tensor of an operation has requires_grad=True, **the computation will be tracked**. For updating weights we dont need to backtrack the operation.\n",
        "\n",
        "So to summarize, they are both used to detach tensor from computation graph and returns a tensor that shares the same data, the difference is **loss.detach() adds another constrain that when the data is changed in-place, the backward won’t be done**.\n",
        "\n",
        "Refer to https://pytorch.org/blog/pytorch-0_4_0-migration-guide/"
      ]
    },
    {
      "metadata": {
        "id": "W8ZIG4Y24200",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ==================USING detach()=======================================\n",
        "#!pip3 install torch torchvision\n",
        "import torch\n",
        "x = torch.tensor(([5.0]))\n",
        "w = torch.tensor(([10.0]),requires_grad=True)\n",
        "y = w*x\n",
        "print(w)\n",
        "\n",
        "c = w.detach()\n",
        "c.zero_()\n",
        "print(w) # Modified by c.zero_()!!\n",
        "\n",
        "y.backward() # Error One of the variables needed for gradient computation has been modified by an inplace operation\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eIhRVoWL8jxT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "5149cc94-3106-43c6-f007-2d528763f679"
      },
      "cell_type": "code",
      "source": [
        "# ==================USING .data()=======================================\n",
        "#!pip3 install torch torchvision\n",
        "import torch\n",
        "x = torch.tensor(([5.0]))\n",
        "w = torch.tensor(([10.0]),requires_grad=True)\n",
        "y = w*x\n",
        "print(w)\n",
        "\n",
        "c = w.data\n",
        "c.zero_()\n",
        "print(w) # Modified by c.zero_()!!\n",
        "\n",
        "y.backward() # Error is not reported as in-place changes are not tracked by autograd"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([10.], requires_grad=True)\n",
            "tensor([0.], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nKDCHA7N7_GS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Thus, **.data** can be unsafe in some cases. Any changes on w.data wouldn't be tracked by autograd, and the computed gradients would be incorrect if w is needed in a backward pass. A safer alternative is to use w.detach(), which also returns a Tensor that shares data with requires_grad=False, but will have its **in-place changes reported by autograd if w is needed in backward**"
      ]
    },
    {
      "metadata": {
        "id": "UYzt3l2rI1wr",
        "colab_type": "code",
        "outputId": "d3712aac-8356-41f5-b4c5-ca99cb3b0f3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "dtype = torch.float\n",
        "device = torch.device(\"cpu\")\n",
        "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
        "\n",
        "# N is batch size; D_in is input dimension;\n",
        "# H is hidden dimension; D_out is output dimension.\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10\n",
        "\n",
        "# Create random Tensors to hold input and outputs.\n",
        "# Setting requires_grad=False indicates that we do not need to compute gradients\n",
        "# with respect to these Tensors during the backward pass.\n",
        "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
        "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
        "\n",
        "# Create random Tensors for weights.\n",
        "# Setting requires_grad=True indicates that we want to compute gradients with\n",
        "# respect to these Tensors during the backward pass.\n",
        "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
        "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
        "\n",
        "learning_rate = 1e-6\n",
        "for t in range(5):\n",
        "    # Forward pass: compute predicted y using operations on Tensors; these\n",
        "    # are exactly the same operations we used to compute the forward pass using\n",
        "    # Tensors, but we do not need to keep references to intermediate values since\n",
        "    # we are not implementing the backward pass by hand.\n",
        "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
        "\n",
        "    # Compute and print loss using operations on Tensors.\n",
        "    # Now loss is a Tensor of shape (1,)\n",
        "    # loss.item() gets the a scalar value held in the loss.\n",
        "    loss = (y_pred - y).pow(2).sum()\n",
        "    print(t, loss.item())\n",
        "\n",
        "    # Use autograd to compute the backward pass. This call will compute the\n",
        "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
        "    # After this call w1.grad and w2.grad will be Tensors holding the gradient\n",
        "    # of the loss with respect to w1 and w2 respectively.\n",
        "    loss.backward()\n",
        "\n",
        "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
        "    # because weights have requires_grad=True, but we don't need to track this\n",
        "    # in autograd.\n",
        "    # An alternative way is to operate on weight.data and weight.grad.data.\n",
        "    # Recall that tensor.data gives a tensor that shares the storage with\n",
        "    # tensor, but doesn't track history.\n",
        "    # You can also use torch.optim.SGD to achieve this.\n",
        "    with torch.no_grad():\n",
        "        w1 -= learning_rate * w1.grad\n",
        "        w2 -= learning_rate * w2.grad\n",
        "\n",
        "        # Manually zero the gradients after updating weights\n",
        "        w1.grad.zero_()\n",
        "        w2.grad.zero_()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 28906468.0\n",
            "1 23217610.0\n",
            "2 22110076.0\n",
            "3 22007888.0\n",
            "4 20815722.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SoISdLMAyPXA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Using the **context manager no_grad()** we can avoid storing the computations done producing the output of our network in the computation graph.\n",
        "\n",
        "The torch.no_grad() context **switches off the autograd machinery**, and can be used for operations such as parameter updates."
      ]
    },
    {
      "metadata": {
        "id": "akOfEKFw34uI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Project\n",
        "\n",
        "Implement a two layer neural network in one of these microcontrollers Microchip (pic32) or Atmel (AtsamD21/Atmega) or STM32 or ESP32 for temperature compensation for water quality monitoring"
      ]
    },
    {
      "metadata": {
        "id": "aRyOWFoh-ynW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Pytorch Libraries"
      ]
    },
    {
      "metadata": {
        "id": "3cFATTM9HfWX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Introduction"
      ]
    },
    {
      "metadata": {
        "id": "Xa7u5iSvc5iq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "$$\\require {enclosed}\n",
        "\\fbox{torch.nn.Functional}\n",
        " \\xrightarrow{}\n",
        "\\fbox{torch.nn}\\\\$$\n",
        "\n",
        "**LAYERS**\n",
        "\n",
        "**torch.nn** exposes specific functionality like layers, activation functions, loss functions etc required for constructing networks and architectures.\n",
        "\n",
        "torch.nn is **built on top** of **torch.nn.Functional**.\n",
        "\n",
        "torch.nn has three main parts:\n",
        "\n",
        "1.   **Parameters** - learnable/trainable parameters, configurable parameters - \n",
        "2.   **Containers** - Containers include modules, sequences, ModuleList, ParameterList, ModuleDict, ParameterDict\n",
        "3.   **Layers** - Layers include linear, normalization, convolutional, pooling, padding, dropout, recurrent\n",
        "\n",
        "For layers with trainable parameters, we use torch.nn to create the layers or the model. We can do it two ways: \n",
        "\n",
        "1. **Using Sequence** - Simple models which dont require much customization\n",
        "2. **Using Class** - Used to define more complicated and custom models. Our layers are stored back in the instance of a class so we can easily access the layer and the trainable parameters later.\n",
        "\n",
        "In **sequence containers**, we use torch.nn.Sequential to compose layers from torch.nn. Thus to create a model that looks like \n",
        "\n",
        "$$\\require {enclosed}\n",
        "\\fbox{input}\n",
        " \\xrightarrow{}\n",
        "\\fbox{linear}\n",
        " \\xrightarrow{}\n",
        "\\fbox{relu}\n",
        "\\xrightarrow{}\n",
        "\\fbox{linear}\n",
        "\\xrightarrow{}\n",
        "\\fbox{sigmoid}\\\\$$\n",
        "\n",
        "\n",
        "* The class torch.nn.Linear does the job for us. For linear layer we need to multiply each input node with a weight, and add a bias. It applies a linear transformation to the incoming data, y=wx+b. Thus\n",
        "                     model = nn.Sequential(\n",
        "                     nn.Linear(n_in, n_h),\n",
        "                     nn.ReLU(),\n",
        "                     nn.Linear(n_h, n_out),\n",
        "                     nn.Sigmoid()\n",
        "                     )\n",
        "* The module torch.nn.Linear(input_size, output_size, bias=True) implements a fully-connected layer.  It takes as input a tensor of size N×C and produce a tensor of size N×D. \n",
        "* We didn’t specify the weight tensors as the **weights and biases are automatically randomized at creation**.  \n",
        "* Trainable parameters of a model are returned by **model.parameters()**.\n",
        "* The function **torch.manual_seed(1)** will give us the same result everytime we run the code.\n",
        "\n",
        "This Torch.nn.Linear applies a linear transformation to the incoming data, i.e. y = Ax + b. The input tensor given in forward(input) must be either a vector (1D tensor) or matrix (2D tensor). If the input is a matrix, then each row is assumed to be an input sample of given batch. The layer can be used without bias by setting bias = false.\n",
        "\n",
        "So given input matrix as follows.\n",
        "\n",
        "$$Sample_1 = [1, 3, 2, 6, 9]$$\n",
        "$$Sample_2 = [2, 7, 1, 4, 8]$$\n",
        "$$Sample_3 = [0, 2, 3, 6, 5]$$\n",
        "$$Sample_4 = [8, 4, 1, 7, 9]\\\\$$\n",
        "\n",
        "\n",
        "$${\\bf X_{4x5}} = \\underbrace{\n",
        "                \\left.\\left( \n",
        "                      \\begin{array}{ccccc}\n",
        "                             1&3&2&6 &9\\\\\n",
        "                             2&7&1&4 &8\\\\\n",
        "                             0&2&3&6 &5\\\\\n",
        "                             8&4&1&7&9\n",
        "                      \\end{array}\n",
        "                \\right)\\right\\}\n",
        "              }_{5\\text{ features}} \n",
        "              \\,4\\text{ samples}$$\n",
        "              \n",
        " $$torch.nn.Linear(features, number \\;of \\;neurons)$$\n",
        " \n",
        " $$torch.nn.Linear(5, 200)$$\n"
      ]
    },
    {
      "metadata": {
        "id": "7aoD2NYBMr3I",
        "colab_type": "code",
        "outputId": "7e910a4d-316f-4d2b-a1d8-3b6a4ebc9c18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "#!pip3 install torch torchvision\n",
        "import torch\n",
        "f = torch.nn.Linear(in_features = 10, out_features = 4)\n",
        "\n",
        "for n, p in f.named_parameters(): \n",
        "  print(n, p.size())\n",
        "  \n",
        "#for param in f.parameters():\n",
        "  #print(param.size())\n",
        "\n",
        "x = torch.empty(523, 10).normal_()\n",
        "y = f(x)\n",
        "y.size() \n",
        "  "
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "weight torch.Size([4, 10])\n",
            "bias torch.Size([4])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([523, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "metadata": {
        "id": "9qaPuLYwOvh5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "There are many methods available for each module to access its children — \n",
        "* model.modules() , model.named_modules() , model.parameters() model.named_parameters() , model.children() and model.named_children(). \n",
        "\n",
        "But the most used is model.parameters() , as this is used to access all the parameters recursively and hence can be used to pass to an optimizer for updating weights/bias.\n",
        "\n",
        "Parameters are of the type torch.nn.Parameter which is a Tensor with requires_grad to True, and known to be a model parameter by various utility functions, in particular torch.nn.Module.parameters()"
      ]
    },
    {
      "metadata": {
        "id": "kcccrd8lMjGq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**LOSS/COST**\n",
        "\n",
        "torch.nn has loss/cost functions called criteria.\n",
        "\n",
        "1. L1Loss, MSELoss, CrossEntropyLoss, CTCLoss, NLLLoss, PoissonNLLLoss, KLDivLoss, BCELoss, BCEWithLogitsLoss, MarginRankingLoss, HingeEmbeddingLoss, MultiLabelMarginLoss, SmoothL1Loss, SoftMarginLoss, MultiLabelSoftMarginLoss, CosineEmbeddingLoss, MultiMarginLoss, TripletMarginLoss\n",
        "\n",
        "The general syntax is\n",
        "\n",
        "> loss = torch.nn.MSELoss()\n",
        "\n",
        "> output = loss(input, target)\n",
        "\n",
        "> output.backward()\n",
        "\n",
        "**Note:** Criteria/Loss do not accept a tensor with requires_grad set to True for target.\n",
        "\n",
        "The first parameter of a loss is traditionally called the input and the second the target. These two quantities may be of different dimensions or even types for some losses (e.g.for classification).\n",
        "\n",
        "**OPTIMIZATION/WEIGHT UPDATES**\n",
        "\n",
        "torch.optim is a package implementing various optimization algorithms\n",
        "\n",
        "1. ASGD, Adadelta, Adagrad, Adam, Adamax, LBFGS, RMSprop, Rprop, SGD, SparseAdam\n",
        "\n",
        "To use torch.optim we construct an optimizer object, that will hold the current state and will update the parameters based on the computed gradients. \n",
        "\n",
        "We give it an iterable containing the parameters to optimize such as the learning rate, weight decay which are optimizer specific.\n",
        "\n",
        "> optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum=0.9)\n",
        "\n",
        "> optimizer = optim.Adam([var1, var2], lr = 0.0001)\n",
        "\n",
        "**model.parameters()** returns an iterator over our model’s parameters (**weights** and **biases**).\n",
        "\n",
        "If we need to move a model to GPU via .cuda(), we need to do so before constructing optimizers for it.\n",
        "\n",
        "All optimizers implement a step() method, that updates the parameters **called after** the gradients are computed using **backward()**.\n",
        "\n",
        "We also specify per-layer learning rates using dictionaries instead of iterables as below.\n",
        "\n",
        "```\n",
        "optim.SGD([\n",
        "                {'params': model.base.parameters()},\n",
        "                {'params': model.classifier.parameters(), 'lr': 1e-3}\n",
        "            ], lr=1e-2, momentum=0.9)\n",
        "```\n",
        "\n",
        "Here \n",
        "\n",
        "> model.base’s parameters will use the default learning rate of 1e-2, \n",
        "\n",
        "> model.classifier’s parameters will use a learning rate of 1e-3, \n",
        "\n",
        "> momentum of 0.9 will be used for all parameters\n",
        "\n",
        "**SCHEDULER - ADJUST LEARNING RATE**\n",
        "\n",
        "torch.optim.lr_scheduler provides several methods to adjust the learning rate based on the number of epochs.\n",
        "\n",
        "* LambdaLR, StepLR, MultiStepLR, ExponentialLR, CosineAnnealingLR, ReduceLROnPlateau\n",
        "\n",
        "optimizer = torch.optim.Adam(dual_encoder.parameters(), lr = 0.001)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma = 0.95)\n",
        "\n",
        "    for epoch in range(200):\n",
        "\n",
        "                  for i in range(len(label_array)):\n",
        "\n",
        "                              optimizer.zero_grad()\n",
        "                              \n",
        "                              loss.backward()\n",
        "                              \n",
        "                              optimizer.step()\n",
        "                               \n",
        "                   scheduler.step()\n",
        "                  \n"
      ]
    },
    {
      "metadata": {
        "id": "ptBaEwirb-OU",
        "colab_type": "code",
        "outputId": "3a3aaad0-5ad6-4118-d082-140ff9f04c3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        }
      },
      "cell_type": "code",
      "source": [
        "#!pip3 install torch torchvision\n",
        "import torch\n",
        "import torch.optim\n",
        "\n",
        "x = torch.randn((5, 5))\n",
        "w = torch.randn((5, 5),requires_grad=True)\n",
        "\n",
        "z = w.mm(x).mean() # Perform an operation\n",
        "\n",
        "opt = torch.optim.Adam([w], lr=2e-4, betas=(0.5, 0.999)) # Define the optimizer\n",
        "z.backward() # Calculate gradients\n",
        "print(w.data) # Print the weights\n",
        "\n",
        "opt.step() # Update w according to Adam's gradient update rules\n",
        "print(w.data) # Print updated weights after training step"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-1.0156,  1.0981,  1.3412, -0.8061, -1.8594],\n",
            "        [-0.3851,  0.0239, -0.5087, -0.1484,  2.1908],\n",
            "        [ 0.6656, -0.1045, -1.9714, -1.7648,  1.9828],\n",
            "        [-1.3232,  0.0486, -1.7935, -0.2444,  1.4195],\n",
            "        [-0.9755,  1.3868,  0.4843, -0.7731, -1.7140]])\n",
            "tensor([[-1.0154,  1.0979,  1.3414, -0.8059, -1.8596],\n",
            "        [-0.3849,  0.0237, -0.5085, -0.1482,  2.1906],\n",
            "        [ 0.6658, -0.1047, -1.9712, -1.7646,  1.9826],\n",
            "        [-1.3230,  0.0484, -1.7933, -0.2442,  1.4193],\n",
            "        [-0.9753,  1.3866,  0.4845, -0.7729, -1.7142]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6V4OXmdMjoYe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Thus torch.nn.Sequential expects a list of the layers that we want in the neural network. in our case our list has two things, we want a linear layer (y=wx+b) whose input is a vector of some length along with a non-linear activation function relu followed another linear layer with sigmoid activation function - this is a two layer neuron model\n",
        "\n",
        "There is also the implicit input layer which is understood."
      ]
    },
    {
      "metadata": {
        "id": "7B3DaHwMYrLx",
        "colab_type": "code",
        "outputId": "ecce7c2c-ba45-4ce6-eec8-2c2966bb3130",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "#device = torch.device(\"cpu\") # Uncomment this to run on CPU\n",
        "dtype = torch.float\n",
        "device = torch.device(\"cuda:0\") \n",
        "\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10\n",
        "\n",
        "# Create random input and output data\n",
        "x = torch.randn(N, D_in, device=device, dtype=dtype)  #64x1000\n",
        "y = torch.randn(N, D_out, device=device, dtype=dtype) #64x10\n",
        "\n",
        "# Randomly initialize weights\n",
        "w1 = torch.randn(D_in, H, device=device, dtype=dtype)  #1000 x 100\n",
        "w2 = torch.randn(H, D_out, device=device, dtype=dtype) #100 x 10\n",
        "\n",
        "# Use the nn package to define our model as a sequence of layers. nn.Sequential\n",
        "# is a Module which contains other Modules, and applies them in sequence to\n",
        "# produce its output. Each Linear Module computes output from input using a\n",
        "# linear function, and holds internal Tensors for its weight and bias.\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(D_in, H),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(H, D_out),\n",
        "    torch.nn.Sigmoid())\n",
        "\n",
        "model = model.cuda()\n",
        "\n",
        "# The nn package also contains definitions of popular loss functions; in this\n",
        "# case we will use Mean Squared Error (MSE) as our loss function.\n",
        "loss_function = torch.nn.MSELoss(reduction='sum').cuda()\n",
        "\n",
        "# Set the learning rate value\n",
        "learning_rate = 1e-4\n",
        "\n",
        "for t in range(5):\n",
        "  # Forward pass: compute predicted y by passing x to the model. Module objects\n",
        "  # override the __call__ operator so you can call them like functions. When\n",
        "  # doing so you pass a Tensor of input data to the Module and it produces\n",
        "  # a Tensor of output data. Basically construct the graph\n",
        "  y_pred=model(x)\n",
        "  \n",
        "  \n",
        "  # Compute and print loss. We pass Tensors containing the predicted and true\n",
        "  # values of y, and the loss function returns a Tensor containing the\n",
        "  # loss.\n",
        "  loss = loss_function(y_pred, y)\n",
        "  print(t, \"{:.20f}\".format(loss.item()))\n",
        "  \n",
        "  # Zero the gradients before running the backward pass.\n",
        "  model.zero_grad()\n",
        "  \n",
        "  # Backward pass: compute gradient of the loss with respect to all the learnable\n",
        "  # parameters of the model. Internally, the parameters of each Module are stored\n",
        "  # in Tensors with requires_grad=True, so this call will compute gradients for\n",
        "  # all learnable parameters in the model.\n",
        "  loss.backward()\n",
        "  \n",
        "  # Update the weights using gradient descent. Each parameter is a Tensor, so\n",
        "    # we can access its gradients like we did before.\n",
        "  with torch.no_grad():\n",
        "    for param in model.parameters():\n",
        "      param -= learning_rate * param.grad"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 719.92944335937500000000\n",
            "1 714.84710693359375000000\n",
            "2 709.86169433593750000000\n",
            "3 704.94543457031250000000\n",
            "4 700.09936523437500000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FtDLlfOKfED6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Up to this point we have updated the weights of our models by manually mutating the Tensors holding learnable parameters (with torch.no_grad() or .data to avoid tracking history in autograd). This is not a huge burden for simple optimization algorithms like stochastic gradient descent, but in practice we often train neural networks using more sophisticated optimizers like AdaGrad, RMSProp, Adam, etc.\n",
        "\n",
        "The optim package in PyTorch abstracts the idea of an optimization algorithm and provides implementations of commonly used optimization algorithms.\n",
        "\n",
        "In this example we will use the nn package to define our model as before, but we will optimize the model using the Adam algorithm provided by the **optim package**:"
      ]
    },
    {
      "metadata": {
        "id": "kwuR00STfGfL",
        "colab_type": "code",
        "outputId": "aa2e79df-eda3-4aef-cb1e-fe2aee9c10a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import torch\n",
        "\n",
        "# N is batch size; D_in is input dimension;\n",
        "# H is hidden dimension; D_out is output dimension.\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10\n",
        "\n",
        "# Create random Tensors to hold inputs and outputs\n",
        "x = torch.randn(N, D_in)\n",
        "y = torch.randn(N, D_out)\n",
        "\n",
        "# Use the nn package to define our model and loss function.\n",
        "model = torch.nn.Sequential(\n",
        "    torch.nn.Linear(D_in, H),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Linear(H, D_out))\n",
        "\n",
        "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
        "\n",
        "# Use the optim package to define an Optimizer that will update the weights of\n",
        "# the model for us. Here we will use Adam; the optim package contains many other\n",
        "# optimization algoriths. The first argument to the Adam constructor tells the\n",
        "# optimizer which Tensors it should update.\n",
        "learning_rate = 1e-4\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for t in range(5):\n",
        "    # Forward pass: compute predicted y by passing x to the model.\n",
        "    y_pred = model(x) # Compute the graph\n",
        "\n",
        "    # Compute and print loss.\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    print(t, \"{:.20f}\".format(loss.item()))\n",
        "\n",
        "    # Before the backward pass, use the optimizer object to zero all of the\n",
        "    # gradients for the variables it will update (which are the learnable\n",
        "    # weights of the model). This is because by default, gradients are\n",
        "    # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
        "    # is called. Checkout docs of torch.autograd.backward for more details.\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Backward pass: compute gradient of the loss with respect to model\n",
        "    # parameters\n",
        "    loss.backward()\n",
        "\n",
        "    # Calling the step function on an Optimizer makes an update to its\n",
        "    # parameters\n",
        "    optimizer.step()\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 735.59613037109375000000\n",
            "1 717.49017333984375000000\n",
            "2 699.83526611328125000000\n",
            "3 682.65753173828125000000\n",
            "4 665.90002441406250000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5sdyoU5cgS9q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Using Class\n",
        "\n",
        "In PyTorch when using classes, we define the Models as subclasses of torch.nn.Module. \n",
        "\n",
        "The two functions required while defining (inherenting) any model using class are:\n",
        "\n",
        "1. **__init__** - We define all the parametric (and sometimes non-parametric) layers. This function has to always be inherited first, then we define parameters of the layer such as the class variables i.e. self.x\n",
        "2. **forward**  - We connect all the layers and other functions to the input to form a graph.\n",
        "\n",
        "In the \\__init\\__ function,  we initialize the layers we want to use and Pytorch goes more low level and we have to specify the sizes of our network so that everything matches.\n",
        "\n",
        "In the forward method, we specify the connections of your layers. This means that we will use the layers we already initialized, in order to re-use the same layer for each forward pass of data we make.\n",
        "\n",
        "For layers that do not have trainable weights, we can use either the **layer form** (from torch.nn) or **connection form** (from torch.nn.functional), \n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "iiAcynHPgW6X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!pip3 install torch torchvision\n",
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class net(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(net,self).__init__() # Can also use torch.nn.Module.__init__(self)\n",
        "    self.linear1=torch.nn.Linear(D_in,H)\n",
        "    self.linear2=torch.nn.Linear(H,D_out)\n",
        "    \n",
        "  def forward(self,x):\n",
        "    h_relu = torch.nn.functional.relu(self.linear1(x))\n",
        "    #h_relu = self.linear1(x).clamp(min=0)\n",
        "    y_pred = self.linear2(h_relu)\n",
        "    return y_pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3u9ejcso5wqm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "During the **forward pass, the inputs must be passed** to the graph being constructed and any output must be returned for the loss to be calculated.\n",
        "\n",
        "In the class definition, you can see the inheritance from the base class nn.Module. The inherited class gets all of the functionality, capabilities in the parenthesis class when subclassing. We then can add additional functionality to it.\n",
        "\n",
        "Then, in the first line of the class initialization **( def \\__init\\__(self)**: ) we have the required super() function, which creates an instance of the base nn.Module class. We initialize the superclass functionality that is the superclass nn.module has to be built before we can add our pieces using it. i.e, we have to construct the superclass first.\n",
        "\n",
        "The **self keyboard** makes sure that the instantiating class is able instantiate its own data.\n",
        "\n",
        "The next lines is where we create **define** our fully connected **layers** as per the architecture diagram. A fully connected neural network layer is represented by the nn.Linear object, with the first argument in the definition being the number of nodes in layer l and the next argument being the number of nodes in layer l+1.\n",
        "\n",
        "Now we’ve setup the “skeleton” of our network architecture, we have to **define how data flows** through out network i.e, how the **computational graph** is connected. We do this by defining a **forward()** method in our class – this method overwrites a dummy method in the base class, and needs to be defined for each network.\n",
        "\n",
        "Note:\n",
        "1. Each class method should have an argument **self** as its **first argument**. **self** is a parameter common to all class methods. In general, a function is floating free, unencumbered whereas a class (instance) method has to be aware of the parent (and parent properties) so self is a way of passing the method a reference to the parent class. \n",
        "\n",
        "2. Variables created by the keyword **self** unique to each instance of the class\n",
        "\n",
        "3. Class variables are shared by all instances and dont have the keyword self.\n",
        "\n",
        "4. \\__init\\__ is the default method that is invoked when the object (instance) is first created. The method call to this method is immediate and automatic after the creation of the class instance. \n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "Re3ILc3k8NWn",
        "colab_type": "code",
        "outputId": "20fbf958-25e4-429e-cf78-0aca7d74c3ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "cell_type": "code",
      "source": [
        "N, D_in, H, D_out = 64, 1000, 100, 10\n",
        "\n",
        "x = torch.randn(N, D_in).cuda()\n",
        "y = torch.randn(N, D_out).cuda()\n",
        "\n",
        "#x = x.type(torch.FloatTensor)\n",
        "#y = y.type(torch.FloatTensor)\n",
        "\n",
        "mymodel = net()\n",
        "print(mymodel)\n",
        "\n",
        "# Move to gpu if available:\n",
        "if torch.cuda.is_available():\n",
        "  mymodel.cuda()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "net(\n",
            "  (linear1): Linear(in_features=1000, out_features=100, bias=True)\n",
            "  (linear2): Linear(in_features=100, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "x5skQCWV9UfN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The above step **creates** an **instance of the network architecture**. We then setup an optimizer and a loss criteria. In PyTorch, the optimizer knows how to optimize any attribute of type Parameter."
      ]
    },
    {
      "metadata": {
        "id": "oSIeO1Gf9reI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# create a stochastic gradient descent optimizer\n",
        "optimizer = torch.optim.SGD(mymodel.parameters(), lr=0.0009, momentum=0.9)\n",
        "# create a loss function\n",
        "#criterion = nn.NLLLoss()\n",
        "criterion = torch.nn.MSELoss(reduction='sum')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s9L4P2me96e-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We finally run the main training loop. The optimizer.step() does a single updation of all the parameters using the new gradients calculated for every backward call."
      ]
    },
    {
      "metadata": {
        "id": "iKCPl6TW-As2",
        "colab_type": "code",
        "outputId": "fad36bd5-0c59-4e3c-ccb2-5664b02e1fe1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "  optimizer.zero_grad()\n",
        "  model_output = mymodel(x)\n",
        "  loss = criterion(model_output, y)\n",
        "  print(epoch, \"{:.20f}\".format(loss.item()))\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 649.82720947265625000000\n",
            "1 355.57498168945312500000\n",
            "2 203.70831298828125000000\n",
            "3 94.66721343994140625000\n",
            "4 204.43922424316406250000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "R8S0iZgLCnjn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The special method **forward()** will **automatically** get called when the layer is calculated and its gonna get passed the data from previous layer."
      ]
    },
    {
      "metadata": {
        "id": "C4OX6_oYfqaP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Using Custom Class"
      ]
    },
    {
      "metadata": {
        "id": "70FUMfEJf6vD",
        "colab_type": "code",
        "outputId": "a2d97f03-2a88-49ed-f163-74d5f02b6c79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import torch\n",
        "\n",
        "\n",
        "class TwoLayerNet(torch.nn.Module):\n",
        "    def __init__(self, D_in, H, D_out):\n",
        "        \"\"\"\n",
        "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
        "        member variables.\n",
        "        \"\"\"\n",
        "        super(TwoLayerNet, self).__init__()\n",
        "        self.linear1 = torch.nn.Linear(D_in, H)\n",
        "        self.linear2 = torch.nn.Linear(H, D_out)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        In the forward function we accept a Tensor of input data and we must return\n",
        "        a Tensor of output data. We can use Modules defined in the constructor as\n",
        "        well as arbitrary operators on Tensors.\n",
        "        \"\"\"\n",
        "        #h_relu = self.linear1(x).clamp(min=0)\n",
        "        h_relu = torch.nn.functional.relu(self.linear1(x))\n",
        "        y_pred = self.linear2(h_relu)\n",
        "        return y_pred\n",
        "\n",
        "\n",
        "# N is batch size; D_in is input dimension;\n",
        "# H is hidden dimension; D_out is output dimension.\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10\n",
        "\n",
        "# Create random Tensors to hold inputs and outputs\n",
        "x = torch.randn(N, D_in)\n",
        "y = torch.randn(N, D_out)\n",
        "\n",
        "# Construct our model by instantiating the class defined above\n",
        "model = TwoLayerNet(D_in, H, D_out)\n",
        "\n",
        "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
        "# in the SGD constructor will contain the learnable parameters of the two\n",
        "# nn.Linear modules which are members of the model.\n",
        "criterion = torch.nn.MSELoss(reduction='sum')\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
        "\n",
        "for t in range(5):\n",
        "    # Forward pass: Compute predicted y by passing x to the model\n",
        "    y_pred = model(x) #Construct the graph\n",
        "\n",
        "    # Compute and print loss, can directly call nn.functional.mse_loss(out, y)since it is a non-parametric function\n",
        "    loss = criterion(y_pred, y)\n",
        "    print(t, \"{:.20f}\".format(loss.item()))\n",
        "\n",
        "    # Zero gradients, perform a backward pass, and update the weights.\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    \n",
        "    optimizer.step()\n",
        "    \n",
        "sum([p.numel() for p in model.parameters()])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 625.60498046875000000000\n",
            "1 575.80346679687500000000\n",
            "2 532.95007324218750000000\n",
            "3 495.57931518554687500000\n",
            "4 462.55068969726562500000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "101110"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "metadata": {
        "id": "7tQS9ZSY39Cv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Notice that we **never explicitly call forward pass** defined within the class. We always call the Module instance afterwards instead since it takes care of running the registered hooks while the former silently ignores them.\n",
        "\n",
        "We can also obtain the number of trainable parameters of a model using in the last line of code."
      ]
    },
    {
      "metadata": {
        "id": "Kp2fjWPtO_YK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**torch.nn only supports mini-batches**. The entire torch.nn package only supports inputs that are a mini-batch of samples, and not a single sample. For example, nn.Conv2d will take in a 4D Tensor of nSamples x nChannels x Height x Width. If you have a single sample, just use input.unsqueeze(0) to add a fake batch dimension.\n",
        "\n",
        "This is motivated by the computational speed-up it induces. To evaluate a module on a sample, both the module’s parameters and the sample have to be first copied into cache memory, which is fast but small. For any model of reasonable size, only a fraction of its parameters can be kept in cache, so a module’s parameter have to be copied there every time it is used. These memory transfers are slower than the computation itself.This is the main reason for batch processing: it cuts down to one per module per batch the number of copies of parameters to the cache. It also cuts down the use of Python loops, which are awfully slow."
      ]
    },
    {
      "metadata": {
        "id": "ETTfFXzKb7ho",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###torch.nn vs torch.nn.functional"
      ]
    },
    {
      "metadata": {
        "id": "VJ5Ql6o39I3-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A nn.Module is actually a OO wrapper around the functional interface, that contains a number of utility methods, like eval() and parameters(), and it automatically creates the parameters of the modules for us. \n",
        "\n",
        "We can use the functional interface whenever we want, but that requires us to define the weights by hand. \n",
        "\n",
        "The difference between torch.nn and torch.nn.functional is very subtle. In fact, many torch.nn.functional have a corresponding equivalent in torch.nn. \n",
        "\n",
        "In many code samples, we **use torch.nn.functional for simpler operations that have no trainable parameters or configurable parameters**. For example for ReLU, we do not require any learnable parameters to be called in forward() method hence it can be defined using the torch.nn.functional interface.\n",
        "\n",
        "Alternatively, in some sections, we use torch.nn.Sequential to compose layers from torch.nn only. \n",
        "\n",
        "Both approaches are simple and more like a coding style issue rather than any major implementation differences. There isnt any performance difference.\n",
        "\n",
        "If all layers are defined with nn.functional, then all variables, such as weights, bias, etc., need to be manually defined by the user, which is very inconvenient.\n",
        "\n",
        "The functions in torch.nn.functional are just some arithmetical operations, not the layers which have trainable parameters such as weights and bias terms.\n",
        "\n",
        "As a result, layers with parameters are usually initialized in init to be shared by the whole module, while some connections or simple operations without parameters can be defined in forward to be used in forward propagation.\n",
        "   "
      ]
    },
    {
      "metadata": {
        "id": "HJcmLF-JUixs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Using Mixed Approach (Class and Non-class)"
      ]
    },
    {
      "metadata": {
        "id": "_YeyqtUZUr3f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class Model(torch.nn.Module):\n",
        "    super().__init__()\n",
        "    self.feature_extractor = nn.Sequential(\n",
        "        Conv2d(3, 12, kernel_size=3, padding=1, stride=1),\n",
        "        Conv2d(12, 24, kernel_size=3, padding=1, stride=1),)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.feature_extractor(x)\n",
        "        return x    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WNEd8Oze-QGy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Initializing parameters of the model\n",
        "\n",
        "1. We can loop over the parameters of the module using model.parameters() and then initializing each with tensor functions such as exponential, uniform, fill etc.\n",
        "\n",
        "2. Every module have an attribute definition **.apply** . We can call this apply on the module and pass it a function which handles the initialization for each of the parameter. Whenever .apply is called on a module, it is called on each of the module and parameter recursively.\n",
        "\n",
        "3. We could use **torch.nn.init** module to initialize our parameters in more practical way. Suppose we have a parameter named m, and we need to initialize it using Xavier (Glorot) initialization, then we can do torch.nn.init.xavier_uniform(m) . Now, combining this with .apply we can use the module torch.nn.init to initialize parameters of all sort.\n",
        "\n",
        "Applies function recursively to every submodule (as returned by .children()) as well as self. Typical use case includes initializing the parameters of a model (we also use torch-nn-init)."
      ]
    },
    {
      "metadata": {
        "id": "2M28fGzJXpg9",
        "colab_type": "code",
        "outputId": "03bd4ebe-8fd6-4ade-a5ea-7f32de77aefb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        }
      },
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "def init_weights(m):\n",
        "        print(m)\n",
        "        if type(m) == nn.Linear:\n",
        "            #m.weight.data.fill_(1.0)\n",
        "            nn.init.xavier_uniform_(m.weight.data)\n",
        "            print(m.weight.data)\n",
        "\n",
        "net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
        "net.apply(init_weights)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear(in_features=2, out_features=2, bias=True)\n",
            "tensor([[-0.3341,  0.1696],\n",
            "        [ 1.0626,  0.4128]])\n",
            "Linear(in_features=2, out_features=2, bias=True)\n",
            "tensor([[-1.1414, -0.7872],\n",
            "        [ 0.1243, -0.1141]])\n",
            "Sequential(\n",
            "  (0): Linear(in_features=2, out_features=2, bias=True)\n",
            "  (1): Linear(in_features=2, out_features=2, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=2, out_features=2, bias=True)\n",
              "  (1): Linear(in_features=2, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "qk3CAqvYDMo7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Accessing Parameters and Modules"
      ]
    },
    {
      "metadata": {
        "id": "bqRkU6WACxho",
        "colab_type": "code",
        "outputId": "476d6a62-f528-47bf-9cbf-f82be6dfe61f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        }
      },
      "cell_type": "code",
      "source": [
        "seq_net = nn.Sequential(\n",
        "\tnn.Linear(2,4),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(4,1)\n",
        ")\n",
        "\n",
        "print(seq_net[0])\n",
        "print(seq_net[1])\n",
        "print(seq_net[0].weight)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear(in_features=2, out_features=4, bias=True)\n",
            "Tanh()\n",
            "Parameter containing:\n",
            "tensor([[-0.2519,  0.0680],\n",
            "        [ 0.2745, -0.6395],\n",
            "        [-0.6839, -0.1879],\n",
            "        [-0.4628,  0.4401]], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "A1e89shabMZm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**parameters(recurse=True)**\n",
        "\n",
        "Returns an iterator over module parameters. This is typically passed to an optimizer."
      ]
    },
    {
      "metadata": {
        "id": "NDhSyMHSbxjA",
        "colab_type": "code",
        "outputId": "d156b1cb-dd6d-4431-d804-53ca95311b36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        }
      },
      "cell_type": "code",
      "source": [
        "for param in net.parameters():\n",
        "  print(param,\":\",param.size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.3341,  0.1696],\n",
            "        [ 1.0626,  0.4128]], requires_grad=True) : torch.Size([2, 2])\n",
            "Parameter containing:\n",
            "tensor([-0.3686, -0.4479], requires_grad=True) : torch.Size([2])\n",
            "Parameter containing:\n",
            "tensor([[-1.1414, -0.7872],\n",
            "        [ 0.1243, -0.1141]], requires_grad=True) : torch.Size([2, 2])\n",
            "Parameter containing:\n",
            "tensor([0.1928, 0.0775], requires_grad=True) : torch.Size([2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-KchXpzrbMwk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**named_parameters(prefix='', recurse=True)**\n",
        "\n",
        "Returns an iterator over module parameters, yielding both the name of the parameter as well as the parameter itself."
      ]
    },
    {
      "metadata": {
        "id": "wpIHtTPxbcWn",
        "colab_type": "code",
        "outputId": "c7613ec9-0b44-4398-874a-89e84883d105",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "cell_type": "code",
      "source": [
        "for name, param in net.named_parameters():\n",
        "    print(name,\": \",param.size())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.weight :  torch.Size([2, 2])\n",
            "0.bias :  torch.Size([2])\n",
            "1.weight :  torch.Size([2, 2])\n",
            "1.bias :  torch.Size([2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AgdcCNllY0Ly",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**buffers(recurse=True)**\n",
        "\n",
        "Returns an iterator over module buffers. If  recurse=True, then yields buffers of this module and all submodules. Otherwise, yields only buffers that are direct members of this module"
      ]
    },
    {
      "metadata": {
        "id": "9O3f4Io0XkOi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for buf in net.buffers():\n",
        "  print(type(buf.data), buf.size())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "izOJrj95ZFtl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**named_buffers(prefix='', recurse=True)**\n",
        "\n",
        "Returns an iterator over module buffers, yielding both the name of the buffer as well as the buffer itself."
      ]
    },
    {
      "metadata": {
        "id": "Z3iaTffGY2RY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Children( modules)**\n",
        "\n",
        "Returns an iterator over immediate children modules"
      ]
    },
    {
      "metadata": {
        "id": "j6VnkxKeYGMX",
        "colab_type": "code",
        "outputId": "ced3c8d6-1dad-4268-ada8-dda3134cdc18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "for child in net.children():\n",
        "  print(child)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear(in_features=2, out_features=2, bias=True)\n",
            "Linear(in_features=2, out_features=2, bias=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1-csKOtFZXj3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**named_children()**\n",
        "\n",
        "Returns an iterator over immediate children modules, yielding both the name of the module as well as the module itself."
      ]
    },
    {
      "metadata": {
        "id": "Uf23rQKXZfAB",
        "colab_type": "code",
        "outputId": "e1d236aa-9e8d-4664-bf43-990009da6d17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "for name, module in net.named_children():\n",
        "  print(name,\" : \", module)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0  :  Linear(in_features=2, out_features=2, bias=True)\n",
            "1  :  Linear(in_features=2, out_features=2, bias=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0G3E2ZuTYoX1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**modules**()\n",
        "\n",
        "Returns an iterator over all modules in the network."
      ]
    },
    {
      "metadata": {
        "id": "bGIMAT7uYytA",
        "colab_type": "code",
        "outputId": "3c439029-a563-46d1-c722-30adbea90f30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "cell_type": "code",
      "source": [
        "for idx, m in enumerate(net.modules()):\n",
        "  print(idx, ' ', m)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0   Sequential(\n",
            "  (0): Linear(in_features=2, out_features=2, bias=True)\n",
            "  (1): Linear(in_features=2, out_features=2, bias=True)\n",
            ")\n",
            "1   Linear(in_features=2, out_features=2, bias=True)\n",
            "2   Linear(in_features=2, out_features=2, bias=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Zz6-yS20aJ9f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**named_modules(memo=None, prefix='')**\n",
        "\n",
        "Returns an iterator over all modules in the network, yielding both the name of the module as well as the module itself. Duplicate modules are returned only once."
      ]
    },
    {
      "metadata": {
        "id": "woADNkidZOvI",
        "colab_type": "code",
        "outputId": "dc2dbe8d-798d-4b86-cf56-a7676c2e1705",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "cell_type": "code",
      "source": [
        "l = nn.Linear(2, 2)\n",
        "net = nn.Sequential(l, l)\n",
        "for idx, m in enumerate(net.named_modules()):\n",
        "  print(idx,\":\",m)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 : ('', Sequential(\n",
            "  (0): Linear(in_features=2, out_features=2, bias=True)\n",
            "  (1): Linear(in_features=2, out_features=2, bias=True)\n",
            "))\n",
            "1 : ('0', Linear(in_features=2, out_features=2, bias=True))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "b2BfydI0YvW9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Move to cpu\n",
        "\n",
        "Moves all model parameters and buffers to the CPU"
      ]
    },
    {
      "metadata": {
        "id": "gqwuK4wUYWXC",
        "colab_type": "code",
        "outputId": "13f2c9ba-ce6e-4ae1-82eb-3405d57f2e53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "cell_type": "code",
      "source": [
        "net.cpu() "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Linear(in_features=2, out_features=2, bias=True)\n",
              "  (1): Linear(in_features=2, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "metadata": {
        "id": "APgmF--uUKau",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Move to GPU\n",
        "\n",
        "The main idea behind PyTorch’s GPU handling is that if an object (tensor, Variable, Parameter) is available on a device with id 0 , then after performing some operation (transformation) on it, the resultant object will also be stored on the same device 0. Further, **operations cannot be performed on two objects residing on different devices**, i.e., no a+b is allowed if a is on CPU and b is on GPU.\n",
        "\n",
        "So, in order to run our training on GPU, it is enough to copy our model parameters and our input data on the GPU. This is done using** .cuda()** attribute available for all tensors and modules. For tensor, it is straightforward that this will copy the tensor to GPU, but for module (model), calling .cuda() will recursively copy all child modules and parameters to GPU.\n",
        "\n",
        "We can verify if torch.cuda.is_available() returns True to check if a GPU is available on a machine.\n",
        "\n",
        "cuda(device=None)"
      ]
    },
    {
      "metadata": {
        "id": "uMMaXKvOOt2n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Access individual layer weights and gradients"
      ]
    },
    {
      "metadata": {
        "id": "KI4vIYivw9w3",
        "colab_type": "code",
        "outputId": "d5b29434-f797-425f-b808-486b0325d137",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "x = torch.randn(1, 2).cuda()\n",
        "y = torch.randn(1, 1).cuda()\n",
        "\n",
        "\n",
        "class net(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(net,self).__init__()\n",
        "    self.linear1=torch.nn.Linear(2,1)\n",
        "    self.linear2=torch.nn.Linear(1,2)\n",
        "    \n",
        "  def forward(self,x):\n",
        "    h_relu = torch.nn.functional.relu(self.linear1(x))\n",
        "    y_pred = self.linear2(h_relu)\n",
        "    return y_pred\n",
        "\n",
        "mymodel = net()\n",
        "  \n",
        "if torch.cuda.is_available():\n",
        "  mymodel.cuda()\n",
        "\n",
        "optimizer = torch.optim.SGD(mymodel.parameters(), lr=0.0009, momentum=0.9)\n",
        "\n",
        "# Before training\n",
        "print(\"predict (before training)\", mymodel(x).data[0])\n",
        "\n",
        "loss_col = []\n",
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "  optimizer.zero_grad()\n",
        "  model_output = mymodel(x)\n",
        "  loss = torch.nn.functional.mse_loss(model_output, y) # direct use of mse_loss from nn.functional\n",
        "  loss_col.append(loss)\n",
        "  #print(epoch, \"{:.20f}\".format(loss.item()))\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  print(\"progress:\", epoch, loss.data.item())\n",
        "\n",
        "plt.plot(loss_col)\n",
        "plt.show()  \n",
        "  \n",
        "#print(list(mymodel.parameters()))\n",
        "\n",
        "# After training\n",
        "hour_var = torch.Tensor([1.0,4.0]).cuda()\n",
        "y_pred = mymodel(hour_var)\n",
        "print(\"predict (after training)\", y_pred.data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predict (before training) tensor([ 0.6933, -0.2722], device='cuda:0')\n",
            "progress: 0 4.059016227722168\n",
            "progress: 1 4.050872802734375\n",
            "progress: 2 4.035438537597656\n",
            "progress: 3 4.01350736618042\n",
            "progress: 4 3.9858198165893555\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAFKCAYAAAAnj5dkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIuJh/n8TLdgKmwoSBQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fbf339a2780>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "predict (after training) tensor([-0.0500,  1.1712], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iIZC4wXmXVHe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Sometimes it gets difficult to plot our loss with respect to every parameter.** As a result, we will store the loss in a list for each iteration.**\n",
        "\n",
        "We simply create an empty list, calculate the loss, and then store the loss value in the list. We plot out the loss for each iteration."
      ]
    },
    {
      "metadata": {
        "id": "iQIWp6_FOsry",
        "colab_type": "code",
        "outputId": "c2baa823-6caa-4a56-da60-461ede755ef1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "cell_type": "code",
      "source": [
        "print(mymodel.linear1.weight.grad.size())\n",
        "print(mymodel.linear1.weight.data[0]) \n",
        "print(mymodel.linear1.weight.data.norm())       # norm of the weight\n",
        "print(mymodel.linear1.weight.grad.data.norm())  # norm of the gradients"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 2])\n",
            "tensor([-0.2707,  0.3983], device='cuda:0')\n",
            "tensor(0.4816, device='cuda:0')\n",
            "tensor(0.1064, device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IAO8Pv-wHP8x",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Save and Load Models\n",
        "\n",
        "You can save the models and load them back for inference. They are two ways of doing it:\n",
        "\n",
        "1. Save the **entire model** and load it back - torch.save function saves a serialized object to disk. This function uses Python’s pickle utility for serialization. Models, tensors, and dictionaries of all kinds of objects can be saved using this function. Similarly torch.load function uses pickle’s unpickling facilities to deserialize pickled object files to memory. This function also facilitates the device to load the data into.\n",
        "\n",
        "2. Save **only model** parameters - Neural network modules as well as optimizers have the ability to save and load their internal state using .state_dict(). With this we can continue training from previously saved state dicts and if needed\n",
        "we'd just need to call .load_state_dict(state_dict). \n",
        "\n",
        " * torch.save(model.state_dict(), '/results/model.pth')\n",
        "\n",
        " * torch.save(optimizer.state_dict(), '/results/optimizer.pth')\n",
        " \n",
        "A **state_dict** is simply a Python dictionary object that maps each layer with learnable parameters (convolutional layers, linear layers, etc.) to its parameter tensor (weights and biases accessed by accessed with model.parameters()).\n",
        "\n",
        "**Optimizer objects** (torch.optim) also have a **state_dict**, which contains information about the optimizer’s state, as well as the hyperparameters used\n",
        "\n",
        "Because state_dict objects are Python dictionaries, they can be easily saved, updated, altered, and restored, adding a great deal of modularity to PyTorch models and optimizers.\n"
      ]
    },
    {
      "metadata": {
        "id": "ZOtUELo4ypLa",
        "colab_type": "code",
        "outputId": "cb969b48-c6bc-49b2-856a-ed26ccd339b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        }
      },
      "cell_type": "code",
      "source": [
        "# Print model's state_dict\n",
        "print(\"Model's state_dict:\")\n",
        "for param_tensor in mymodel.state_dict():\n",
        "    print(\"\\n\",param_tensor, \"\\t\", mymodel.state_dict()[param_tensor].size())\n",
        "\n",
        "# Print optimizer's state_dict\n",
        "print(\"\\nOptimizer's state_dict:\")\n",
        "for var_name in optimizer.state_dict():\n",
        "    print(\"\\n\",var_name, \"\\t\", optimizer.state_dict()[var_name])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model's state_dict:\n",
            "\n",
            " linear1.weight \t torch.Size([1, 2])\n",
            "\n",
            " linear1.bias \t torch.Size([1])\n",
            "\n",
            " linear2.weight \t torch.Size([2, 1])\n",
            "\n",
            " linear2.bias \t torch.Size([2])\n",
            "\n",
            "Optimizer's state_dict:\n",
            "\n",
            " state \t {140457425290800: {'momentum_buffer': tensor([[ 0.1319, -0.4384]], device='cuda:0')}, 140457425291016: {'momentum_buffer': tensor([1.2258], device='cuda:0')}, 140457425291088: {'momentum_buffer': tensor([[3.1567],\n",
            "        [1.9048]], device='cuda:0')}, 140457425291160: {'momentum_buffer': tensor([9.9480, 6.0028], device='cuda:0')}}\n",
            "\n",
            " param_groups \t [{'lr': 0.0009, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [140457425290800, 140457425291016, 140457425291088, 140457425291160]}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "e1cYLs2yMxvv",
        "colab_type": "code",
        "outputId": "165c2061-bc13-46d2-c7cb-43b32510acb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "cell_type": "code",
      "source": [
        "# Save only model parameters\n",
        "torch.save(mymodel.state_dict(), \"/content/rahul.pth\")\n",
        "\n",
        "# Load only model parameters\n",
        "# To re-read the parameters of the model, first we need to redefine the model once, then re-read the parameters \n",
        "mymodel_n = net().cuda()\n",
        "mymodel_n.load_state_dict(torch.load(\"/content/rahul.pth\"))\n",
        "\n",
        "print(mymodel_n)\n",
        "# print(list(mymodel_n.parameters()))\n",
        "\n",
        "# model.eval()\n",
        "hour_var = torch.Tensor([1.0,4.0]).cuda()\n",
        "y_pred_n = mymodel_n(hour_var)\n",
        "print(\"predict (after training)\", y_pred_n.data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "net(\n",
            "  (linear1): Linear(in_features=2, out_features=1, bias=True)\n",
            "  (linear2): Linear(in_features=1, out_features=2, bias=True)\n",
            ")\n",
            "predict (after training) tensor([-0.0500,  1.1712], device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9wwKwmpMz5iP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Notice that the load_state_dict() function takes a dictionary object, NOT a path to a saved object. This means that you must **deserialize the saved state_dict** before you pass it to the load_state_dict() function. For example, you CANNOT load using model.load_state_dict(PATH)"
      ]
    },
    {
      "metadata": {
        "id": "CzsYqVYU07oR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Save entire model**"
      ]
    },
    {
      "metadata": {
        "id": "KzrIIkbD00hj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Saving a model in the below way will save the entire module using Python’s pickle module. The **disadvantage** of this approach is that the **serialized data is bound to the specific classes** and the exact directory structure used when the model is saved. The reason for this is because pickle does not save the model class itself. Rather, it saves a path to the file containing the class, which is used during load time. Because of this, your code can break in various ways when used in other projects or after refactors."
      ]
    },
    {
      "metadata": {
        "id": "YarS8tKwOPG5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Save the entire model\n",
        "torch.save(mymodel,\"/content/rahule.pt\") \n",
        "\n",
        "# Load the entire model\n",
        "mymodel_nf = torch.load(\"/content/rahule.pt\")\n",
        "\n",
        "# model.eval()\n",
        "hour_var = torch.Tensor([1.0,4.0]).cuda()\n",
        "y_pred_nf = mymodel_nf(hour_var)\n",
        "print(\"predict (after training)\", y_pred_nf.data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n-dGczOo1AcS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Checkpoint saving**"
      ]
    },
    {
      "metadata": {
        "id": "4hQSQ9LRC24E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "When saving using a general checkpoint (to be used for either inference or resuming training) we must save more than just the model’s state_dict. It is important to also save the optimizer’s state_dict, as this contains buffers and parameters that are updated as the model trains. \n",
        "\n",
        "Other items that we may want to save are the epoch we left off on, the latest recorded training loss, external torch.nn.Embedding layers, etc.\n",
        "\n",
        "To save multiple components, organize them in a dictionary and use torch.save() to serialize the dictionary. A common PyTorch convention is to save these checkpoints using the .tar file extension.\n",
        "\n",
        "To **load the items, first initialize the model and optimizer**, then load the dictionary locally using torch.load(). From here, you can easily access the saved items by simply querying the dictionary as you would expect.\n",
        "\n",
        "Remember that you must **call model.eval()** to set dropout and batch normalization layers to evaluation mode **before running inference**. Failing to do this will yield inconsistent inference results. If you wish to resuming training, call model.train() to ensure these layers are in training mode."
      ]
    },
    {
      "metadata": {
        "id": "bIKP7e8Z1Ije",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Checkpoint save\n",
        "torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': mymodel.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss,\n",
        "            ...\n",
        "            }, PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-eILYwXF1MIG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Checkpoint load\n",
        "model = TheModelClass(*args, **kwargs)\n",
        "optimizer = TheOptimizerClass(*args, **kwargs)\n",
        "\n",
        "checkpoint = torch.load(PATH)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "epoch = checkpoint['epoch']\n",
        "loss = checkpoint['loss']\n",
        "\n",
        "model.eval()\n",
        "# - or -\n",
        "model.train()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9JnIRg6LDNl-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Multiple Models in One File**"
      ]
    },
    {
      "metadata": {
        "id": "iQts7XInDbT8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "When saving a model comprised of multiple torch.nn.Modules, such as a GAN, a sequence-to-sequence model, or an ensemble of models, you follow the same approach as when we are saving a general checkpoint. \n",
        "\n",
        "In other words, save a dictionary of each model’s state_dict and corresponding optimizer. As mentioned before, we can save any other items that may aid us in resuming training by simply appending them to the dictionary.\n",
        "\n",
        "A common PyTorch convention is to save these checkpoints using the .tar file extension.\n",
        "\n",
        "To load the models, first initialize the models and optimizers, then load the dictionary locally using torch.load(). From here, you can easily access the saved items by simply querying the dictionary as you would expect.\n",
        "\n",
        "Remember that you must call model.eval() to set dropout and batch normalization layers to evaluation mode before running inference. Failing to do this will yield inconsistent inference results. If you wish to resuming training, call model.train() to set these layers to training mode."
      ]
    },
    {
      "metadata": {
        "id": "_8oHMGgODQpi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "torch.save({\n",
        "            'modelA_state_dict': modelA.state_dict(),\n",
        "            'modelB_state_dict': modelB.state_dict(),\n",
        "            'optimizerA_state_dict': optimizerA.state_dict(),\n",
        "            'optimizerB_state_dict': optimizerB.state_dict(),\n",
        "            ...\n",
        "            }, PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GmVwSZoEDSlw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "modelA = TheModelAClass(*args, **kwargs)\n",
        "modelB = TheModelBClass(*args, **kwargs)\n",
        "optimizerA = TheOptimizerAClass(*args, **kwargs)\n",
        "optimizerB = TheOptimizerBClass(*args, **kwargs)\n",
        "\n",
        "checkpoint = torch.load(PATH)\n",
        "modelA.load_state_dict(checkpoint['modelA_state_dict'])\n",
        "modelB.load_state_dict(checkpoint['modelB_state_dict'])\n",
        "optimizerA.load_state_dict(checkpoint['optimizerA_state_dict'])\n",
        "optimizerB.load_state_dict(checkpoint['optimizerB_state_dict'])\n",
        "\n",
        "modelA.eval()\n",
        "modelB.eval()\n",
        "# - or -\n",
        "modelA.train()\n",
        "modelB.train()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2_HiezyOKic9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Using Parameters from a Different Model**\n",
        "\n",
        "Partially loading a model or loading a partial model are common scenarios when transfer learning or training a new complex model. Leveraging trained parameters, even if only a few are usable, will help to warmstart the training process and hopefully help your model converge much faster than training from scratch.\n",
        "\n",
        "Whether you are loading from a partial state_dict, which is missing some keys, or loading a state_dict with more keys than the model that you are loading into, you can set the strict argument to False in the load_state_dict() function to ignore non-matching keys.\n",
        "\n",
        "If you want to load parameters from one layer to another, but some keys do not match, simply change the name of the parameter keys in the state_dict that you are loading to match the keys in the model that you are loading into."
      ]
    },
    {
      "metadata": {
        "id": "OqTeB-Z5K_np",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "torch.save(modelA.state_dict(), PATH)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qCXsosKyLBqW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "modelB = TheModelBClass(*args, **kwargs)\n",
        "modelB.load_state_dict(torch.load(PATH), strict=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tw8_-OZnK8Dh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Saving & Loading Model Across Devices**\n",
        "\n",
        "1. Save on GPU, Load on CPU\n",
        "2. Save on GPU, Load on GPU\n",
        "3. Save on CPU, Load on GPU"
      ]
    },
    {
      "metadata": {
        "id": "3i4x4-I7Xw24",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Save on GPU, Load on CPU**\n",
        "\n",
        "When loading a model on a CPU that was trained with a GPU, **pass torch.device('cpu') to the map_location argument** in the torch.load() function. In this case, the storages underlying the tensors are dynamically remapped to the CPU device using the map_location argument."
      ]
    },
    {
      "metadata": {
        "id": "hbTWVjiLXXQ-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Save\n",
        "torch.save(model.state_dict(), PATH)\n",
        "\n",
        "# Load\n",
        "device = torch.device('cpu')\n",
        "model = TheModelClass(*args, **kwargs)\n",
        "model.load_state_dict(torch.load(PATH, map_location=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZNlqIwh8jEPO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Save on CPU, Load on GPU**\n",
        "\n",
        "When loading a model on a GPU that was trained and saved on CPU, set the map_location argument in the torch.load() function to cuda:device_id. This loads the model to a given GPU device. Next, be sure to call model.to(torch.device('cuda')) to convert the model’s parameter tensors to CUDA tensors. Finally, be sure to use the .to(torch.device('cuda')) function on all model inputs to prepare the data for the CUDA optimized model. Note that calling my_tensor.to(device) returns a new copy of my_tensor on GPU. It does NOT overwrite my_tensor. Therefore, remember to manually overwrite tensors: my_tensor = my_tensor.to(torch.device('cuda'))."
      ]
    },
    {
      "metadata": {
        "id": "yDjb3LREjHkw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), PATH)\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "model = TheModelClass(*args, **kwargs)\n",
        "model.load_state_dict(torch.load(PATH, map_location=\"cuda:0\"))  # Choose whatever GPU device number you want\n",
        "model.to(device) # Model to GPU\n",
        "# Make sure to call the code; input = input.to(device) on any input tensors that you feed to the model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o8ueZ6sHa0uR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Save on GPU, Load on GPU**\n",
        "\n",
        "When loading a model on a GPU that was trained and saved on GPU, simply convert the initialized model to a CUDA optimized model using model.to(torch.device('cuda')). Also, be sure to use the .to(torch.device('cuda')) function on all model inputs to prepare the data for the model. Note that calling my_tensor.to(device) returns a new copy of my_tensor on GPU. It does NOT overwrite my_tensor. Therefore, remember to manually overwrite tensors: my_tensor = my_tensor.to(torch.device('cuda'))."
      ]
    },
    {
      "metadata": {
        "id": "t2UL8dexa5xv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Save\n",
        "torch.save(model.state_dict(), PATH)\n",
        "\n",
        "# Load\n",
        "device = torch.device(\"cuda\")\n",
        "model = TheModelClass(*args, **kwargs)\n",
        "model.load_state_dict(torch.load(PATH))\n",
        "model.to(device) # Model to GPU\n",
        "# Make sure to call the code; input = input.to(device) on any input tensors that you feed to the model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "knscVU6Zy410",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Forward and Backward Hooks\n",
        "\n",
        "For inspecting / modifying the output and grad_output of a layer during forward or backward propogation we introduce hooks. \n",
        "\n",
        "A **hook is a function that can be registered on a module or a tensor**.\n",
        "\n",
        "The hook can be a forward hook or a backward hook. \n",
        "\n",
        "The **register_forward_hook** & **register_backward_hook** functions are similar to the register_hook variable function, which can be register_hook when the module propagates forward or backpropagates.\n",
        "\n",
        "A **forward hook function is executed each time the forward propagation execution ends**. \n",
        "\n",
        "The forward-propagating hook function has the following form: \n",
        "\n",
        "**hook(module, input, output)** -> None, \n",
        "\n",
        "and backpropagation has the following form: \n",
        "\n",
        "**hook(module, grad_input, grad_output)** -> Tensor or None .\n",
        "\n",
        "The hook function should not modify the input and output, and **should be deleted in time after use** to avoid running the hook every time which can increase the running load. \n",
        "\n",
        "Hook functions are mainly used in scenarios where some intermediate results are obtained, such as the output of a layer in the middle or the gradient of a layer. These results should have been written in the forward function, but if you add these processing to the forward function, it may make the processing logic more complicated. At this time, it is more appropriate to use the hook technique. \n",
        "\n",
        "Let's consider a scenario. There is a pre-trained model that needs to extract the output of a layer (not the last layer) of the model as a feature, but does not want to modify its original model definition file. You can use the hook function. The code of the implementation is given below.\n",
        "\n",
        "PyTorch recursively applies any hook to all submodules on a register_forward_hook() call."
      ]
    },
    {
      "metadata": {
        "id": "e-FnbfBn0wF9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!pip3 install torch torchvision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6Ox2bMmszm9c",
        "colab_type": "code",
        "outputId": "b1ed4406-3b86-4067-cbf0-94677dcbae6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "x = torch.randn(1, 2).cuda()\n",
        "y = torch.randn(1, 1).cuda()\n",
        "\n",
        "\n",
        "class net(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(net,self).__init__()\n",
        "    self.linear1=torch.nn.Linear(2,1)\n",
        "    self.linear2=torch.nn.Linear(1,2)\n",
        "    \n",
        "  def forward(self,x):\n",
        "    h_relu = torch.nn.functional.relu(self.linear1(x))\n",
        "    y_pred = self.linear2(h_relu)\n",
        "    return y_pred\n",
        "  \n",
        "def forwardcall(module,input,output):\n",
        "  print(\"\\nInside forward of \", module,\" : \",input,\" : \", output)\n",
        "  \n",
        "def backwardcall(module, grad_input, grad_output):\n",
        "  print(\"Inside backward of \", module,\" : \",grad_input,\" : \", grad_output)\n",
        "\n",
        "mymodel = net()\n",
        "\n",
        "print(mymodel)\n",
        "hook1 = mymodel.linear1.register_forward_hook(forwardcall)\n",
        "hook2 = mymodel.linear2.register_backward_hook(backwardcall)\n",
        "\n",
        "  \n",
        "if torch.cuda.is_available():\n",
        "  mymodel.cuda()\n",
        "\n",
        "optimizer = torch.optim.SGD(mymodel.parameters(), lr=0.0009, momentum=0.9)\n",
        "\n",
        "loss_col = []\n",
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "  optimizer.zero_grad()\n",
        "  model_output = mymodel(x)\n",
        "  loss = torch.nn.functional.mse_loss(model_output, y) \n",
        "  loss_col.append(loss)\n",
        "  #print(epoch, \"{:.20f}\".format(loss.item()))\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  \n",
        "hook1.remove()  # removes the hook\n",
        "hook2.remove()  # removes the hook"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "net(\n",
            "  (linear1): Linear(in_features=2, out_features=1, bias=True)\n",
            "  (linear2): Linear(in_features=1, out_features=2, bias=True)\n",
            ")\n",
            "\n",
            "Inside forward of  Linear(in_features=2, out_features=1, bias=True)  :  (tensor([[-0.7917,  1.6142]], device='cuda:0'),)  :  tensor([[-0.0311]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "Inside backward of  Linear(in_features=1, out_features=2, bias=True)  :  (tensor([-1.5735, -1.8884], device='cuda:0'), tensor([[-0.2341]], device='cuda:0'), tensor([[-0., -0.]], device='cuda:0'))  :  (tensor([[-1.5735, -1.8884]], device='cuda:0'),)\n",
            "\n",
            "Inside forward of  Linear(in_features=2, out_features=1, bias=True)  :  (tensor([[-0.7917,  1.6142]], device='cuda:0'),)  :  tensor([[-0.0311]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "Inside backward of  Linear(in_features=1, out_features=2, bias=True)  :  (tensor([-1.5721, -1.8867], device='cuda:0'), tensor([[-0.2339]], device='cuda:0'), tensor([[-0., -0.]], device='cuda:0'))  :  (tensor([[-1.5721, -1.8867]], device='cuda:0'),)\n",
            "\n",
            "Inside forward of  Linear(in_features=2, out_features=1, bias=True)  :  (tensor([[-0.7917,  1.6142]], device='cuda:0'),)  :  tensor([[-0.0311]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "Inside backward of  Linear(in_features=1, out_features=2, bias=True)  :  (tensor([-1.5694, -1.8835], device='cuda:0'), tensor([[-0.2335]], device='cuda:0'), tensor([[-0., -0.]], device='cuda:0'))  :  (tensor([[-1.5694, -1.8835]], device='cuda:0'),)\n",
            "\n",
            "Inside forward of  Linear(in_features=2, out_features=1, bias=True)  :  (tensor([[-0.7917,  1.6142]], device='cuda:0'),)  :  tensor([[-0.0311]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "Inside backward of  Linear(in_features=1, out_features=2, bias=True)  :  (tensor([-1.5656, -1.8789], device='cuda:0'), tensor([[-0.2329]], device='cuda:0'), tensor([[-0., -0.]], device='cuda:0'))  :  (tensor([[-1.5656, -1.8789]], device='cuda:0'),)\n",
            "\n",
            "Inside forward of  Linear(in_features=2, out_features=1, bias=True)  :  (tensor([[-0.7917,  1.6142]], device='cuda:0'),)  :  tensor([[-0.0311]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
            "Inside backward of  Linear(in_features=1, out_features=2, bias=True)  :  (tensor([-1.5607, -1.8731], device='cuda:0'), tensor([[-0.2322]], device='cuda:0'), tensor([[-0., -0.]], device='cuda:0'))  :  (tensor([[-1.5607, -1.8731]], device='cuda:0'),)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JqXdm94UHVC7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The **current implementation will not have above behavior for complex Module** that perform many operations. \n",
        "\n",
        "In some failure cases, grad_input and grad_output will only contain the gradients for a subset of the inputs and outputs. \n",
        "\n",
        "For such Module, you should use torch.Tensor.register_hook() directly on a specific input or output to get the required gradients."
      ]
    },
    {
      "metadata": {
        "id": "XGSScsIlKCO3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Training/Evaluation Modes\n",
        "\n",
        "This is to change the mode of the model, to put it in training mode while training and to put it in evaluation mode while testing. This affects only those layers that behave differently during training and testing such as Dropout, BatchNorm etc\n",
        "\n",
        "eval(): Sets the module in evaluation mode. This has any effect only on certain modules. See particular modules for details of their behaviors in training/evaluation mode, if they are affected, e.g. Dropout, BatchNorm, etc.\n",
        "\n",
        "train(mode=True): Sets the module in training mode. This has any effect only on certain modules\n",
        "\n",
        "For example **loss is not required during evaluation** as the model is already built with weights. We only need to infer it with new data."
      ]
    },
    {
      "metadata": {
        "id": "kS3hvUiJJq74",
        "colab_type": "code",
        "outputId": "67a71457-02fb-4e40-c033-d7f899c18560",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# You should be able to check the training state of the model:\n",
        "\n",
        "if mymodel.training == True:\n",
        "    print(\"Model is in training mode\")\n",
        "if mymodel.training == False:\n",
        "    print(\"Model is in Evaluation mode\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model is in training mode\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DGcIazMrKR_v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Moves and/or cast the parameters and buffers"
      ]
    },
    {
      "metadata": {
        "id": "-6gmVhfhKahH",
        "colab_type": "code",
        "outputId": "be6b5a29-6cd1-4f65-f17b-8f7e546c67c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        }
      },
      "cell_type": "code",
      "source": [
        "linear = nn.Linear(2, 2)\n",
        "print(linear.weight)\n",
        "\n",
        "linear.to(torch.double)\n",
        "print(linear.weight)\n",
        "\n",
        "gpu1 = torch.device(\"cuda:0\")\n",
        "print(linear.to(gpu1, dtype=torch.half, non_blocking=True))\n",
        "\n",
        "print(linear.weight)\n",
        "cpu = torch.device(\"cpu\")\n",
        "\n",
        "linear.to(cpu)\n",
        "print(linear.weight)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[-0.3195,  0.6755],\n",
            "        [ 0.2388,  0.2219]], requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[-0.3195,  0.6755],\n",
            "        [ 0.2388,  0.2219]], dtype=torch.float64, requires_grad=True)\n",
            "Linear(in_features=2, out_features=2, bias=True)\n",
            "Parameter containing:\n",
            "tensor([[-0.3196,  0.6758],\n",
            "        [ 0.2388,  0.2218]], device='cuda:0', dtype=torch.float16,\n",
            "       requires_grad=True)\n",
            "Parameter containing:\n",
            "tensor([[-0.3196,  0.6758],\n",
            "        [ 0.2388,  0.2218]], dtype=torch.float16, requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pld1XewLioum",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Scheduler\n",
        "\n",
        "Decays the learning rate of each parameter group by gamma every step_size epochs. Notice that such decay can happen simultaneously with  other changes to the learning rate from outside this scheduler.\n",
        "\n",
        "**scheduler.step() only changes learning rate, but does not perform optimizer step. You also have to call optimizer.step(), regardless of the fact that you use scheduler or not.**\n",
        "\n",
        "note that optim.param_groups is a list of the different weight groups which can have different learning rates and it is accessible as:\n",
        "\n",
        "    for g in optim.param_groups:\n",
        "        g['lr'] = 0.001"
      ]
    },
    {
      "metadata": {
        "id": "EWD7dTnWiphi",
        "colab_type": "code",
        "outputId": "8171af27-d95c-49b3-810d-923ed26ade44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2845
        }
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = torch.randn(1, 2).cuda()\n",
        "y = torch.randn(1, 1).cuda()\n",
        "\n",
        "class net(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(net,self).__init__()\n",
        "    self.linear1=torch.nn.Linear(2,1)\n",
        "    self.linear2=torch.nn.Linear(1,2)\n",
        "    \n",
        "  def forward(self,x):\n",
        "    h_relu = torch.nn.functional.relu(self.linear1(x))\n",
        "    y_pred = self.linear2(h_relu)\n",
        "    return y_pred\n",
        "\n",
        "mymodel = net()\n",
        "print(mymodel)\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  mymodel.cuda()\n",
        "\n",
        "#optimizer = torch.optim.SGD(mymodel.parameters(), lr=0.0009, momentum=0.9)\n",
        "optimizer = torch.optim.SGD(mymodel.parameters(), lr=0.01)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.99)\n",
        "\n",
        "# Train flag can be updated with boolean\n",
        "# to disable dropout and batch norm learning\n",
        "mymodel.train(True)\n",
        "# execute train step\n",
        "\n",
        "loss_col = []\n",
        "epochs = 50\n",
        "for epoch in range(epochs):\n",
        "  optimizer.zero_grad()\n",
        "  model_output = mymodel(x)\n",
        "  loss = torch.nn.functional.mse_loss(model_output, y) \n",
        "  loss_col.append(loss)\n",
        "  print(\"\\nIteration:\",epoch,\"Loss:\",\"{:.20f}\".format(loss.item()))\n",
        "  print(\"lr: \", \"{:.20f}\".format(optimizer.param_groups[0]['lr']))\n",
        "  loss.backward()\n",
        "  scheduler.step()\n",
        "  optimizer.step()\n",
        "  \n",
        "\n",
        "mymodel.train(False)\n",
        "# run inference step\n",
        "\n",
        "# CPU seed\n",
        "torch.manual_seed(42)\n",
        "# GPU seed\n",
        "torch.cuda.manual_seed_all(42)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "net(\n",
            "  (linear1): Linear(in_features=2, out_features=1, bias=True)\n",
            "  (linear2): Linear(in_features=1, out_features=2, bias=True)\n",
            ")\n",
            "\n",
            "Iteration: 0 Loss: 0.92028093338012695312\n",
            "lr:  0.01000000000000000021\n",
            "\n",
            "Iteration: 1 Loss: 0.88778233528137207031\n",
            "lr:  0.01000000000000000021\n",
            "\n",
            "Iteration: 2 Loss: 0.85650908946990966797\n",
            "lr:  0.00990000000000000081\n",
            "\n",
            "Iteration: 3 Loss: 0.82641303539276123047\n",
            "lr:  0.00980100000000000068\n",
            "\n",
            "Iteration: 4 Loss: 0.79744863510131835938\n",
            "lr:  0.00970298999999999988\n",
            "\n",
            "Iteration: 5 Loss: 0.76957285404205322266\n",
            "lr:  0.00960596009999999979\n",
            "\n",
            "Iteration: 6 Loss: 0.74274420738220214844\n",
            "lr:  0.00950990049899999926\n",
            "\n",
            "Iteration: 7 Loss: 0.71692287921905517578\n",
            "lr:  0.00941480149400999999\n",
            "\n",
            "Iteration: 8 Loss: 0.69207096099853515625\n",
            "lr:  0.00932065347906989999\n",
            "\n",
            "Iteration: 9 Loss: 0.66815221309661865234\n",
            "lr:  0.00922744694427920023\n",
            "\n",
            "Iteration: 10 Loss: 0.64513164758682250977\n",
            "lr:  0.00913517247483640847\n",
            "\n",
            "Iteration: 11 Loss: 0.62297552824020385742\n",
            "lr:  0.00904382075008804479\n",
            "\n",
            "Iteration: 12 Loss: 0.60165178775787353516\n",
            "lr:  0.00895338254258716375\n",
            "\n",
            "Iteration: 13 Loss: 0.58112925291061401367\n",
            "lr:  0.00886384871716129272\n",
            "\n",
            "Iteration: 14 Loss: 0.56137794256210327148\n",
            "lr:  0.00877521022998967865\n",
            "\n",
            "Iteration: 15 Loss: 0.54236906766891479492\n",
            "lr:  0.00868745812768978269\n",
            "\n",
            "Iteration: 16 Loss: 0.52407491207122802734\n",
            "lr:  0.00860058354641288388\n",
            "\n",
            "Iteration: 17 Loss: 0.50646871328353881836\n",
            "lr:  0.00851457771094875533\n",
            "\n",
            "Iteration: 18 Loss: 0.48952445387840270996\n",
            "lr:  0.00842943193383926745\n",
            "\n",
            "Iteration: 19 Loss: 0.47321736812591552734\n",
            "lr:  0.00834513761450087453\n",
            "\n",
            "Iteration: 20 Loss: 0.45752346515655517578\n",
            "lr:  0.00826168623835586702\n",
            "\n",
            "Iteration: 21 Loss: 0.44241937994956970215\n",
            "lr:  0.00817906937597230772\n",
            "\n",
            "Iteration: 22 Loss: 0.42788296937942504883\n",
            "lr:  0.00809727868221258446\n",
            "\n",
            "Iteration: 23 Loss: 0.41389256715774536133\n",
            "lr:  0.00801630589539045930\n",
            "\n",
            "Iteration: 24 Loss: 0.40042743086814880371\n",
            "lr:  0.00793614283643655390\n",
            "\n",
            "Iteration: 25 Loss: 0.38746741414070129395\n",
            "lr:  0.00785678140807218733\n",
            "\n",
            "Iteration: 26 Loss: 0.37499326467514038086\n",
            "lr:  0.00777821359399146709\n",
            "\n",
            "Iteration: 27 Loss: 0.36298626661300659180\n",
            "lr:  0.00770043145805155132\n",
            "\n",
            "Iteration: 28 Loss: 0.35142856836318969727\n",
            "lr:  0.00762342714347103573\n",
            "\n",
            "Iteration: 29 Loss: 0.34030264616012573242\n",
            "lr:  0.00754719287203632627\n",
            "\n",
            "Iteration: 30 Loss: 0.32959192991256713867\n",
            "lr:  0.00747172094331596121\n",
            "\n",
            "Iteration: 31 Loss: 0.31928032636642456055\n",
            "lr:  0.00739700373388280184\n",
            "\n",
            "Iteration: 32 Loss: 0.30935245752334594727\n",
            "lr:  0.00732303369654397490\n",
            "\n",
            "Iteration: 33 Loss: 0.29979318380355834961\n",
            "lr:  0.00724980335957853359\n",
            "\n",
            "Iteration: 34 Loss: 0.29058831930160522461\n",
            "lr:  0.00717730532598274962\n",
            "\n",
            "Iteration: 35 Loss: 0.28172391653060913086\n",
            "lr:  0.00710553227272292098\n",
            "\n",
            "Iteration: 36 Loss: 0.27318686246871948242\n",
            "lr:  0.00703447694999569193\n",
            "\n",
            "Iteration: 37 Loss: 0.26496419310569763184\n",
            "lr:  0.00696413218049573429\n",
            "\n",
            "Iteration: 38 Loss: 0.25704365968704223633\n",
            "lr:  0.00689449085869077698\n",
            "\n",
            "Iteration: 39 Loss: 0.24941343069076538086\n",
            "lr:  0.00682554595010386941\n",
            "\n",
            "Iteration: 40 Loss: 0.24206207692623138428\n",
            "lr:  0.00675729049060283093\n",
            "\n",
            "Iteration: 41 Loss: 0.23497870564460754395\n",
            "lr:  0.00668971758569680310\n",
            "\n",
            "Iteration: 42 Loss: 0.22815278172492980957\n",
            "lr:  0.00662282040983983456\n",
            "\n",
            "Iteration: 43 Loss: 0.22157412767410278320\n",
            "lr:  0.00655659220574143602\n",
            "\n",
            "Iteration: 44 Loss: 0.21523323655128479004\n",
            "lr:  0.00649102628368402202\n",
            "\n",
            "Iteration: 45 Loss: 0.20912054181098937988\n",
            "lr:  0.00642611602084718157\n",
            "\n",
            "Iteration: 46 Loss: 0.20322723686695098877\n",
            "lr:  0.00636185486063870972\n",
            "\n",
            "Iteration: 47 Loss: 0.19754466414451599121\n",
            "lr:  0.00629823631203232231\n",
            "\n",
            "Iteration: 48 Loss: 0.19206455349922180176\n",
            "lr:  0.00623525394891199982\n",
            "\n",
            "Iteration: 49 Loss: 0.18677902221679687500\n",
            "lr:  0.00617290140942287988\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9Ll_OqRjkdsk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Debugging\n",
        "\n",
        "With its clean and minimal design, PyTorch makes debugging a breeze. We can place breakpoints using pdb.set_trace() at any line in our code. We can then execute further computations, examine the PyTorch Tensors/Variables and pinpoint the root cause of the error."
      ]
    },
    {
      "metadata": {
        "id": "5fG49VIUdJMw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Inplace Operations\n",
        "\n",
        "torch.nn.functional.relu(input, inplace=False) takes a tensor of any size as input, applies ReLU on each value to produce a result tensor of same size.\n",
        "\n",
        "inplace indicates if the operation should modify the argument itself.  This may be desirable to reduce the memory footprint of the processing. \n"
      ]
    },
    {
      "metadata": {
        "id": "E-4a8TqxMd-y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Wrap-up\n",
        "                          \n",
        "1. Inherit a class from torch.nn.module - initialize: define modules, forward: build network\n",
        "2. Create an object (instance) - Construct graphs when class in initialized and forward is called\n",
        "3. Define optimizer and loss function\n",
        "4. Run loop - reset gradient, backward, update step \n",
        "\n",
        "\n",
        "\n",
        "**Multi-processing:**\n",
        "\n",
        "torch.nn.dataparallel\n",
        "\n",
        "torch.multiprocessing\n",
        "\n",
        "hogwild(async)\n"
      ]
    },
    {
      "metadata": {
        "id": "Ovi0VBoSMBMO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Homework\n",
        "\n",
        "Solve the previous celsius to fahrenheit problem using pytorch classes and functions. Hook to the backpropogation for each iteration and plot the change of values. \n",
        "\n",
        "Finally, Save the model, load the model and run in evaluation mode and evalute the performance of the model for a given sample.\n",
        "\n",
        "Check if a gpu is present and use gpu for accelerating the entire model and training."
      ]
    },
    {
      "metadata": {
        "id": "6Qb6EHsqEsBB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "## ANNs for Images"
      ]
    },
    {
      "metadata": {
        "id": "piwHTCcyEy6K",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Image Theory\n",
        "\n",
        "Pixels are the raw building blocks of an image. Every image consists of a set of pixels. There is no finer granularity than the pixel. Normally, a pixel is considered the “color” or the “intensity” of light that appears in a given place in our image. \n",
        "\n",
        "Most pixels are represented in two ways: \n",
        "\n",
        "1. Grayscale/single channel \n",
        "2. Color - 3 Channel (RGB)\n",
        "\n",
        "![alt text](https://upload.wikimedia.org/wikipedia/commons/3/33/Beyoglu_4671_tricolor.png)\n",
        "\n",
        "In a **grayscale image**, each pixel is a scalar value between 0 and 255, where **zero** corresponds to **“black”** and **255** being **“white”**. Values between 0 and 255 are varying shades of gray, where values closer to 0 are darker and values closer to 255 are lighter.\n",
        "\n",
        "* Grayscale = Image Batch x Channel x Height x Weight  = Image Batch x 1 x Height x Weight = Image Batch x H x W\n",
        "\n",
        "Pixels in the RGB color space are represented by a list of three values: one value for the Red component, one for Green, and another for Blue.\n",
        "\n",
        "* Color = Image Batch x C x H x W\n",
        "\n",
        "\n",
        "\n",
        "**RGB and BGR Ordering** - It is important to note that some softwares for example OpenCV stores RGB channels in reverse order. While we normally think in terms of Red, Green, and Blue, OpenCV actually stores the pixel values in Blue, Green, Red order (BGR). \n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "IYEh2N12oksb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Data Preparation and Preprocessing\n",
        "\n",
        "Data Loading\n",
        "\n",
        "For convenience, PyTorch provides a number of utilities to load, preprocess and interact with datasets. These helper classes and functions are found in the torch.utils.data module. \n",
        "\n",
        "PyTorch offers the torch.utils.data.DataLoader object which combines a data-set and a sampling policy to create an iterator over mini-batches. \n",
        "\n",
        "Standard data-sets are available in torchvision.datasets, and they allow to apply transformations over the images or the labels transparently\n",
        "\n",
        "The two major concepts here are:\n",
        "\n",
        "    A Dataset, which encapsulates a source of data,\n",
        "    A DataLoader, which is responsible for loading a dataset, possibly in parallel.\n",
        "\n",
        "New **datasets** are created by \n",
        "* **subclassing** the torch.utils.data.Dataset class and \n",
        "* initializating the __init__ function to actually load the data from media or database or url or using generators\n",
        "* the __getitem__ method to access a **single value** at a **certain index** and\n",
        "* overriding the __len__ method to return the number of samples in the dataset "
      ]
    },
    {
      "metadata": {
        "id": "ZWeKJHWwpoCI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "9bdb629e-799e-40c8-c75f-8bf26de91be9"
      },
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class c2fdata(Dataset):\n",
        "  def __init__(self):\n",
        "    self.celsius = torch.tensor([(float)(c) for c in range(-273,1000)])\n",
        "    self.fahrenheit = torch.tensor([c*1.8+32.0 for c in self.celsius])\n",
        "  \n",
        "  def __getitem__(self,index):\n",
        "    return self.celsius[index],self.fahrenheit[index]\n",
        "  \n",
        "  def __len__(self):\n",
        "    return self.celsius.shape[0]\n",
        "  \n",
        "mydataset = c2fdata()\n",
        "\n",
        "print(mydataset[0])\n",
        "print(len(mydataset))\n",
        "    "
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(tensor(-273.), tensor(-459.4000))\n",
            "1273\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pSW1plli7vuT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Inside __init__ we would usually configure some paths or change the set of samples ultimately returned. In __len__, we specify the upper bound for the index with which __getitem__ may be called, and in __getitem__ we return the actual sample, which could be an image or an audio snippet.\n",
        "\n",
        "To iterate over the dataset we could, in theory, simply have a for i in range loop and access samples via __getitem__. "
      ]
    },
    {
      "metadata": {
        "id": "6xfq8_kxOsaA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "5db0cdd9-1e87-488c-d072-42c0ab7ab1e4"
      },
      "cell_type": "code",
      "source": [
        "for i in range(len(mydataset)):\n",
        "  print(mydataset[i])\n",
        "  if i == 3:\n",
        "        break"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(tensor(-273.), tensor(-459.4000))\n",
            "(tensor(-272.), tensor(-457.6000))\n",
            "(tensor(-271.), tensor(-455.8000))\n",
            "(tensor(-270.), tensor(-454.))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PTtOyipcPMH-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "However, it would be much more convenient if the dataset implemented the iterator protocol itself, so we could simply loop over samples with for sample in dataset. \n",
        "\n",
        "We are losing a lot of features by using a simple for loop to iterate over the data. In particular, we are missing out on:\n",
        "\n",
        "    Batching the data\n",
        "    Shuffling the data\n",
        "    Load the data in parallel using multiprocessing workers.\n",
        "\n",
        "Fortunately, this functionality is provided by the **DataLoader class** available via the **torch.utils.data.DataLoader** which is an iterator which provides all these features.\n",
        "\n",
        "A DataLoader object takes a **dataset object as argument** and a number of options that configure the way samples are retrieved. For example, it is possible to load samples in **parallel**, using multiple processes. For this, the DataLoader constructor takes a **num_workers** argument. \n",
        "\n",
        "Note that DataLoaders always return batches, whose size is set with the **batch_size** parameter. \n",
        "\n",
        "List of important arguments for **dataloader**:\n",
        "\n",
        "$$\\begin{array}{|c|c|}\n",
        "\\hline\n",
        " Argument & Description \\\\\\hline\n",
        "dataset \\;(Dataset) & dataset\\; object \\;from\\; which\\; to\\; load\\; the\\; data\\\\\\hline\n",
        "batch\\_size (int, optional) & how \\;many \\;samples \\;per \\;batch \\;to \\;load (default: 1)\\\\\\hline\n",
        "shuffle (bool, optional) & set \\;to\\; True\\; to\\; have\\; the\\; data\\; reshuffled\\; at\\; every\\; epoch\\; (default: False).\\\\\\hline\n",
        "num_workers (int, optional) &  how\\; many\\; subprocesses\\; to\\; use\\; for\\; data\\; loading.\\; 0\\; means\\; that\\; the\\; data\\; will\\; be\\; loaded\\; in\\; the\\; main\\; process (default: 0)\\\\\\hline\n",
        "collate\\_fn (callable, optional) & specify\\; how\\; exactly\\; the\\; samples\\; need\\; to\\; be\\; batched\\; to\\; form\\; a\\; mini-batch\\;\\\\\\hline\n",
        "\\end{array}$$  \n"
      ]
    },
    {
      "metadata": {
        "id": "-W9QGpebP-Xx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3929
        },
        "outputId": "45c4fbf9-99ff-46d3-eb1c-236a43c93c4e"
      },
      "cell_type": "code",
      "source": [
        "mydataloader = torch.utils.data.DataLoader(mydataset,batch_size=6,num_workers=2)\n",
        "for i, batch in enumerate(mydataloader):\n",
        "  print(i, batch)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 [tensor([-273., -272., -271., -270., -269., -268.]), tensor([-459.4000, -457.6000, -455.8000, -454.0000, -452.2000, -450.4000])]\n",
            "1 [tensor([-267., -266., -265., -264., -263., -262.]), tensor([-448.6000, -446.8000, -445.0000, -443.2000, -441.4000, -439.6000])]\n",
            "2 [tensor([-261., -260., -259., -258., -257., -256.]), tensor([-437.8000, -436.0000, -434.2000, -432.4000, -430.6000, -428.8000])]\n",
            "3 [tensor([-255., -254., -253., -252., -251., -250.]), tensor([-427.0000, -425.2000, -423.4000, -421.6000, -419.8000, -418.0000])]\n",
            "4 [tensor([-249., -248., -247., -246., -245., -244.]), tensor([-416.2000, -414.4000, -412.6000, -410.8000, -409.0000, -407.2000])]\n",
            "5 [tensor([-243., -242., -241., -240., -239., -238.]), tensor([-405.4000, -403.6000, -401.8000, -400.0000, -398.2000, -396.4000])]\n",
            "6 [tensor([-237., -236., -235., -234., -233., -232.]), tensor([-394.6000, -392.8000, -391.0000, -389.2000, -387.4000, -385.6000])]\n",
            "7 [tensor([-231., -230., -229., -228., -227., -226.]), tensor([-383.8000, -382.0000, -380.2000, -378.4000, -376.6000, -374.8000])]\n",
            "8 [tensor([-225., -224., -223., -222., -221., -220.]), tensor([-373.0000, -371.2000, -369.4000, -367.6000, -365.8000, -364.0000])]\n",
            "9 [tensor([-219., -218., -217., -216., -215., -214.]), tensor([-362.2000, -360.4000, -358.6000, -356.8000, -355.0000, -353.2000])]\n",
            "10 [tensor([-213., -212., -211., -210., -209., -208.]), tensor([-351.4000, -349.6000, -347.8000, -346.0000, -344.2000, -342.4000])]\n",
            "11 [tensor([-207., -206., -205., -204., -203., -202.]), tensor([-340.6000, -338.8000, -337.0000, -335.2000, -333.4000, -331.6000])]\n",
            "12 [tensor([-201., -200., -199., -198., -197., -196.]), tensor([-329.8000, -328.0000, -326.2000, -324.4000, -322.6000, -320.8000])]\n",
            "13 [tensor([-195., -194., -193., -192., -191., -190.]), tensor([-319.0000, -317.2000, -315.4000, -313.6000, -311.8000, -310.0000])]\n",
            "14 [tensor([-189., -188., -187., -186., -185., -184.]), tensor([-308.2000, -306.4000, -304.6000, -302.8000, -301.0000, -299.2000])]\n",
            "15 [tensor([-183., -182., -181., -180., -179., -178.]), tensor([-297.4000, -295.6000, -293.8000, -292.0000, -290.2000, -288.4000])]\n",
            "16 [tensor([-177., -176., -175., -174., -173., -172.]), tensor([-286.6000, -284.8000, -283.0000, -281.2000, -279.4000, -277.6000])]\n",
            "17 [tensor([-171., -170., -169., -168., -167., -166.]), tensor([-275.8000, -274.0000, -272.2000, -270.4000, -268.6000, -266.8000])]\n",
            "18 [tensor([-165., -164., -163., -162., -161., -160.]), tensor([-265.0000, -263.2000, -261.4000, -259.6000, -257.8000, -256.0000])]\n",
            "19 [tensor([-159., -158., -157., -156., -155., -154.]), tensor([-254.2000, -252.4000, -250.6000, -248.8000, -247.0000, -245.2000])]\n",
            "20 [tensor([-153., -152., -151., -150., -149., -148.]), tensor([-243.4000, -241.6000, -239.8000, -238.0000, -236.2000, -234.4000])]\n",
            "21 [tensor([-147., -146., -145., -144., -143., -142.]), tensor([-232.6000, -230.8000, -229.0000, -227.2000, -225.4000, -223.6000])]\n",
            "22 [tensor([-141., -140., -139., -138., -137., -136.]), tensor([-221.8000, -220.0000, -218.2000, -216.4000, -214.6000, -212.8000])]\n",
            "23 [tensor([-135., -134., -133., -132., -131., -130.]), tensor([-211.0000, -209.2000, -207.4000, -205.6000, -203.8000, -202.0000])]\n",
            "24 [tensor([-129., -128., -127., -126., -125., -124.]), tensor([-200.2000, -198.4000, -196.6000, -194.8000, -193.0000, -191.2000])]\n",
            "25 [tensor([-123., -122., -121., -120., -119., -118.]), tensor([-189.4000, -187.6000, -185.8000, -184.0000, -182.2000, -180.4000])]\n",
            "26 [tensor([-117., -116., -115., -114., -113., -112.]), tensor([-178.6000, -176.8000, -175.0000, -173.2000, -171.4000, -169.6000])]\n",
            "27 [tensor([-111., -110., -109., -108., -107., -106.]), tensor([-167.8000, -166.0000, -164.2000, -162.4000, -160.6000, -158.8000])]\n",
            "28 [tensor([-105., -104., -103., -102., -101., -100.]), tensor([-157.0000, -155.2000, -153.4000, -151.6000, -149.8000, -148.0000])]\n",
            "29 [tensor([-99., -98., -97., -96., -95., -94.]), tensor([-146.2000, -144.4000, -142.6000, -140.8000, -139.0000, -137.2000])]\n",
            "30 [tensor([-93., -92., -91., -90., -89., -88.]), tensor([-135.4000, -133.6000, -131.8000, -130.0000, -128.2000, -126.4000])]\n",
            "31 [tensor([-87., -86., -85., -84., -83., -82.]), tensor([-124.6000, -122.8000, -121.0000, -119.2000, -117.4000, -115.6000])]\n",
            "32 [tensor([-81., -80., -79., -78., -77., -76.]), tensor([-113.8000, -112.0000, -110.2000, -108.4000, -106.6000, -104.8000])]\n",
            "33 [tensor([-75., -74., -73., -72., -71., -70.]), tensor([-103.0000, -101.2000,  -99.4000,  -97.6000,  -95.8000,  -94.0000])]\n",
            "34 [tensor([-69., -68., -67., -66., -65., -64.]), tensor([-92.2000, -90.4000, -88.6000, -86.8000, -85.0000, -83.2000])]\n",
            "35 [tensor([-63., -62., -61., -60., -59., -58.]), tensor([-81.4000, -79.6000, -77.8000, -76.0000, -74.2000, -72.4000])]\n",
            "36 [tensor([-57., -56., -55., -54., -53., -52.]), tensor([-70.6000, -68.8000, -67.0000, -65.2000, -63.4000, -61.6000])]\n",
            "37 [tensor([-51., -50., -49., -48., -47., -46.]), tensor([-59.8000, -58.0000, -56.2000, -54.4000, -52.6000, -50.8000])]\n",
            "38 [tensor([-45., -44., -43., -42., -41., -40.]), tensor([-49.0000, -47.2000, -45.4000, -43.6000, -41.8000, -40.0000])]\n",
            "39 [tensor([-39., -38., -37., -36., -35., -34.]), tensor([-38.2000, -36.4000, -34.6000, -32.8000, -31.0000, -29.2000])]\n",
            "40 [tensor([-33., -32., -31., -30., -29., -28.]), tensor([-27.4000, -25.6000, -23.8000, -22.0000, -20.2000, -18.4000])]\n",
            "41 [tensor([-27., -26., -25., -24., -23., -22.]), tensor([-16.6000, -14.8000, -13.0000, -11.2000,  -9.4000,  -7.6000])]\n",
            "42 [tensor([-21., -20., -19., -18., -17., -16.]), tensor([-5.8000, -4.0000, -2.2000, -0.4000,  1.4000,  3.2000])]\n",
            "43 [tensor([-15., -14., -13., -12., -11., -10.]), tensor([ 5.0000,  6.8000,  8.6000, 10.4000, 12.2000, 14.0000])]\n",
            "44 [tensor([-9., -8., -7., -6., -5., -4.]), tensor([15.8000, 17.6000, 19.4000, 21.2000, 23.0000, 24.8000])]\n",
            "45 [tensor([-3., -2., -1.,  0.,  1.,  2.]), tensor([26.6000, 28.4000, 30.2000, 32.0000, 33.8000, 35.6000])]\n",
            "46 [tensor([3., 4., 5., 6., 7., 8.]), tensor([37.4000, 39.2000, 41.0000, 42.8000, 44.6000, 46.4000])]\n",
            "47 [tensor([ 9., 10., 11., 12., 13., 14.]), tensor([48.2000, 50.0000, 51.8000, 53.6000, 55.4000, 57.2000])]\n",
            "48 [tensor([15., 16., 17., 18., 19., 20.]), tensor([59.0000, 60.8000, 62.6000, 64.4000, 66.2000, 68.0000])]\n",
            "49 [tensor([21., 22., 23., 24., 25., 26.]), tensor([69.8000, 71.6000, 73.4000, 75.2000, 77.0000, 78.8000])]\n",
            "50 [tensor([27., 28., 29., 30., 31., 32.]), tensor([80.6000, 82.4000, 84.2000, 86.0000, 87.8000, 89.6000])]\n",
            "51 [tensor([33., 34., 35., 36., 37., 38.]), tensor([ 91.4000,  93.2000,  95.0000,  96.8000,  98.6000, 100.4000])]\n",
            "52 [tensor([39., 40., 41., 42., 43., 44.]), tensor([102.2000, 104.0000, 105.8000, 107.6000, 109.4000, 111.2000])]\n",
            "53 [tensor([45., 46., 47., 48., 49., 50.]), tensor([113.0000, 114.8000, 116.6000, 118.4000, 120.2000, 122.0000])]\n",
            "54 [tensor([51., 52., 53., 54., 55., 56.]), tensor([123.8000, 125.6000, 127.4000, 129.2000, 131.0000, 132.8000])]\n",
            "55 [tensor([57., 58., 59., 60., 61., 62.]), tensor([134.6000, 136.4000, 138.2000, 140.0000, 141.8000, 143.6000])]\n",
            "56 [tensor([63., 64., 65., 66., 67., 68.]), tensor([145.4000, 147.2000, 149.0000, 150.8000, 152.6000, 154.4000])]\n",
            "57 [tensor([69., 70., 71., 72., 73., 74.]), tensor([156.2000, 158.0000, 159.8000, 161.6000, 163.4000, 165.2000])]\n",
            "58 [tensor([75., 76., 77., 78., 79., 80.]), tensor([167.0000, 168.8000, 170.6000, 172.4000, 174.2000, 176.0000])]\n",
            "59 [tensor([81., 82., 83., 84., 85., 86.]), tensor([177.8000, 179.6000, 181.4000, 183.2000, 185.0000, 186.8000])]\n",
            "60 [tensor([87., 88., 89., 90., 91., 92.]), tensor([188.6000, 190.4000, 192.2000, 194.0000, 195.8000, 197.6000])]\n",
            "61 [tensor([93., 94., 95., 96., 97., 98.]), tensor([199.4000, 201.2000, 203.0000, 204.8000, 206.6000, 208.4000])]\n",
            "62 [tensor([ 99., 100., 101., 102., 103., 104.]), tensor([210.2000, 212.0000, 213.8000, 215.6000, 217.4000, 219.2000])]\n",
            "63 [tensor([105., 106., 107., 108., 109., 110.]), tensor([221.0000, 222.8000, 224.6000, 226.4000, 228.2000, 230.0000])]\n",
            "64 [tensor([111., 112., 113., 114., 115., 116.]), tensor([231.8000, 233.6000, 235.4000, 237.2000, 239.0000, 240.8000])]\n",
            "65 [tensor([117., 118., 119., 120., 121., 122.]), tensor([242.6000, 244.4000, 246.2000, 248.0000, 249.8000, 251.6000])]\n",
            "66 [tensor([123., 124., 125., 126., 127., 128.]), tensor([253.4000, 255.2000, 257.0000, 258.8000, 260.6000, 262.4000])]\n",
            "67 [tensor([129., 130., 131., 132., 133., 134.]), tensor([264.2000, 266.0000, 267.8000, 269.6000, 271.4000, 273.2000])]\n",
            "68 [tensor([135., 136., 137., 138., 139., 140.]), tensor([275.0000, 276.8000, 278.6000, 280.4000, 282.2000, 284.0000])]\n",
            "69 [tensor([141., 142., 143., 144., 145., 146.]), tensor([285.8000, 287.6000, 289.4000, 291.2000, 293.0000, 294.8000])]\n",
            "70 [tensor([147., 148., 149., 150., 151., 152.]), tensor([296.6000, 298.4000, 300.2000, 302.0000, 303.8000, 305.6000])]\n",
            "71 [tensor([153., 154., 155., 156., 157., 158.]), tensor([307.4000, 309.2000, 311.0000, 312.8000, 314.6000, 316.4000])]\n",
            "72 [tensor([159., 160., 161., 162., 163., 164.]), tensor([318.2000, 320.0000, 321.8000, 323.6000, 325.4000, 327.2000])]\n",
            "73 [tensor([165., 166., 167., 168., 169., 170.]), tensor([329.0000, 330.8000, 332.6000, 334.4000, 336.2000, 338.0000])]\n",
            "74 [tensor([171., 172., 173., 174., 175., 176.]), tensor([339.8000, 341.6000, 343.4000, 345.2000, 347.0000, 348.8000])]\n",
            "75 [tensor([177., 178., 179., 180., 181., 182.]), tensor([350.6000, 352.4000, 354.2000, 356.0000, 357.8000, 359.6000])]\n",
            "76 [tensor([183., 184., 185., 186., 187., 188.]), tensor([361.4000, 363.2000, 365.0000, 366.8000, 368.6000, 370.4000])]\n",
            "77 [tensor([189., 190., 191., 192., 193., 194.]), tensor([372.2000, 374.0000, 375.8000, 377.6000, 379.4000, 381.2000])]\n",
            "78 [tensor([195., 196., 197., 198., 199., 200.]), tensor([383.0000, 384.8000, 386.6000, 388.4000, 390.2000, 392.0000])]\n",
            "79 [tensor([201., 202., 203., 204., 205., 206.]), tensor([393.8000, 395.6000, 397.4000, 399.2000, 401.0000, 402.8000])]\n",
            "80 [tensor([207., 208., 209., 210., 211., 212.]), tensor([404.6000, 406.4000, 408.2000, 410.0000, 411.8000, 413.6000])]\n",
            "81 [tensor([213., 214., 215., 216., 217., 218.]), tensor([415.4000, 417.2000, 419.0000, 420.8000, 422.6000, 424.4000])]\n",
            "82 [tensor([219., 220., 221., 222., 223., 224.]), tensor([426.2000, 428.0000, 429.8000, 431.6000, 433.4000, 435.2000])]\n",
            "83 [tensor([225., 226., 227., 228., 229., 230.]), tensor([437.0000, 438.8000, 440.6000, 442.4000, 444.2000, 446.0000])]\n",
            "84 [tensor([231., 232., 233., 234., 235., 236.]), tensor([447.8000, 449.6000, 451.4000, 453.2000, 455.0000, 456.8000])]\n",
            "85 [tensor([237., 238., 239., 240., 241., 242.]), tensor([458.6000, 460.4000, 462.2000, 464.0000, 465.8000, 467.6000])]\n",
            "86 [tensor([243., 244., 245., 246., 247., 248.]), tensor([469.4000, 471.2000, 473.0000, 474.8000, 476.6000, 478.4000])]\n",
            "87 [tensor([249., 250., 251., 252., 253., 254.]), tensor([480.2000, 482.0000, 483.8000, 485.6000, 487.4000, 489.2000])]\n",
            "88 [tensor([255., 256., 257., 258., 259., 260.]), tensor([491.0000, 492.8000, 494.6000, 496.4000, 498.2000, 500.0000])]\n",
            "89 [tensor([261., 262., 263., 264., 265., 266.]), tensor([501.8000, 503.6000, 505.4000, 507.2000, 509.0000, 510.8000])]\n",
            "90 [tensor([267., 268., 269., 270., 271., 272.]), tensor([512.6000, 514.4000, 516.2000, 518.0000, 519.8000, 521.6000])]\n",
            "91 [tensor([273., 274., 275., 276., 277., 278.]), tensor([523.4000, 525.2000, 527.0000, 528.8000, 530.6000, 532.4000])]\n",
            "92 [tensor([279., 280., 281., 282., 283., 284.]), tensor([534.2000, 536.0000, 537.8000, 539.6000, 541.4000, 543.2000])]\n",
            "93 [tensor([285., 286., 287., 288., 289., 290.]), tensor([545.0000, 546.8000, 548.6000, 550.4000, 552.2000, 554.0000])]\n",
            "94 [tensor([291., 292., 293., 294., 295., 296.]), tensor([555.8000, 557.6000, 559.4000, 561.2000, 563.0000, 564.8000])]\n",
            "95 [tensor([297., 298., 299., 300., 301., 302.]), tensor([566.6000, 568.4000, 570.2000, 572.0000, 573.8000, 575.6000])]\n",
            "96 [tensor([303., 304., 305., 306., 307., 308.]), tensor([577.4000, 579.2000, 581.0000, 582.8000, 584.6000, 586.4000])]\n",
            "97 [tensor([309., 310., 311., 312., 313., 314.]), tensor([588.2000, 590.0000, 591.8000, 593.6000, 595.4000, 597.2000])]\n",
            "98 [tensor([315., 316., 317., 318., 319., 320.]), tensor([599.0000, 600.8000, 602.6000, 604.4000, 606.2000, 608.0000])]\n",
            "99 [tensor([321., 322., 323., 324., 325., 326.]), tensor([609.8000, 611.6000, 613.4000, 615.2000, 617.0000, 618.8000])]\n",
            "100 [tensor([327., 328., 329., 330., 331., 332.]), tensor([620.6000, 622.4000, 624.2000, 626.0000, 627.8000, 629.6000])]\n",
            "101 [tensor([333., 334., 335., 336., 337., 338.]), tensor([631.4000, 633.2000, 635.0000, 636.8000, 638.6000, 640.4000])]\n",
            "102 [tensor([339., 340., 341., 342., 343., 344.]), tensor([642.2000, 644.0000, 645.8000, 647.6000, 649.4000, 651.2000])]\n",
            "103 [tensor([345., 346., 347., 348., 349., 350.]), tensor([653.0000, 654.8000, 656.6000, 658.4000, 660.2000, 662.0000])]\n",
            "104 [tensor([351., 352., 353., 354., 355., 356.]), tensor([663.8000, 665.6000, 667.4000, 669.2000, 671.0000, 672.8000])]\n",
            "105 [tensor([357., 358., 359., 360., 361., 362.]), tensor([674.6000, 676.4000, 678.2000, 680.0000, 681.8000, 683.6000])]\n",
            "106 [tensor([363., 364., 365., 366., 367., 368.]), tensor([685.4000, 687.2000, 689.0000, 690.8000, 692.6000, 694.4000])]\n",
            "107 [tensor([369., 370., 371., 372., 373., 374.]), tensor([696.2000, 698.0000, 699.8000, 701.6000, 703.4000, 705.2000])]\n",
            "108 [tensor([375., 376., 377., 378., 379., 380.]), tensor([707.0000, 708.8000, 710.6000, 712.4000, 714.2000, 716.0000])]\n",
            "109 [tensor([381., 382., 383., 384., 385., 386.]), tensor([717.8000, 719.6000, 721.4000, 723.2000, 725.0000, 726.8000])]\n",
            "110 [tensor([387., 388., 389., 390., 391., 392.]), tensor([728.6000, 730.4000, 732.2000, 734.0000, 735.8000, 737.6000])]\n",
            "111 [tensor([393., 394., 395., 396., 397., 398.]), tensor([739.4000, 741.2000, 743.0000, 744.8000, 746.6000, 748.4000])]\n",
            "112 [tensor([399., 400., 401., 402., 403., 404.]), tensor([750.2000, 752.0000, 753.8000, 755.6000, 757.4000, 759.2000])]\n",
            "113 [tensor([405., 406., 407., 408., 409., 410.]), tensor([761.0000, 762.8000, 764.6000, 766.4000, 768.2000, 770.0000])]\n",
            "114 [tensor([411., 412., 413., 414., 415., 416.]), tensor([771.8000, 773.6000, 775.4000, 777.2000, 779.0000, 780.8000])]\n",
            "115 [tensor([417., 418., 419., 420., 421., 422.]), tensor([782.6000, 784.4000, 786.2000, 788.0000, 789.8000, 791.6000])]\n",
            "116 [tensor([423., 424., 425., 426., 427., 428.]), tensor([793.4000, 795.2000, 797.0000, 798.8000, 800.6000, 802.4000])]\n",
            "117 [tensor([429., 430., 431., 432., 433., 434.]), tensor([804.2000, 806.0000, 807.8000, 809.6000, 811.4000, 813.2000])]\n",
            "118 [tensor([435., 436., 437., 438., 439., 440.]), tensor([815.0000, 816.8000, 818.6000, 820.4000, 822.2000, 824.0000])]\n",
            "119 [tensor([441., 442., 443., 444., 445., 446.]), tensor([825.8000, 827.6000, 829.4000, 831.2000, 833.0000, 834.8000])]\n",
            "120 [tensor([447., 448., 449., 450., 451., 452.]), tensor([836.6000, 838.4000, 840.2000, 842.0000, 843.8000, 845.6000])]\n",
            "121 [tensor([453., 454., 455., 456., 457., 458.]), tensor([847.4000, 849.2000, 851.0000, 852.8000, 854.6000, 856.4000])]\n",
            "122 [tensor([459., 460., 461., 462., 463., 464.]), tensor([858.2000, 860.0000, 861.8000, 863.6000, 865.4000, 867.2000])]\n",
            "123 [tensor([465., 466., 467., 468., 469., 470.]), tensor([869.0000, 870.8000, 872.6000, 874.4000, 876.2000, 878.0000])]\n",
            "124 [tensor([471., 472., 473., 474., 475., 476.]), tensor([879.8000, 881.6000, 883.4000, 885.2000, 887.0000, 888.8000])]\n",
            "125 [tensor([477., 478., 479., 480., 481., 482.]), tensor([890.6000, 892.4000, 894.2000, 896.0000, 897.8000, 899.6000])]\n",
            "126 [tensor([483., 484., 485., 486., 487., 488.]), tensor([901.4000, 903.2000, 905.0000, 906.8000, 908.6000, 910.4000])]\n",
            "127 [tensor([489., 490., 491., 492., 493., 494.]), tensor([912.2000, 914.0000, 915.8000, 917.6000, 919.4000, 921.2000])]\n",
            "128 [tensor([495., 496., 497., 498., 499., 500.]), tensor([923.0000, 924.8000, 926.6000, 928.4000, 930.2000, 932.0000])]\n",
            "129 [tensor([501., 502., 503., 504., 505., 506.]), tensor([933.8000, 935.6000, 937.4000, 939.2000, 941.0000, 942.8000])]\n",
            "130 [tensor([507., 508., 509., 510., 511., 512.]), tensor([944.6000, 946.4000, 948.2000, 950.0000, 951.8000, 953.6000])]\n",
            "131 [tensor([513., 514., 515., 516., 517., 518.]), tensor([955.4000, 957.2000, 959.0000, 960.8000, 962.6000, 964.4000])]\n",
            "132 [tensor([519., 520., 521., 522., 523., 524.]), tensor([966.2000, 968.0000, 969.8000, 971.6000, 973.4000, 975.2000])]\n",
            "133 [tensor([525., 526., 527., 528., 529., 530.]), tensor([977.0000, 978.8000, 980.6000, 982.4000, 984.2000, 986.0000])]\n",
            "134 [tensor([531., 532., 533., 534., 535., 536.]), tensor([987.8000, 989.6000, 991.4000, 993.2000, 995.0000, 996.8000])]\n",
            "135 [tensor([537., 538., 539., 540., 541., 542.]), tensor([ 998.6000, 1000.4000, 1002.2000, 1004.0000, 1005.8000, 1007.6000])]\n",
            "136 [tensor([543., 544., 545., 546., 547., 548.]), tensor([1009.4000, 1011.2000, 1013.0000, 1014.8000, 1016.6000, 1018.4000])]\n",
            "137 [tensor([549., 550., 551., 552., 553., 554.]), tensor([1020.2000, 1022.0000, 1023.8000, 1025.6000, 1027.3999, 1029.2000])]\n",
            "138 [tensor([555., 556., 557., 558., 559., 560.]), tensor([1031.0000, 1032.8000, 1034.6000, 1036.3999, 1038.2000, 1040.0000])]\n",
            "139 [tensor([561., 562., 563., 564., 565., 566.]), tensor([1041.8000, 1043.6000, 1045.3999, 1047.2000, 1049.0000, 1050.8000])]\n",
            "140 [tensor([567., 568., 569., 570., 571., 572.]), tensor([1052.6000, 1054.3999, 1056.2000, 1058.0000, 1059.7999, 1061.6000])]\n",
            "141 [tensor([573., 574., 575., 576., 577., 578.]), tensor([1063.4000, 1065.2000, 1067.0000, 1068.7999, 1070.6000, 1072.4000])]\n",
            "142 [tensor([579., 580., 581., 582., 583., 584.]), tensor([1074.2000, 1076.0000, 1077.7999, 1079.6000, 1081.4000, 1083.2000])]\n",
            "143 [tensor([585., 586., 587., 588., 589., 590.]), tensor([1085.0000, 1086.7999, 1088.6000, 1090.4000, 1092.2000, 1094.0000])]\n",
            "144 [tensor([591., 592., 593., 594., 595., 596.]), tensor([1095.7999, 1097.6000, 1099.4000, 1101.2000, 1103.0000, 1104.7999])]\n",
            "145 [tensor([597., 598., 599., 600., 601., 602.]), tensor([1106.6000, 1108.4000, 1110.2000, 1112.0000, 1113.7999, 1115.6000])]\n",
            "146 [tensor([603., 604., 605., 606., 607., 608.]), tensor([1117.4000, 1119.2000, 1121.0000, 1122.7999, 1124.6000, 1126.4000])]\n",
            "147 [tensor([609., 610., 611., 612., 613., 614.]), tensor([1128.2000, 1130.0000, 1131.7999, 1133.6000, 1135.4000, 1137.2000])]\n",
            "148 [tensor([615., 616., 617., 618., 619., 620.]), tensor([1139.0000, 1140.7999, 1142.6000, 1144.4000, 1146.2000, 1148.0000])]\n",
            "149 [tensor([621., 622., 623., 624., 625., 626.]), tensor([1149.7999, 1151.6000, 1153.4000, 1155.2000, 1157.0000, 1158.7999])]\n",
            "150 [tensor([627., 628., 629., 630., 631., 632.]), tensor([1160.6000, 1162.4000, 1164.2000, 1166.0000, 1167.7999, 1169.6000])]\n",
            "151 [tensor([633., 634., 635., 636., 637., 638.]), tensor([1171.4000, 1173.2000, 1175.0000, 1176.7999, 1178.6000, 1180.4000])]\n",
            "152 [tensor([639., 640., 641., 642., 643., 644.]), tensor([1182.2000, 1184.0000, 1185.7999, 1187.6000, 1189.4000, 1191.2000])]\n",
            "153 [tensor([645., 646., 647., 648., 649., 650.]), tensor([1193.0000, 1194.7999, 1196.6000, 1198.4000, 1200.2000, 1202.0000])]\n",
            "154 [tensor([651., 652., 653., 654., 655., 656.]), tensor([1203.7999, 1205.6000, 1207.4000, 1209.2000, 1211.0000, 1212.7999])]\n",
            "155 [tensor([657., 658., 659., 660., 661., 662.]), tensor([1214.6000, 1216.4000, 1218.2000, 1220.0000, 1221.7999, 1223.6000])]\n",
            "156 [tensor([663., 664., 665., 666., 667., 668.]), tensor([1225.4000, 1227.2000, 1229.0000, 1230.7999, 1232.6000, 1234.4000])]\n",
            "157 [tensor([669., 670., 671., 672., 673., 674.]), tensor([1236.2000, 1238.0000, 1239.7999, 1241.6000, 1243.4000, 1245.2000])]\n",
            "158 [tensor([675., 676., 677., 678., 679., 680.]), tensor([1247.0000, 1248.7999, 1250.6000, 1252.4000, 1254.2000, 1256.0000])]\n",
            "159 [tensor([681., 682., 683., 684., 685., 686.]), tensor([1257.7999, 1259.6000, 1261.4000, 1263.2000, 1265.0000, 1266.7999])]\n",
            "160 [tensor([687., 688., 689., 690., 691., 692.]), tensor([1268.6000, 1270.4000, 1272.2000, 1274.0000, 1275.7999, 1277.6000])]\n",
            "161 [tensor([693., 694., 695., 696., 697., 698.]), tensor([1279.4000, 1281.2000, 1283.0000, 1284.7999, 1286.6000, 1288.4000])]\n",
            "162 [tensor([699., 700., 701., 702., 703., 704.]), tensor([1290.2000, 1292.0000, 1293.7999, 1295.6000, 1297.4000, 1299.2000])]\n",
            "163 [tensor([705., 706., 707., 708., 709., 710.]), tensor([1301.0000, 1302.7999, 1304.6000, 1306.4000, 1308.2000, 1310.0000])]\n",
            "164 [tensor([711., 712., 713., 714., 715., 716.]), tensor([1311.7999, 1313.6000, 1315.4000, 1317.2000, 1319.0000, 1320.7999])]\n",
            "165 [tensor([717., 718., 719., 720., 721., 722.]), tensor([1322.6000, 1324.4000, 1326.2000, 1328.0000, 1329.7999, 1331.6000])]\n",
            "166 [tensor([723., 724., 725., 726., 727., 728.]), tensor([1333.4000, 1335.2000, 1337.0000, 1338.7999, 1340.6000, 1342.4000])]\n",
            "167 [tensor([729., 730., 731., 732., 733., 734.]), tensor([1344.2000, 1346.0000, 1347.7999, 1349.6000, 1351.4000, 1353.2000])]\n",
            "168 [tensor([735., 736., 737., 738., 739., 740.]), tensor([1355.0000, 1356.7999, 1358.6000, 1360.4000, 1362.2000, 1364.0000])]\n",
            "169 [tensor([741., 742., 743., 744., 745., 746.]), tensor([1365.7999, 1367.6000, 1369.4000, 1371.2000, 1373.0000, 1374.7999])]\n",
            "170 [tensor([747., 748., 749., 750., 751., 752.]), tensor([1376.6000, 1378.4000, 1380.2000, 1382.0000, 1383.7999, 1385.6000])]\n",
            "171 [tensor([753., 754., 755., 756., 757., 758.]), tensor([1387.4000, 1389.2000, 1391.0000, 1392.7999, 1394.6000, 1396.4000])]\n",
            "172 [tensor([759., 760., 761., 762., 763., 764.]), tensor([1398.2000, 1400.0000, 1401.7999, 1403.6000, 1405.4000, 1407.2000])]\n",
            "173 [tensor([765., 766., 767., 768., 769., 770.]), tensor([1409.0000, 1410.7999, 1412.6000, 1414.3999, 1416.2000, 1418.0000])]\n",
            "174 [tensor([771., 772., 773., 774., 775., 776.]), tensor([1419.7999, 1421.6000, 1423.3999, 1425.2000, 1427.0000, 1428.7999])]\n",
            "175 [tensor([777., 778., 779., 780., 781., 782.]), tensor([1430.6000, 1432.3999, 1434.2000, 1436.0000, 1437.7999, 1439.6000])]\n",
            "176 [tensor([783., 784., 785., 786., 787., 788.]), tensor([1441.3999, 1443.2000, 1445.0000, 1446.7999, 1448.6000, 1450.3999])]\n",
            "177 [tensor([789., 790., 791., 792., 793., 794.]), tensor([1452.2000, 1454.0000, 1455.7999, 1457.6000, 1459.3999, 1461.2000])]\n",
            "178 [tensor([795., 796., 797., 798., 799., 800.]), tensor([1463.0000, 1464.7999, 1466.6000, 1468.3999, 1470.2000, 1472.0000])]\n",
            "179 [tensor([801., 802., 803., 804., 805., 806.]), tensor([1473.7999, 1475.6000, 1477.3999, 1479.2000, 1481.0000, 1482.7999])]\n",
            "180 [tensor([807., 808., 809., 810., 811., 812.]), tensor([1484.6000, 1486.3999, 1488.2000, 1490.0000, 1491.7999, 1493.6000])]\n",
            "181 [tensor([813., 814., 815., 816., 817., 818.]), tensor([1495.3999, 1497.2000, 1499.0000, 1500.7999, 1502.6000, 1504.3999])]\n",
            "182 [tensor([819., 820., 821., 822., 823., 824.]), tensor([1506.2000, 1508.0000, 1509.7999, 1511.6000, 1513.3999, 1515.2000])]\n",
            "183 [tensor([825., 826., 827., 828., 829., 830.]), tensor([1517.0000, 1518.7999, 1520.6000, 1522.3999, 1524.2000, 1526.0000])]\n",
            "184 [tensor([831., 832., 833., 834., 835., 836.]), tensor([1527.7999, 1529.6000, 1531.3999, 1533.2000, 1535.0000, 1536.7999])]\n",
            "185 [tensor([837., 838., 839., 840., 841., 842.]), tensor([1538.6000, 1540.3999, 1542.2000, 1544.0000, 1545.7999, 1547.6000])]\n",
            "186 [tensor([843., 844., 845., 846., 847., 848.]), tensor([1549.3999, 1551.2000, 1553.0000, 1554.7999, 1556.6000, 1558.3999])]\n",
            "187 [tensor([849., 850., 851., 852., 853., 854.]), tensor([1560.2000, 1562.0000, 1563.7999, 1565.6000, 1567.3999, 1569.2000])]\n",
            "188 [tensor([855., 856., 857., 858., 859., 860.]), tensor([1571.0000, 1572.7999, 1574.6000, 1576.3999, 1578.2000, 1580.0000])]\n",
            "189 [tensor([861., 862., 863., 864., 865., 866.]), tensor([1581.7999, 1583.6000, 1585.3999, 1587.2000, 1589.0000, 1590.7999])]\n",
            "190 [tensor([867., 868., 869., 870., 871., 872.]), tensor([1592.6000, 1594.3999, 1596.2000, 1598.0000, 1599.7999, 1601.6000])]\n",
            "191 [tensor([873., 874., 875., 876., 877., 878.]), tensor([1603.3999, 1605.2000, 1607.0000, 1608.7999, 1610.6000, 1612.3999])]\n",
            "192 [tensor([879., 880., 881., 882., 883., 884.]), tensor([1614.2000, 1616.0000, 1617.7999, 1619.6000, 1621.3999, 1623.2000])]\n",
            "193 [tensor([885., 886., 887., 888., 889., 890.]), tensor([1625.0000, 1626.7999, 1628.6000, 1630.3999, 1632.2000, 1634.0000])]\n",
            "194 [tensor([891., 892., 893., 894., 895., 896.]), tensor([1635.7999, 1637.6000, 1639.3999, 1641.2000, 1643.0000, 1644.7999])]\n",
            "195 [tensor([897., 898., 899., 900., 901., 902.]), tensor([1646.6000, 1648.3999, 1650.2000, 1652.0000, 1653.7999, 1655.6000])]\n",
            "196 [tensor([903., 904., 905., 906., 907., 908.]), tensor([1657.3999, 1659.2000, 1661.0000, 1662.7999, 1664.6000, 1666.3999])]\n",
            "197 [tensor([909., 910., 911., 912., 913., 914.]), tensor([1668.2000, 1670.0000, 1671.7999, 1673.6000, 1675.3999, 1677.2000])]\n",
            "198 [tensor([915., 916., 917., 918., 919., 920.]), tensor([1679.0000, 1680.7999, 1682.6000, 1684.3999, 1686.2000, 1688.0000])]\n",
            "199 [tensor([921., 922., 923., 924., 925., 926.]), tensor([1689.7999, 1691.6000, 1693.3999, 1695.2000, 1697.0000, 1698.7999])]\n",
            "200 [tensor([927., 928., 929., 930., 931., 932.]), tensor([1700.6000, 1702.3999, 1704.2000, 1706.0000, 1707.7999, 1709.6000])]\n",
            "201 [tensor([933., 934., 935., 936., 937., 938.]), tensor([1711.3999, 1713.2000, 1715.0000, 1716.7999, 1718.6000, 1720.3999])]\n",
            "202 [tensor([939., 940., 941., 942., 943., 944.]), tensor([1722.2000, 1724.0000, 1725.7999, 1727.6000, 1729.3999, 1731.2000])]\n",
            "203 [tensor([945., 946., 947., 948., 949., 950.]), tensor([1733.0000, 1734.7999, 1736.6000, 1738.3999, 1740.2000, 1742.0000])]\n",
            "204 [tensor([951., 952., 953., 954., 955., 956.]), tensor([1743.7999, 1745.6000, 1747.3999, 1749.2000, 1751.0000, 1752.7999])]\n",
            "205 [tensor([957., 958., 959., 960., 961., 962.]), tensor([1754.6000, 1756.3999, 1758.2000, 1760.0000, 1761.7999, 1763.6000])]\n",
            "206 [tensor([963., 964., 965., 966., 967., 968.]), tensor([1765.3999, 1767.2000, 1769.0000, 1770.7999, 1772.6000, 1774.3999])]\n",
            "207 [tensor([969., 970., 971., 972., 973., 974.]), tensor([1776.2000, 1778.0000, 1779.7999, 1781.6000, 1783.3999, 1785.2000])]\n",
            "208 [tensor([975., 976., 977., 978., 979., 980.]), tensor([1787.0000, 1788.7999, 1790.6000, 1792.3999, 1794.2000, 1796.0000])]\n",
            "209 [tensor([981., 982., 983., 984., 985., 986.]), tensor([1797.7999, 1799.6000, 1801.3999, 1803.2000, 1805.0000, 1806.7999])]\n",
            "210 [tensor([987., 988., 989., 990., 991., 992.]), tensor([1808.6000, 1810.3999, 1812.2000, 1814.0000, 1815.7999, 1817.6000])]\n",
            "211 [tensor([993., 994., 995., 996., 997., 998.]), tensor([1819.3999, 1821.2000, 1823.0000, 1824.7999, 1826.6000, 1828.3999])]\n",
            "212 [tensor([999.]), tensor([1830.2000])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RZp6ft4SXihj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3929
        },
        "outputId": "d9488246-31ae-48e5-e052-a821bc489463"
      },
      "cell_type": "code",
      "source": [
        "mydataloader = torch.utils.data.DataLoader(mydataset,batch_size=6, shuffle=True, num_workers=2, drop_last=True)\n",
        "for i, batch in enumerate(mydataloader):\n",
        "  print(i, batch)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 [tensor([  80.,  693.,  236., -111.,  235.,  279.]), tensor([ 176.0000, 1279.4000,  456.8000, -167.8000,  455.0000,  534.2000])]\n",
            "1 [tensor([ 244.,  248.,  -53., -182.,  417.,  403.]), tensor([ 471.2000,  478.4000,  -63.4000, -295.6000,  782.6000,  757.4000])]\n",
            "2 [tensor([704., -28., 563., 645., 690., 660.]), tensor([1299.2000,  -18.4000, 1045.3999, 1193.0000, 1274.0000, 1220.0000])]\n",
            "3 [tensor([ 252.,  -84.,   69.,  -77., -191.,  232.]), tensor([ 485.6000, -119.2000,  156.2000, -106.6000, -311.8000,  449.6000])]\n",
            "4 [tensor([ 881.,  267.,  398.,  590.,  132., -118.]), tensor([1617.7999,  512.6000,  748.4000, 1094.0000,  269.6000, -180.4000])]\n",
            "5 [tensor([420., 507., 644., 947., 378., 174.]), tensor([ 788.0000,  944.6000, 1191.2000, 1736.6000,  712.4000,  345.2000])]\n",
            "6 [tensor([647., 954., 506., 184., 842., 914.]), tensor([1196.6000, 1749.2000,  942.8000,  363.2000, 1547.6000, 1677.2000])]\n",
            "7 [tensor([ 694.,  623.,  189.,  908.,  589., -180.]), tensor([1281.2000, 1153.4000,  372.2000, 1666.3999, 1092.2000, -292.0000])]\n",
            "8 [tensor([ 777.,  182., -192.,  535.,  217.,  -32.]), tensor([1430.6000,  359.6000, -313.6000,  995.0000,  422.6000,  -25.6000])]\n",
            "9 [tensor([-189.,  178., -121., -219.,  766.,  641.]), tensor([-308.2000,  352.4000, -185.8000, -362.2000, 1410.7999, 1185.7999])]\n",
            "10 [tensor([350., 998., 452., 903., 872., 926.]), tensor([ 662.0000, 1828.3999,  845.6000, 1657.3999, 1601.6000, 1698.7999])]\n",
            "11 [tensor([ 418.,  199.,  263., -161.,  -64.,  719.]), tensor([ 784.4000,  390.2000,  505.4000, -257.8000,  -83.2000, 1326.2000])]\n",
            "12 [tensor([698., 783., 100., 352., 725., 568.]), tensor([1288.4000, 1441.3999,  212.0000,  665.6000, 1337.0000, 1054.3999])]\n",
            "13 [tensor([113.,  89., 251., 822., 803., 210.]), tensor([ 235.4000,  192.2000,  483.8000, 1511.6000, 1477.3999,  410.0000])]\n",
            "14 [tensor([ 193., -240.,  -55.,  -67.,  441.,  408.]), tensor([ 379.4000, -400.0000,  -67.0000,  -88.6000,  825.8000,  766.4000])]\n",
            "15 [tensor([ 343.,  866., -254.,  805.,  802.,  285.]), tensor([ 649.4000, 1590.7999, -425.2000, 1481.0000, 1475.6000,  545.0000])]\n",
            "16 [tensor([ -27.,  423., -106.,  227.,   26.,  218.]), tensor([ -16.6000,  793.4000, -158.8000,  440.6000,   78.8000,  424.4000])]\n",
            "17 [tensor([-176.,  413.,  780.,  769., -179.,  294.]), tensor([-284.8000,  775.4000, 1436.0000, 1416.2000, -290.2000,  561.2000])]\n",
            "18 [tensor([ 457.,  754.,  755.,   74.,  482., -226.]), tensor([ 854.6000, 1389.2000, 1391.0000,  165.2000,  899.6000, -374.8000])]\n",
            "19 [tensor([-76., 384., 856., 703., 497., 810.]), tensor([-104.8000,  723.2000, 1572.7999, 1297.4000,  926.6000, 1490.0000])]\n",
            "20 [tensor([-221.,  868.,  -75.,  500., -244.,   86.]), tensor([-365.8000, 1594.3999, -103.0000,  932.0000, -407.2000,  186.8000])]\n",
            "21 [tensor([ 819.,  386.,   52., -213.,  374.,  503.]), tensor([1506.2000,  726.8000,  125.6000, -351.4000,  705.2000,  937.4000])]\n",
            "22 [tensor([485., 663., 811.,  -4., 177.,  81.]), tensor([ 905.0000, 1225.4000, 1491.7999,   24.8000,  350.6000,  177.8000])]\n",
            "23 [tensor([187.,   9., 611., 578.,  99., 297.]), tensor([ 368.6000,   48.2000, 1131.7999, 1072.4000,  210.2000,  566.6000])]\n",
            "24 [tensor([ 594., -107.,  468.,  526.,  -37.,  600.]), tensor([1101.2000, -160.6000,  874.4000,  978.8000,  -34.6000, 1112.0000])]\n",
            "25 [tensor([ 775.,  750., -196.,  547.,  592.,  631.]), tensor([1427.0000, 1382.0000, -320.8000, 1016.6000, 1097.6000, 1167.7999])]\n",
            "26 [tensor([ -60.,  733.,  781.,   33.,  395., -184.]), tensor([ -76.0000, 1351.4000, 1437.7999,   91.4000,  743.0000, -299.2000])]\n",
            "27 [tensor([-178.,  453.,  896.,  260.,  131.,  142.]), tensor([-288.4000,  847.4000, 1644.7999,  500.0000,  267.8000,  287.6000])]\n",
            "28 [tensor([264.,  -8.,  22., 375., 467., 201.]), tensor([507.2000,  17.6000,  71.6000, 707.0000, 872.6000, 393.8000])]\n",
            "29 [tensor([618., 567., 520., 153., -25., 301.]), tensor([1144.4000, 1052.6000,  968.0000,  307.4000,  -13.0000,  573.8000])]\n",
            "30 [tensor([ 276.,  579., -142.,  333.,  674.,  560.]), tensor([ 528.8000, 1074.2000, -223.6000,  631.4000, 1245.2000, 1040.0000])]\n",
            "31 [tensor([ 967.,  376., -233.,  321.,  186.,  829.]), tensor([1772.6000,  708.8000, -387.4000,  609.8000,  366.8000, 1524.2000])]\n",
            "32 [tensor([ 582.,  110., -139.,  -59.,  359.,  159.]), tensor([1079.6000,  230.0000, -218.2000,  -74.2000,  678.2000,  318.2000])]\n",
            "33 [tensor([ 857.,   98.,  167.,  686.,  179., -263.]), tensor([1574.6000,  208.4000,  332.6000, 1266.7999,  354.2000, -441.4000])]\n",
            "34 [tensor([460., 146., 221., 404., 628.,  77.]), tensor([ 860.0000,  294.8000,  429.8000,  759.2000, 1162.4000,  170.6000])]\n",
            "35 [tensor([ 925.,   43.,  957.,  850., -212.,  449.]), tensor([1697.0000,  109.4000, 1754.6000, 1562.0000, -349.6000,  840.2000])]\n",
            "36 [tensor([-50., 532.,   7., 847., 114., 985.]), tensor([ -58.0000,  989.6000,   44.6000, 1556.6000,  237.2000, 1805.0000])]\n",
            "37 [tensor([-194.,  157.,  598.,  472.,  -68.,  553.]), tensor([-317.2000,  314.6000, 1108.4000,  881.6000,  -90.4000, 1027.3999])]\n",
            "38 [tensor([  17.,  126., -157., -138.,  826.,  853.]), tensor([  62.6000,  258.8000, -250.6000, -216.4000, 1518.7999, 1567.3999])]\n",
            "39 [tensor([ 885.,  234.,  -95., -214., -147.,  172.]), tensor([1625.0000,  453.2000, -139.0000, -353.2000, -232.6000,  341.6000])]\n",
            "40 [tensor([965., 840., 679., 160.,  42., 115.]), tensor([1769.0000, 1544.0000, 1254.2000,  320.0000,  107.6000,  239.0000])]\n",
            "41 [tensor([597., 342., 738., 585.,  25., 345.]), tensor([1106.6000,  647.6000, 1360.4000, 1085.0000,   77.0000,  653.0000])]\n",
            "42 [tensor([ 763.,   39., -217.,  918.,  163.,  877.]), tensor([1405.4000,  102.2000, -358.6000, 1684.3999,  325.4000, 1610.6000])]\n",
            "43 [tensor([ 801.,  909.,  405.,  846., -167.,  858.]), tensor([1473.7999, 1668.2000,  761.0000, 1554.7999, -268.6000, 1576.3999])]\n",
            "44 [tensor([ 596.,  344.,  213.,  617., -109.,  471.]), tensor([1104.7999,  651.2000,  415.4000, 1142.6000, -164.2000,  879.8000])]\n",
            "45 [tensor([ 743.,  818.,  209., -247.,  108.,  440.]), tensor([1369.4000, 1504.3999,  408.2000, -412.6000,  226.4000,  824.0000])]\n",
            "46 [tensor([ 299.,  765.,  -79.,  642., -166.,  192.]), tensor([ 570.2000, 1409.0000, -110.2000, 1187.6000, -266.8000,  377.6000])]\n",
            "47 [tensor([  -7., -260., -172.,  488.,  745.,  915.]), tensor([  19.4000, -436.0000, -277.6000,  910.4000, 1373.0000, 1679.0000])]\n",
            "48 [tensor([ 550.,  438.,  785.,  692., -200.,  -97.]), tensor([1022.0000,  820.4000, 1445.0000, 1277.6000, -328.0000, -142.6000])]\n",
            "49 [tensor([ 430.,  945.,  484.,  977., -272.,  830.]), tensor([ 806.0000, 1733.0000,  903.2000, 1790.6000, -457.6000, 1526.0000])]\n",
            "50 [tensor([ 893.,  806., -256.,  261., -108.,  446.]), tensor([1639.3999, 1482.7999, -428.8000,  501.8000, -162.4000,  834.8000])]\n",
            "51 [tensor([303., 176., 898., 939., -21.,  19.]), tensor([ 577.4000,  348.8000, 1648.3999, 1722.2000,   -5.8000,   66.2000])]\n",
            "52 [tensor([ 875.,  966.,  433.,  979., -103.,  270.]), tensor([1607.0000, 1770.7999,  811.4000, 1794.2000, -153.4000,  518.0000])]\n",
            "53 [tensor([ 397., -251.,  706.,  916., -174.,  681.]), tensor([ 746.6000, -419.8000, 1302.7999, 1680.7999, -281.2000, 1257.7999])]\n",
            "54 [tensor([812., -43., 286., 996., 409., 943.]), tensor([1493.6000,  -45.4000,  546.8000, 1824.7999,  768.2000, 1729.3999])]\n",
            "55 [tensor([358., 571., 383., 357., 103., 910.]), tensor([ 676.4000, 1059.7999,  721.4000,  674.6000,  217.4000, 1670.0000])]\n",
            "56 [tensor([-122.,  993.,  656.,  -10.,  668.,  986.]), tensor([-187.6000, 1819.3999, 1212.7999,   14.0000, 1234.4000, 1806.7999])]\n",
            "57 [tensor([ 434., -154.,  -29.,  480.,  827.,  990.]), tensor([ 813.2000, -245.2000,  -20.2000,  896.0000, 1520.6000, 1814.0000])]\n",
            "58 [tensor([ 175.,  640.,  528.,  878.,  256., -137.]), tensor([ 347.0000, 1184.0000,  982.4000, 1612.3999,  492.8000, -214.6000])]\n",
            "59 [tensor([ 454.,  282.,  836.,  154.,  638., -164.]), tensor([ 849.2000,  539.6000, 1536.7999,  309.2000, 1180.4000, -263.2000])]\n",
            "60 [tensor([838., 804., 316., 511.,  20., 863.]), tensor([1540.3999, 1479.2000,  600.8000,  951.8000,   68.0000, 1585.3999])]\n",
            "61 [tensor([ 662.,  469.,  510.,  637., -126.,  932.]), tensor([1223.6000,  876.2000,  950.0000, 1178.6000, -194.8000, 1709.6000])]\n",
            "62 [tensor([ 385., -204.,  734.,  429.,  -33.,  -80.]), tensor([ 725.0000, -335.2000, 1353.2000,  804.2000,  -27.4000, -112.0000])]\n",
            "63 [tensor([666., 389., 318., 963., -86., 586.]), tensor([1230.7999,  732.2000,  604.4000, 1765.3999, -122.8000, 1086.7999])]\n",
            "64 [tensor([776., 581., 740., 102., 603., 120.]), tensor([1428.7999, 1077.7999, 1364.0000,  215.6000, 1117.4000,  248.0000])]\n",
            "65 [tensor([ 502.,  505.,  897.,  646.,  712., -115.]), tensor([ 935.6000,  941.0000, 1646.6000, 1194.7999, 1313.6000, -175.0000])]\n",
            "66 [tensor([900., 292., 672., 392., 859., -56.]), tensor([1652.0000,  557.6000, 1241.6000,  737.6000, 1578.2000,  -68.8000])]\n",
            "67 [tensor([ 483., -160.,  133.,  652.,  140.,  661.]), tensor([ 901.4000, -256.0000,  271.4000, 1205.6000,  284.0000, 1221.7999])]\n",
            "68 [tensor([ 57., 360., 410., 492., 238., 658.]), tensor([ 134.6000,  680.0000,  770.0000,  917.6000,  460.4000, 1216.4000])]\n",
            "69 [tensor([ 913.,  -78.,  837., -129.,  609.,  237.]), tensor([1675.3999, -108.4000, 1538.6000, -200.2000, 1128.2000,  458.6000])]\n",
            "70 [tensor([ -87.,  269.,  -19., -159.,  166.,  651.]), tensor([-124.6000,  516.2000,   -2.2000, -254.2000,  330.8000, 1203.7999])]\n",
            "71 [tensor([ 147., -239.,  593.,  787.,  -49., -169.]), tensor([ 296.6000, -398.2000, 1099.4000, 1448.6000,  -56.2000, -272.2000])]\n",
            "72 [tensor([825., 655., 865., 243., 476., 602.]), tensor([1517.0000, 1211.0000, 1589.0000,  469.4000,  888.8000, 1115.6000])]\n",
            "73 [tensor([ 771.,  752., -199.,  158.,  144.,  481.]), tensor([1419.7999, 1385.6000, -326.2000,  316.4000,  291.2000,  897.8000])]\n",
            "74 [tensor([687., 518., 109., 928.,  94., 406.]), tensor([1268.6000,  964.4000,  228.2000, 1702.3999,  201.2000,  762.8000])]\n",
            "75 [tensor([431., 705., 427., 302., 334., 363.]), tensor([ 807.8000, 1301.0000,  800.6000,  575.6000,  633.2000,  685.4000])]\n",
            "76 [tensor([-92., 988., 361., 744., 710., 987.]), tensor([-133.6000, 1810.3999,  681.8000, 1371.2000, 1310.0000, 1808.6000])]\n",
            "77 [tensor([  2., -11., 817., 788., 683., 229.]), tensor([  35.6000,   12.2000, 1502.6000, 1450.3999, 1261.4000,  444.2000])]\n",
            "78 [tensor([ 424.,  212., -222.,  188.,  124.,  -69.]), tensor([ 795.2000,  413.6000, -367.6000,  370.4000,  255.2000,  -92.2000])]\n",
            "79 [tensor([ 682., -144.,  615., -104.,  575.,  981.]), tensor([1259.6000, -227.2000, 1139.0000, -155.2000, 1067.0000, 1797.7999])]\n",
            "80 [tensor([844., 625., 289., 724., -22., 211.]), tensor([1551.2000, 1157.0000,  552.2000, 1335.2000,   -7.6000,  411.8000])]\n",
            "81 [tensor([305., 711., 691., 530., 138., 948.]), tensor([ 581.0000, 1311.7999, 1275.7999,  986.0000,  280.4000, 1738.3999])]\n",
            "82 [tensor([-65., 917., 702., 976., 495., 855.]), tensor([ -85.0000, 1682.6000, 1295.6000, 1788.7999,  923.0000, 1571.0000])]\n",
            "83 [tensor([ 562., -133.,  791.,  831.,  474.,  215.]), tensor([1043.6000, -207.4000, 1455.7999, 1527.7999,  885.2000,  419.0000])]\n",
            "84 [tensor([720., 447.,  60., -72., -15., 274.]), tensor([1328.0000,  836.6000,  140.0000,  -97.6000,    5.0000,  525.2000])]\n",
            "85 [tensor([512.,  41., 128., 388., 833., -85.]), tensor([ 953.6000,  105.8000,  262.4000,  730.4000, 1531.3999, -121.0000])]\n",
            "86 [tensor([ 516.,  369., -156.,  268.,  323.,  717.]), tensor([ 960.8000,  696.2000, -248.8000,  514.4000,  613.4000, 1322.6000])]\n",
            "87 [tensor([ -46., -113.,  747., -171.,   66.,  536.]), tensor([ -50.8000, -171.4000, 1376.6000, -275.8000,  150.8000,  996.8000])]\n",
            "88 [tensor([ 450.,  886.,  879.,  566.,   40., -123.]), tensor([ 842.0000, 1626.7999, 1614.2000, 1050.8000,  104.0000, -189.4000])]\n",
            "89 [tensor([-190.,  393.,  774.,  216.,  630.,  961.]), tensor([-310.0000,  739.4000, 1425.2000,  420.8000, 1166.0000, 1761.7999])]\n",
            "90 [tensor([ 583.,  716., -261.,  555., -201.,  -18.]), tensor([ 1.0814e+03,  1.3208e+03, -4.3780e+02,  1.0310e+03, -3.2980e+02,\n",
            "        -4.0000e-01])]\n",
            "91 [tensor([931., 448., 907., 657., 517., 790.]), tensor([1707.7999,  838.4000, 1664.6000, 1214.6000,  962.6000, 1454.0000])]\n",
            "92 [tensor([ 841.,  364., -208.,  892.,  185.,  924.]), tensor([1545.7999,  687.2000, -342.4000, 1637.6000,  365.0000, 1695.2000])]\n",
            "93 [tensor([508., -20., 529., 277., 854., 887.]), tensor([ 946.4000,   -4.0000,  984.2000,  530.6000, 1569.2000, 1628.6000])]\n",
            "94 [tensor([330.,  93., 473., 255., 864., -94.]), tensor([ 626.0000,  199.4000,  883.4000,  491.0000, 1587.2000, -137.2000])]\n",
            "95 [tensor([230.,  21., 118., 902., 439., 807.]), tensor([ 446.0000,   69.8000,  244.4000, 1655.6000,  822.2000, 1484.6000])]\n",
            "96 [tensor([ 240., -128.,   47.,  992.,  -44.,  542.]), tensor([ 464.0000, -198.4000,  116.6000, 1817.6000,  -47.2000, 1007.6000])]\n",
            "97 [tensor([ 757.,  -57.,  665.,  319.,  764., -215.]), tensor([1394.6000,  -70.6000, 1229.0000,  606.2000, 1407.2000, -355.0000])]\n",
            "98 [tensor([ 194.,  317.,  789.,  669., -248.,  551.]), tensor([ 381.2000,  602.6000, 1452.2000, 1236.2000, -414.4000, 1023.8000])]\n",
            "99 [tensor([ 523.,  937.,   38., -241.,  486.,  713.]), tensor([ 973.4000, 1718.6000,  100.4000, -401.8000,  906.8000, 1315.4000])]\n",
            "100 [tensor([ 929.,  258., -253.,  198.,   87., -110.]), tensor([1704.2000,  496.4000, -423.4000,  388.4000,  188.6000, -166.0000])]\n",
            "101 [tensor([ -23.,   54.,  953.,   23., -140.,  262.]), tensor([  -9.4000,  129.2000, 1747.3999,   73.4000, -220.0000,  503.6000])]\n",
            "102 [tensor([-218., -116.,  798.,  155.,   10.,  920.]), tensor([-360.4000, -176.8000, 1468.3999,  311.0000,   50.0000, 1688.0000])]\n",
            "103 [tensor([888., 402., 382.,  49., 635., 355.]), tensor([1630.3999,  755.6000,  719.6000,  120.2000, 1175.0000,  671.0000])]\n",
            "104 [tensor([ 183.,  809.,  773.,  727.,  906., -148.]), tensor([ 361.4000, 1488.2000, 1423.3999, 1340.6000, 1662.7999, -234.4000])]\n",
            "105 [tensor([722., 815., 329., 959., 137., -58.]), tensor([1331.6000, 1499.0000,  624.2000, 1758.2000,  278.6000,  -72.4000])]\n",
            "106 [tensor([-12., 714., 253., 814., -66., 876.]), tensor([  10.4000, 1317.2000,  487.4000, 1497.2000,  -86.8000, 1608.7999])]\n",
            "107 [tensor([444., 519., -91., 491., 527., 254.]), tensor([ 831.2000,  966.2000, -131.8000,  915.8000,  980.6000,  489.2000])]\n",
            "108 [tensor([552., 394., 591., -13., 796.,  67.]), tensor([1025.6000,  741.2000, 1095.7999,    8.6000, 1464.7999,  152.6000])]\n",
            "109 [tensor([718.,  97.,  79., 284., 327., 549.]), tensor([1324.4000,  206.6000,  174.2000,  543.2000,  620.6000, 1020.2000])]\n",
            "110 [tensor([ 577.,  533., -245.,  442.,  464.,  938.]), tensor([1070.6000,  991.4000, -409.0000,  827.6000,  867.2000, 1720.3999])]\n",
            "111 [tensor([624., 997., -30., 105., 573., 940.]), tensor([1155.2000, 1826.6000,  -22.0000,  221.0000, 1063.4000, 1724.0000])]\n",
            "112 [tensor([ 391.,  905.,  607.,  -81., -230.,  462.]), tensor([ 735.8000, 1661.0000, 1124.6000, -113.8000, -382.0000,  863.6000])]\n",
            "113 [tensor([899.,  32., 678.,  -9., 730., 606.]), tensor([1650.2000,   89.6000, 1252.4000,   15.8000, 1346.0000, 1122.7999])]\n",
            "114 [tensor([ 139.,  565.,  964.,  206., -130.,  -98.]), tensor([ 282.2000, 1049.0000, 1767.2000,  402.8000, -202.0000, -144.4000])]\n",
            "115 [tensor([346., 975., 605., 761., 860., 371.]), tensor([ 654.8000, 1787.0000, 1121.0000, 1401.7999, 1580.0000,  699.8000])]\n",
            "116 [tensor([293., -16., -35.,  44., 340., 515.]), tensor([559.4000,   3.2000, -31.0000, 111.2000, 644.0000, 959.0000])]\n",
            "117 [tensor([671., 952., 287., -89., 220., 633.]), tensor([1239.7999, 1745.6000,  548.6000, -128.2000,  428.0000, 1171.4000])]\n",
            "118 [tensor([ 266., -141.,  664.,  381., -153.,   58.]), tensor([ 510.8000, -221.8000, 1227.2000,  717.8000, -243.4000,  136.4000])]\n",
            "119 [tensor([ 770.,  786.,  222.,  134., -262., -173.]), tensor([1418.0000, 1446.7999,  431.6000,  273.2000, -439.6000, -279.4000])]\n",
            "120 [tensor([-181.,  707.,  852.,  587.,  351., -112.]), tensor([-293.8000, 1304.6000, 1565.6000, 1088.6000,  663.8000, -169.6000])]\n",
            "121 [tensor([861., 290., 612., -14., 307.,  83.]), tensor([1581.7999,  554.0000, 1133.6000,    6.8000,  584.6000,  181.4000])]\n",
            "122 [tensor([ 723., -136.,   55.,  983.,  621.,  561.]), tensor([1333.4000, -212.8000,  131.0000, 1801.3999, 1149.7999, 1041.8000])]\n",
            "123 [tensor([ 239.,  432.,  832., -125.,  396.,  564.]), tensor([ 462.2000,  809.6000, 1529.6000, -193.0000,  744.8000, 1047.2000])]\n",
            "124 [tensor([ 978.,  610., -223.,  758.,  778.,  808.]), tensor([1792.3999, 1130.0000, -369.4000, 1396.4000, 1432.3999, 1486.3999])]\n",
            "125 [tensor([  5., 314., 622., 373., 525., 336.]), tensor([  41.0000,  597.2000, 1151.6000,  703.4000,  977.0000,  636.8000])]\n",
            "126 [tensor([ 982.,  999.,  545.,  767., -224.,  367.]), tensor([1799.6000, 1830.2000, 1013.0000, 1412.6000, -371.2000,  692.6000])]\n",
            "127 [tensor([ 436.,  149.,   85.,  325.,  728., -271.]), tensor([ 816.8000,  300.2000,  185.0000,  617.0000, 1342.4000, -455.8000])]\n",
            "128 [tensor([ 869.,  726.,  106., -101., -143.,  -54.]), tensor([1596.2000, 1338.7999,  222.8000, -149.8000, -225.4000,  -65.2000])]\n",
            "129 [tensor([951., 820., 171.,  61., 499., 933.]), tensor([1743.7999, 1508.0000,  339.8000,  141.8000,  930.2000, 1711.3999])]\n",
            "130 [tensor([328., 521., 111., 399., 649.,  82.]), tensor([ 622.4000,  969.8000,  231.8000,  750.2000, 1200.2000,  179.6000])]\n",
            "131 [tensor([-258.,  -39.,  493.,  416.,  -73.,  498.]), tensor([-432.4000,  -38.2000,  919.4000,  780.8000,  -99.4000,  928.4000])]\n",
            "132 [tensor([246., 540.,   4., 169., 708., 792.]), tensor([ 474.8000, 1004.0000,   39.2000,  336.2000, 1306.4000, 1457.6000])]\n",
            "133 [tensor([ 46., 608., 968., 823., 422., 619.]), tensor([ 114.8000, 1126.4000, 1774.3999, 1513.3999,  791.6000, 1146.2000])]\n",
            "134 [tensor([ 291.,  784.,  -52.,   78.,   63., -211.]), tensor([ 555.8000, 1443.2000,  -61.6000,  172.4000,  145.4000, -347.8000])]\n",
            "135 [tensor([ 56., 994., 152., 443., 629., 196.]), tensor([ 132.8000, 1821.2000,  305.6000,  829.4000, 1164.2000,  384.8000])]\n",
            "136 [tensor([ 779.,   62.,   92.,  181.,  456., -175.]), tensor([1434.2000,  143.6000,  197.6000,  357.8000,  852.8000, -283.0000])]\n",
            "137 [tensor([ 31., -34., 729., 272., 143., 880.]), tensor([  87.8000,  -29.2000, 1344.2000,  521.6000,  289.4000, 1616.0000])]\n",
            "138 [tensor([-155.,  -36.,   12., -183.,  921., -250.]), tensor([-247.0000,  -32.8000,   53.6000, -297.4000, 1689.7999, -418.0000])]\n",
            "139 [tensor([569., 214., 122., 338., 298., 435.]), tensor([1056.2000,  417.2000,  251.6000,  640.4000,  568.4000,  815.0000])]\n",
            "140 [tensor([-207.,   30.,  231.,  849.,  782.,    1.]), tensor([-340.6000,   86.0000,  447.8000, 1560.2000, 1439.6000,   33.8000])]\n",
            "141 [tensor([ 634.,  -38.,  748., -227., -145.,  884.]), tensor([1173.2000,  -36.4000, 1378.4000, -376.6000, -229.0000, 1623.2000])]\n",
            "142 [tensor([ 558.,  891.,  922., -134.,  543.,  -71.]), tensor([1036.3999, 1635.7999, 1691.6000, -209.2000, 1009.4000,  -95.8000])]\n",
            "143 [tensor([-193.,  580.,   16.,   53., -195., -237.]), tensor([-315.4000, 1076.0000,   60.8000,  127.4000, -319.0000, -394.6000])]\n",
            "144 [tensor([ 697.,  737.,  604.,  313., -238.,  816.]), tensor([1286.6000, 1358.6000, 1119.2000,  595.4000, -396.4000, 1500.7999])]\n",
            "145 [tensor([ 749.,  556., -119.,  273.,   35.,  576.]), tensor([1380.2000, 1032.8000, -182.2000,  523.4000,   95.0000, 1068.7999])]\n",
            "146 [tensor([  13.,  379.,  451., -158., -132.,   -5.]), tensor([  55.4000,  714.2000,  843.8000, -252.4000, -205.6000,   23.0000])]\n",
            "147 [tensor([-197.,  242.,  972.,  421.,  696.,   48.]), tensor([-322.6000,  467.6000, 1781.6000,  789.8000, 1284.7999,  118.4000])]\n",
            "148 [tensor([958., 101., 584., 461., 676., 935.]), tensor([1756.3999,  213.8000, 1083.2000,  861.8000, 1248.7999, 1715.0000])]\n",
            "149 [tensor([  51.,  465.,  559.,   91., -198.,  326.]), tensor([ 123.8000,  869.0000, 1038.2000,  195.8000, -324.4000,  618.8000])]\n",
            "150 [tensor([546., 643., 257., -93., 190., 524.]), tensor([1014.8000, 1189.4000,  494.6000, -135.4000,  374.0000,  975.2000])]\n",
            "151 [tensor([ 121.,  648.,    8., -146., -225.,  123.]), tensor([ 249.8000, 1198.4000,   46.4000, -230.8000, -373.0000,  253.4000])]\n",
            "152 [tensor([-249., -259.,  127., -265.,  195.,  701.]), tensor([-416.2000, -434.2000,  260.6000, -445.0000,  383.0000, 1293.7999])]\n",
            "153 [tensor([ 673.,  636.,  949.,   14.,  601., -163.]), tensor([1243.4000, 1176.7999, 1740.2000,   57.2000, 1113.7999, -261.4000])]\n",
            "154 [tensor([459., 813., 170., 670., 490., 205.]), tensor([ 858.2000, 1495.3999,  338.0000, 1238.0000,  914.0000,  401.0000])]\n",
            "155 [tensor([685., 797., 265., 136., 980., 995.]), tensor([1265.0000, 1466.6000,  509.0000,  276.8000, 1796.0000, 1823.0000])]\n",
            "156 [tensor([  -2.,  -51.,  387., -120.,  795., -202.]), tensor([  28.4000,  -59.8000,  728.6000, -184.0000, 1463.0000, -331.6000])]\n",
            "157 [tensor([-131., -185., -229., -269.,  331.,  377.]), tensor([-203.8000, -301.0000, -380.2000, -452.2000,  627.8000,  710.6000])]\n",
            "158 [tensor([ 971.,  288., -162.,  165.,   29., -220.]), tensor([1779.7999,  550.4000, -259.6000,  329.0000,   84.2000, -364.0000])]\n",
            "159 [tensor([772., 839., 145., 824., -26.,  18.]), tensor([1421.6000, 1542.2000,  293.0000, 1515.2000,  -14.8000,   64.4000])]\n",
            "160 [tensor([-102.,  741.,  632.,  956.,  946.,  129.]), tensor([-151.6000, 1365.7999, 1169.6000, 1752.7999, 1734.7999,  264.2000])]\n",
            "161 [tensor([ 768.,  233.,   -1.,  541.,  207., -188.]), tensor([1414.3999,  451.4000,   30.2000, 1005.8000,  404.6000, -306.4000])]\n",
            "162 [tensor([250., 335., 513., 487., 197., 489.]), tensor([482.0000, 635.0000, 955.4000, 908.6000, 386.6000, 912.2000])]\n",
            "163 [tensor([895.,  68., -99., 380.,  84.,  -3.]), tensor([1643.0000,  154.4000, -146.2000,  716.0000,  183.2000,   26.6000])]\n",
            "164 [tensor([539.,  45., 354., 969., 862., 365.]), tensor([1002.2000,  113.0000,  669.2000, 1776.2000, 1583.6000,  689.0000])]\n",
            "165 [tensor([-114.,  324.,  927., -268., -266.,  843.]), tensor([-173.2000,  615.2000, 1700.6000, -450.4000, -446.8000, 1549.3999])]\n",
            "166 [tensor([-234.,    6.,  -17.,  509.,  627.,  760.]), tensor([-389.2000,   42.8000,    1.4000,  948.2000, 1160.6000, 1400.0000])]\n",
            "167 [tensor([911., -88., 709.,   3.,  72.,  34.]), tensor([1671.7999, -126.4000, 1308.2000,   37.4000,  161.6000,   93.2000])]\n",
            "168 [tensor([411., 226., 419., 834., -47., 821.]), tensor([ 771.8000,  438.8000,  786.2000, 1533.2000,  -52.6000, 1509.7999])]\n",
            "169 [tensor([ 588.,   27., -186.,  522.,  151.,  470.]), tensor([1090.4000,   80.6000, -302.8000,  971.6000,  303.8000,  878.0000])]\n",
            "170 [tensor([ 974.,   65.,  225., -187.,  742.,  874.]), tensor([1785.2000,  149.0000,  437.0000, -304.6000, 1367.6000, 1605.2000])]\n",
            "171 [tensor([882., 415.,  96., 339., 407., 667.]), tensor([1619.6000,  779.0000,  204.8000,  642.2000,  764.6000, 1232.6000])]\n",
            "172 [tensor([ 70., 501., 310., -31., 203., -48.]), tensor([158.0000, 933.8000, 590.0000, -23.8000, 397.4000, -54.4000])]\n",
            "173 [tensor([ 223., -206.,  793., -257.,   75.,  894.]), tensor([ 433.4000, -338.8000, 1459.3999, -430.6000,  167.0000, 1641.2000])]\n",
            "174 [tensor([ 164.,   28., -135.,  312.,  800.,  271.]), tensor([ 327.2000,   82.4000, -211.0000,  593.6000, 1472.0000,  519.8000])]\n",
            "175 [tensor([ 504.,  173.,  348.,  735.,  112., -152.]), tensor([ 939.2000,  343.4000,  658.4000, 1355.0000,  233.6000, -241.6000])]\n",
            "176 [tensor([ 639.,  125., -270.,  936.,  191.,  574.]), tensor([1182.2000,  257.0000, -454.0000, 1716.7999,  375.8000, 1065.2000])]\n",
            "177 [tensor([-228., -151.,  599.,   71.,  883.,  544.]), tensor([-378.4000, -239.8000, 1110.2000,  159.8000, 1621.3999, 1011.2000])]\n",
            "178 [tensor([ 889.,  401.,  828.,  659., -168.,  537.]), tensor([1632.2000,  753.8000, 1522.3999, 1218.2000, -270.4000,  998.6000])]\n",
            "179 [tensor([  95.,  295., -235.,  731.,  -40.,  941.]), tensor([ 203.0000,  563.0000, -391.0000, 1347.7999,  -40.0000, 1725.7999])]\n",
            "180 [tensor([-273.,  851.,   -6.,  477.,  148.,  538.]), tensor([-459.4000, 1563.7999,   21.2000,  890.6000,  298.4000, 1000.4000])]\n",
            "181 [tensor([ 73., 496., 746., 349., 332., 370.]), tensor([ 163.4000,  924.8000, 1374.7999,  660.2000,  629.6000,  698.0000])]\n",
            "182 [tensor([-170.,  156.,  107.,  572.,  219.,  -70.]), tensor([-274.0000,  312.8000,  224.6000, 1061.6000,  426.2000,  -94.0000])]\n",
            "183 [tensor([736., 845., 514., 762., 930., 534.]), tensor([1356.7999, 1553.0000,  957.2000, 1403.6000, 1706.0000,  993.2000])]\n",
            "184 [tensor([ 322.,  -42.,  942.,  650.,  249., -243.]), tensor([ 611.6000,  -43.6000, 1727.6000, 1202.0000,  480.2000, -405.4000])]\n",
            "185 [tensor([ 721.,  890.,  479.,  283.,  616., -205.]), tensor([1329.7999, 1634.0000,  894.2000,  541.4000, 1140.7999, -337.0000])]\n",
            "186 [tensor([ 425., -252., -203., -150.,  700.,  835.]), tensor([ 797.0000, -421.6000, -333.4000, -238.0000, 1292.0000, 1535.0000])]\n",
            "187 [tensor([ 400.,  308.,  -41., -117.,  150.,  613.]), tensor([ 752.0000,  586.4000,  -41.8000, -178.6000,  302.0000, 1135.4000])]\n",
            "188 [tensor([595., 960., 280.,  90., 984., 654.]), tensor([1103.0000, 1760.0000,  536.0000,  194.0000, 1803.2000, 1209.2000])]\n",
            "189 [tensor([ 141.,  626.,  -82., -105.,  168.,  732.]), tensor([ 285.8000, 1158.7999, -115.6000, -157.0000,  334.4000, 1349.6000])]\n",
            "190 [tensor([135.,  37., 759., 570., 873., 309.]), tensor([ 275.0000,   98.6000, 1398.2000, 1058.0000, 1603.3999,  588.2000])]\n",
            "191 [tensor([ 208.,  356., -127.,  414.,  104.,  848.]), tensor([ 406.4000,  672.8000, -196.6000,  777.2000,  219.2000, 1558.3999])]\n",
            "192 [tensor([ 162.,  684.,  962.,  680.,  247., -100.]), tensor([ 323.6000, 1263.2000, 1763.6000, 1256.0000,  476.6000, -148.0000])]\n",
            "193 [tensor([ 614., -165.,  259.,  228.,  715.,  756.]), tensor([1137.2000, -265.0000,  498.2000,  442.4000, 1319.0000, 1392.7999])]\n",
            "194 [tensor([ 59., 119., 224., 699., 689., 296.]), tensor([ 138.2000,  246.2000,  435.2000, 1290.2000, 1272.2000,  564.8000])]\n",
            "195 [tensor([241., 557., 337., 372., 944., 923.]), tensor([ 465.8000, 1034.6000,  638.6000,  701.6000, 1731.2000, 1693.3999])]\n",
            "196 [tensor([ 970., -232., -267.,  620.,  353.,  366.]), tensor([1778.0000, -385.6000, -448.6000, 1148.0000,  667.4000,  690.8000])]\n",
            "197 [tensor([ 989.,  311., -231.,  245.,  554.,  368.]), tensor([1812.2000,  591.8000, -383.8000,  473.0000, 1029.2000,  694.4000])]\n",
            "198 [tensor([300., 428., 463.,  76.,  50., -90.]), tensor([ 572.0000,  802.4000,  865.4000,  168.8000,  122.0000, -130.0000])]\n",
            "199 [tensor([ 180.,  161.,  867.,  362.,  278., -177.]), tensor([ 356.0000,  321.8000, 1592.6000,  683.6000,  532.4000, -286.6000])]\n",
            "200 [tensor([ 390.,  494.,  445.,  -62., -246.,  677.]), tensor([ 734.0000,  921.2000,  833.0000,  -79.6000, -410.8000, 1250.6000])]\n",
            "201 [tensor([ 475.,  688.,  653.,  695., -264.,  -83.]), tensor([ 887.0000, 1270.4000, 1207.4000, 1283.0000, -443.2000, -117.4000])]\n",
            "202 [tensor([ 478.,  458., -124.,  -96.,  200.,  870.]), tensor([ 892.4000,  856.4000, -191.2000, -140.8000,  392.0000, 1598.0000])]\n",
            "203 [tensor([ 455., -210.,  751.,  -24., -255.,  426.]), tensor([ 851.0000, -346.0000, 1383.7999,  -11.2000, -427.0000,  798.8000])]\n",
            "204 [tensor([117., -45.,  15., 304., 919., 794.]), tensor([ 242.6000,  -49.0000,   59.0000,  579.2000, 1686.2000, 1461.2000])]\n",
            "205 [tensor([  24., -242.,  912.,  753., -236.,  281.]), tensor([  75.2000, -403.6000, 1673.6000, 1387.4000, -392.8000,  537.8000])]\n",
            "206 [tensor([ 412.,  991.,  950.,  116.,   64., -209.]), tensor([ 773.6000, 1815.7999, 1742.0000,  240.8000,  147.2000, -344.2000])]\n",
            "207 [tensor([466., 347., 904., 871., 341., 675.]), tensor([ 870.8000,  656.6000, 1659.2000, 1599.7999,  645.8000, 1247.0000])]\n",
            "208 [tensor([ 204.,  955.,  799.,  934., -216.,  901.]), tensor([ 399.2000, 1751.0000, 1470.2000, 1713.2000, -356.8000, 1653.7999])]\n",
            "209 [tensor([ 88., -74., 739.,  11., 548., 130.]), tensor([ 190.4000, -101.2000, 1362.2000,   51.8000, 1018.4000,  266.0000])]\n",
            "210 [tensor([ 306.,  202.,  437., -149.,   36.,  -63.]), tensor([ 582.8000,  395.6000,  818.6000, -236.2000,   96.8000,  -81.4000])]\n",
            "211 [tensor([-61., 531., 275., 973.,   0., 320.]), tensor([ -77.8000,  987.8000,  527.0000, 1783.3999,   32.0000,  608.0000])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gb7YfSLF7pq0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "class RangeDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, start, end, step=1):\n",
        "    self.start = start\n",
        "    self.end = end\n",
        "    self.step = step\n",
        "  def __getitem__(self, index):\n",
        "    value = self.start + index * self.step\n",
        "    assert value < self.end\n",
        "    return value\n",
        "  def __len__(self):\n",
        "    return math.ceil((self.end - self.start) / self.step)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8je0OqhR7yKC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "9cb17890-7e31-4cac-de11-bbcfa5d4702e"
      },
      "cell_type": "code",
      "source": [
        "dataset = RangeDataset(0, 10)\n",
        "data_loader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=True, num_workers=2, drop_last=True)\n",
        "\n",
        "for i, batch in enumerate(data_loader):\n",
        "  print(i, batch)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 tensor([0, 6, 2, 4])\n",
            "1 tensor([8, 1, 3, 7])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "khir-2og7-Yx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* Here, we set batch_size to 4, so returned tensors will contain exactly four values. \n",
        "\n",
        "* By passing shuffle=True, the index sequence with which data is accessed is permuted, such that individual samples will be returned in random order. \n",
        "\n",
        "* We also passed drop_last=True, so that if the number of samples left for the final batch of the dataset is less than the specified batch_size, that batch is not returned. This ensures that all batches have the same number of elements, which may be an invariant that we need. \n",
        "\n",
        "* Finally, we specified num_workers to be two, meaning data will be fetched in parallel by two processes. Once the DataLoader has been created, iterating over the dataset and thereby retrieving batches is simple and natural."
      ]
    },
    {
      "metadata": {
        "id": "59SHs1jYYDu4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Transforms"
      ]
    },
    {
      "metadata": {
        "id": "HXO5tVMUYHxC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Torch.Vision"
      ]
    },
    {
      "metadata": {
        "id": "71oPowr-Y17n",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We have seen how to use datasets, transforms and dataloader. \n",
        "\n",
        "torchvision package provides some common datasets and transforms. You might not even have to write custom classes. One of the more generic datasets available in torchvision is **ImageFolder**. It assumes that images are organized in the following way:\n",
        "\n",
        "root/ants/xxx.png\n",
        "\n",
        "root/ants/xxy.jpeg\n",
        "\n",
        "root/ants/xxz.png\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        "root/bees/123.jpg\n",
        "\n",
        "root/bees/nsdf3.png\n",
        "\n",
        "root/bees/asd932_.png\n",
        "\n",
        "where ‘ants’, ‘bees’ etc. are class labels."
      ]
    },
    {
      "metadata": {
        "id": "Iy05U94pZLu7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import transforms, datasets\n",
        "\n",
        "data_transform = transforms.Compose([\n",
        "        transforms.RandomSizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                             std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "mydataset = datasets.ImageFolder(root='root',transform=data_transform)\n",
        "dataset_loader = torch.utils.data.DataLoader(mydataset,batch_size=4, shuffle=True,num_workers=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i9b0Berwarjx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The DataLoader actually has some reasonably sophisticated logic to determine how to collate individual samples returned from our dataset’s __getitem__ method into a batch, as returned by the DataLoader during iteration. \n",
        "\n",
        "For example, if __getitem__ returns a dictionary, the DataLoader will aggregate the values of that dictionary into a single mapping for the entire batch, using the same keys. This means that if the Dataset’s __getitem__ returns a dict(example=example, label=label), then the batch returned by the DataLoader will return something like dict(example=[example1, example2, ...], label=[label1, label2, ...]), i.e. unpacking the values of indidvidual samples and re-packing them into a single key for the batch’s dictionary. \n",
        "\n",
        "To override this behavior, you can pass a function argument for the collate_fn parameter to the DataLoader object."
      ]
    },
    {
      "metadata": {
        "id": "9LWN7mkQFC_y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### MNIST Example"
      ]
    }
  ]
}